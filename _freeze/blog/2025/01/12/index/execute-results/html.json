{
  "hash": "96886437e019d21c40160417ad78fd00",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices\"\ndate: 2025-01-12\ndescription: \"In this post, I re-evaluate a method that was recently published in arXiv, critiquing the baseline model used in the paper and then implementing a new baseline model that implements standard best practices and similar multi-round aggregation. I find that the SAMRE method does not perform better than the new baseline model. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as the being skeptical of claims in research papers that compare new methods to a baseline.\"\ncategories:\n  - prompt-engineering\n  - python\n  - LLM-as-judge\n  - LLM-evals\nfreeze: true\n---\n\n\nI've been doing a lot of work with LLM-based evaluations lately, and I've been thinking about how to improve the quality of these evaluations.\n\nI like to read research papers from arXiv for inspiration, and I recently came across a paper called [Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates](https://arxiv.org/abs/2410.04663), which introduces a new method inspired by judicial process called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process.\n\nI was initially impressed by the results, as they reported a gain of 6-8% over the baseline method(!!). However, I am often skeptical of comparisons to \"baseline\" models in these research papers, as I find that they often fail to implement standard best practices and are therefore not represenative of true gains over baseline.\n\nGiven this skepticism of mine, I decided that it might be interesting to put it to the test: What if I implemented the SAMRE method, and compared it to a baseline model that does implement standard best practices for prompt engineering? Would I find that the SAMRE method is indeed an improvement over the baseline? Or would I find that SAMRE is inferior to a properly implemented baseline?\n\nUsing a sample of 180 conversations from MT-bench for testing and evaluation, I evaluated three methods:\n\n1. SAMRE, as implemented in the paper\n2. Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)\n3. Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.\n\nAfter running the evaluations and calculating agreement with human judges, I found that although SAMRE did yield better agreement than Baseline-Weak (18% improvement), it was inferior to Baseline-Strong (which was 36% better than SAMRE!).\n\nThese results serve to highlight the importance of implementing standard best practices in baseline models, as well as being skeptical of claims in research papers that compare new methods to a \"baseline model\".\n\n# Baseline model inadequacies\n\nFirst, let's consider some of the inadequacies in the Baseline model's prompt reported in the paper. The prompt they used was as follows:\n\n```prompt\nYou are a fair, impartial judge scoring a debate on the following question:\nquestion.\nAnswer 1: answer_1\nAnswer 2: answer_2\nScore each answer on a scale of 1-20 for each of the following criteria:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nProvide scores as [answer_1_score, answer_2_score] for each criterion in a list format, then sum for final scores. Please keep an eye on the slightest difference that should make a difference in the scoring. Don’t overthink!\nRelevance:\nAccuracy:\nDepth:\nClarity:\nLogic and Factuality:\nAddressing opponent’s points:\nFinal Scores (sum of above) as a tuple (example: (18, 9)):\nExplain your scoring, focusing on why one answer is better than the other based on the criteria above. Keep your explanation concise but informative.\nFinally, return the final score tuple (score1, score2) as a tuple (in parentheses).\nExample: (18, 9)\nYour scores and explanation:\n```\n\nHere are the issues I see with this prompt:\n\n1. The prompt does not use delimiters for most of the inputs. I would enclose the inputs inside XML tags like <Question></Question>, <Answer1></Answer1>, and <Answer2></Answer2>, but in a pinch delimiters like triple backticks can be used.\n\n2. The prompt instructs the model to first generate scores in list format, and then to sum them. But as we know, language models models often make arithmetic mistakes. It would be better to ask the model to generate scores for each criterion, and then to programmatically extract and summarize them in python (or another programming language) from which the routine is run.\n\n3. Although the prompt asks the model to \"explain your scoring\", it is not clear if the model should be reasoning about each criterion before it scores them, or if it should provide reasoning at the end when giving its final score. I would ask the model to provide reasoning for each criterion that it is asked to score, and ask it to reason before scoring.\n\n4. It's unclear why a scale of 1-20 is used. This is not a standard scale for scoring. I would use a scale of 1-10 which is likely more familiar to the model and can be expected to be used more consistently.\n\n5. Although the prompt does suggest that the model provide its scores in tuple format, it would be better to provide more explicit format instructions.\n\n6. The prompt includes an \"Effectiveness in addressing opponent's points\" criterion, but this is almost certainly irrelevant given that the answers to the question were not generated with the goal of addressing an opponent.\n\n7. Finally, although this goes beyond the prompt itself, the authors of the paper are comparing a multi-round method to a single-round method. This is obviously an unfair comparison. Instead, it would be better to compare the SAMRE method to a baseline that uses the same number of rounds and then similarly averages its scores.\n\nWith all of that in mind, here's how I would rewrite the prompt:\n\n```prompt\nYou are a fair, impartial judge scoring a debate on Question.\n\n<Question>\n{question}\n</Question>\n\nTwo Answers have been given to the Question.\n\n<Answer1>\n{answer_1}\n</Answer1>\n\n<Answer2>\n{answer_2}\n</Answer2>\n\nThe Answers are being judged on the following Criteria:\n\n<Criteria>\n<Criterion1>Relevance to their task</Criterion1>\n<Criterion2>Accuracy and credible sources</Criterion2>\n<Criterion3>Depth and completeness</Criterion3>\n<Criterion4>Clarity and logical flow</Criterion4>\n<Criterion5>Reasoning and factual support</Criterion5>\n</Criteria>\n\nFor each Criterion, briefly analyze the performance of \nthe two Answers, then give a score between 1 and 10.\n\nRespond as follows:\n<Criterion1>\n<CriterionName>Relevance to their task</CriterionName>\n<Analysis>\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n</Analysis>\n<Scores>\n<Answer1Score>[score between 1 and 10]</Answer1Score>\n<Answer2Score>[score between 1 and 10]</Answer2Score>\n</Scores>\n</Criterion1>\n<Criterion2>\n<CriterionName>Accuracy and credible sources</CriterionName>\n<Analysis>\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n</Analysis>\n<Scores>\n<Answer1Score>[score between 1 and 10]</Answer1Score>\n<Answer2Score>[score between 1 and 10]</Answer2Score>\n</Scores>\n</Criterion2>\n...\n```\n\nNotice that the prompt now uses XML tags to structure the instructions, that it asks the model to provide reasoning for each criterion before scoring, and that it gives the model a clear format for its response that reinforces analysis before scoring for each criterion.\n\nI've also change the scale from 1-20 to 1-10, removed the unnecessary \"Effectiveness in addressing opponent's points\" criterion, and removed the instruction to summarize the scores, as I would handle this within the code.\n\n# Hypothesis and predictions\n\nI hypothesize that SAMRE will not perform better than a baseline model that implements standard best practices for prompt engineering.\n\nMy predictions are as follows:\n\n1. SAMRE will perform better than Baseline-Weak.\n2. Baseline-Strong will perform better than Baseline-Weak.\n3. Baseline-Strong will perform equal to or better than SAMRE.\n\n# My implementation of SAMRE and Baseline\n\nOkay, so with those criticisms out of the way, let's design evaluators to implement three methods:\n\n1. SAMRE\n2. Baseline-Weak: The baseline model used in the paper that does not implement standard best practices for prompt engineering.\n3. Baseline-Strong: A baseline model that does implement standard best practices for prompt engineering.\n\nBelow is my python implementation of the evaluators. To the best of my ability, I have implemented the SAMRE and Baseline methods as described in the paper (I call the paper's Baseline method \"Baseline-Weak\"). And I have implemented a Baseline-Strong evaluator that implements what I would consider to be standard best practices for prompt engineering.\n\nThis will allow me to test my hypothesis and predictions.\n\n::: {#16fd84d4 .cell code-fold-show='false' execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the implementation code\"}\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple, Dict\nimport os\nfrom openai import AsyncOpenAI\nimport logging\nimport re\nfrom contextlib import asynccontextmanager\n\n# Define the prompts\nPROMPTS = {\n# Prompt for advocates to defend their assigned answer, used in SAMRE\n\"defend_prompt\": \"\"\"\nYou’re an eloquent advocate (Advocate {advocate_id}) in a group defending this answer:\n{answer} to the question: {question}. Your opponent group defends: {opponent_answer}.\nCollaborate with your fellow advocates to convince the audience your argument is\nbetter. Use the latest feedback, your opponent’s last argument, and your team’s previous\narguments to improve your case.\nLatest feedback: {feedback}\nOpponent’s last argument: {opponent_argument}\nYour team’s previous arguments: team_arguments\nRespond in under 80 words.\nYour defense:\n\"\"\",\n# Prompt for judge to provide feedback on debate progress, used in SAMRE\n\"judge_prompt\": \"\"\"\nYou’re a fair, impartial judge in a debate on: \"{question}\". Answer 1: \"{answer_1}\".\nAnswer 2: \"{answer_2}\". Your goal is to provide feedback that will help advocate groups\nimprove and differentiate their arguments more clearly.\nCurrent round: {current_round}\nTotal rounds: {total_rounds}\nPrevious scores: {previous_scores}\nDefense for 1st answer: {defense_1}\nDefense for 2nd answer: {defense_2}\nProvide specific, constructive feedback to help each advocate group strengthen their\nunique position. Encourage them to address weaknesses and highlight distinctions. Aim\nfor your feedback to lead to more divergent scores in future rounds.\nGive your feedback in under 50 words:\n\"\"\",\n# Prompt for SAMRE method scoring\n\"score_prompt_samre\": \"\"\"\nYou’re a critical, impartial judge in a high-stakes debate on: \"{question}\". Answer\n1: \"{answer_1}\". Answer 2: \"{answer_2}\". Your goal is to provide detailed, constructive\nfeedback that will push advocates to significantly improve their arguments.\nTotal rounds: {total_rounds}\nPrevious scores: {previous_scores}\nDefense for 1st answer: {defense_1}\nDefense for 2nd answer: {defense_2}\nAnalyze each argument meticulously. Be thorough and unbiased in your assessment of:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nFor each criterion, provide a score on a scale of 1-20 and detailed justification.\nScores should be given as [answer_1_score, answer_2_score] for each criterion.\nYour comprehensive feedback for each advocate (50 words each):\nFeedback for Advocate 1:\nFeedback for Advocate 2:\nSum up the scores and return the final score tuple (score1, score2). Example: (95, 87)\nYour detailed scores and final tally:\n\"\"\",\n# Prompt for Baseline-Weak method scoring, which represents the baseline model used in the paper\n\"score_prompt_baseline_weak\": \"\"\"\nYou are a fair, impartial judge scoring a debate on the following question:\nquestion.\nAnswer 1: {answer_1}\nAnswer 2: {answer_2}\nScore each answer on a scale of 1-20 for each of the following criteria:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nProvide scores as [Answer1_score, Answer2_score] for each criterion in a list format,\nthen sum for final scores. Please keep an eye on the slightest difference that should\nmake a difference in the scoring. Don’t overthink!\nRelevance:\nAccuracy:\nDepth:\nClarity:\nLogic and Factuality:\nAddressing opponent’s points:\nFinal Scores (sum of above) as a tuple (example: (18, 9)):\nExplain your scoring, focusing on why one answer is better than the other based on the\ncriteria above. Keep your explanation concise but informative.\nFinally, return the final score tuple (score1, score2) as a tuple (in parentheses).\nExample: (18, 9)\nYour scores and explanation:\n\"\"\",\n# Prompt for Baseline-Strong method scoring, which implements what I consider to be standard best practices for prompt engineering\n\"score_prompt_baseline_strong\": \"\"\"\nYou are a fair, impartial judge scoring a debate on Question.\n\n<Question>\n{question}\n</Question>\n\nTwo Answers have been given to the Question.\n\n<Answer1>\n{answer_1}\n</Answer1>\n\n<Answer2>\n{answer_2}\n</Answer2>\n\nThe Answers are being judged on the following Criteria:\n\n<Criteria>\n<Criterion1>Relevance to their task</Criterion1>\n<Criterion2>Accuracy and credible sources</Criterion2>\n<Criterion3>Depth and completeness</Criterion3>\n<Criterion4>Clarity and logical flow</Criterion4>\n<Criterion5>Reasoning and factual support</Criterion5>\n</Criteria>\n\nFor each Criterion, briefly analyze the performance of \nthe two Answers, then give a score between 1 and 10.\n\nRespond as follows:\n<Criterion1>\n<CriterionName>Relevance to their task</CriterionName>\n<Analysis>\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n</Analysis>\n<Scores>\n<Answer1Score>[score between 1 and 10]</Answer1Score>\n<Answer2Score>[score between 1 and 10]</Answer2Score>\n</Scores>\n</Criterion1>\n<Criterion2>\n<CriterionName>Accuracy and credible sources</CriterionName>\n<Analysis>\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n</Analysis>\n<Scores>\n<Answer1Score>[score between 1 and 10]</Answer1Score>\n<Answer2Score>[score between 1 and 10]</Answer2Score>\n</Scores>\n</Criterion2>\n...\n\"\"\"\n}\n\n@dataclass\nclass Memory:\n    \"\"\"Stores debate history including arguments, scores, and feedback for each round, used in SAMRE\"\"\"\n    arguments: List[Tuple[str, str]] = field(default_factory=list)\n    scores: List[Tuple[float, float]] = field(default_factory=list)\n    feedback: List[str] = field(default_factory=list)\n\nclass ModelEvaluator:\n    @classmethod\n    @asynccontextmanager\n    async def create(cls, mode=\"samre\", model=\"gpt-4o-mini\", logging_level=logging.WARNING):\n        \"\"\"Factory method to create evaluator instance with proper async context management\"\"\"\n        instance = cls(mode=mode, model=model, logging_level=logging_level)\n        instance.client = AsyncOpenAI()\n        try:\n            yield instance\n        finally:\n            await instance.client.close()\n\n    def _setup_logger(self, logging_level):\n        \"\"\"Setup logger with word wrapping.\"\"\"\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging_level)\n        if not logger.handlers:\n            handler = logging.StreamHandler()\n            class WrapFormatter(logging.Formatter):\n                def format(self, record):\n                    import textwrap\n                    message = super().format(record)\n                    return '\\n'.join(textwrap.fill(line, width=80) \n                                for line in message.split('\\n'))\n            \n            formatter = WrapFormatter('%(message)s')\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n        return logger\n\n    def __init__(self, mode=\"samre\", model=\"gpt-4o-mini\", logging_level=logging.WARNING):\n        self.mode = mode\n        self.model = model\n        # Modify to handle both baseline modes\n        self.max_rounds = 1 if mode.startswith(\"baseline\") else 4\n        self.logger = self._setup_logger(logging_level)\n        \n        # Initialize all prompts\n        self.defend_prompt = PROMPTS[\"defend_prompt\"]\n        self.judge_prompt = PROMPTS[\"judge_prompt\"]\n\n\n    async def get_completion(self, prompt: str) -> str:\n        \"\"\"Get a completion from the OpenAI API.\"\"\"\n        if not self.client:\n            raise RuntimeError(\"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'\")\n            \n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"system\", \"content\": prompt}],\n            temperature=0\n        )\n        return response.choices[0].message.content\n\n    def _extract_final_scores(self, score_response: str) -> Tuple[float, float]:\n        \"\"\"Extracts final scores from model response based on evaluation mode\"\"\"\n        if self.mode == \"samre\":\n            # Look for final tuple in format (score1, score2)\n            tuple_pattern = r'\\((\\d+\\.?\\d*),\\s*(\\d+\\.?\\d*)\\)'\n            match = re.search(tuple_pattern, score_response)\n            if match:\n                return (float(match.group(1)), float(match.group(2)))\n            raise ValueError(\"Could not find score tuple in SAMRE response\")\n        \n        elif self.mode == \"baseline_weak\":\n            # Look for final tuple in format (score1, score2)\n            tuple_pattern = r'\\((\\d+\\.?\\d*),\\s*(\\d+\\.?\\d*)\\)'\n            match = re.search(tuple_pattern, score_response)\n            if match:\n                return (float(match.group(1)), float(match.group(2)))\n            raise ValueError(\"Could not find score tuple in weak baseline response\")\n        \n        elif self.mode == \"baseline_strong\":\n            # Use XML parsing for strong baseline\n            score_a_pattern = r'<Answer1Score>\\s*(\\d+\\.?\\d*)\\s*</Answer1Score>'\n            score_b_pattern = r'<Answer2Score>\\s*(\\d+\\.?\\d*)\\s*</Answer2Score>'\n            \n            scores_a = [float(match.group(1)) for match in re.finditer(score_a_pattern, score_response)]\n            scores_b = [float(match.group(1)) for match in re.finditer(score_b_pattern, score_response)]\n            \n            if not scores_a or not scores_b:\n                raise ValueError(\"Could not find scores for both candidates\")\n            \n            if len(scores_a) != len(scores_b):\n                raise ValueError(f\"Mismatched number of scores: A={len(scores_a)}, B={len(scores_b)}\")\n            \n            final_score_a = sum(scores_a) / len(scores_a)\n            final_score_b = sum(scores_b) / len(scores_b)\n            \n            return (final_score_a, final_score_b)\n        \n        else:\n            raise ValueError(f\"Unknown mode: {self.mode}\")\n\n    async def evaluate(self, question: str, answer_1: str, answer_2: str, num_rounds: int = 1) -> Dict:\n        \"\"\"Main evaluation entry point that routes to appropriate evaluation method based on mode\"\"\"\n        if not self.client:\n            raise RuntimeError(\"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'\")\n            \n        if self.mode.startswith(\"baseline\"):\n            self.logger.info(f\"\\n=== Starting {self.mode.title()} Evaluation ===\\n\")\n            return await self._evaluate_baseline(question, answer_1, answer_2, num_rounds)\n        else:\n            self.logger.info(\"\\n=== Starting SAMRE Evaluation ===\\n\")\n            return await self._evaluate_samre(question, answer_1, answer_2)\n\n    async def _evaluate_baseline(self, question: str, answer_1: str, answer_2: str, num_rounds: int = 1) -> Dict:\n        \"\"\"Implements baseline evaluation methods (both weak and strong)\"\"\"\n        score_history = []\n        \n        num_rounds = 1 if self.mode == \"baseline_weak\" else num_rounds\n        for _ in range(num_rounds):\n            # Select appropriate prompt based on mode\n            prompt_key = \"score_prompt_\" + self.mode\n            score_prompt = PROMPTS[prompt_key].format(\n                question=question,\n                answer_1=answer_1,\n                answer_2=answer_2\n            )\n            score_response = await self.get_completion(score_prompt)\n            self.logger.info(f\"Score response: {score_response}\")\n            \n            try:\n                round_scores = self._extract_final_scores(score_response)\n                score_history.append(list(round_scores))\n            except Exception as e:\n                self.logger.error(f\"Score parsing error: {e}\")\n                self.logger.error(f\"Raw score response: {score_response}\")\n                score_history.append([10.0, 10.0])\n\n        # Calculate average scores across all rounds\n        avg_scores = [\n            sum(scores[i] for scores in score_history) / len(score_history)\n            for i in range(2)\n        ]\n\n        # Determine winner based on average scores\n        winner = (\n            'model_a' if avg_scores[0] > avg_scores[1]\n            else 'model_b' if avg_scores[0] < avg_scores[1]\n            else 'tie'\n        )\n\n        return {\n            \"winner\": winner,\n            \"average_scores\": [round(score, 2) for score in avg_scores] ,\n            \"rounds\": len(score_history),\n            \"score_history\": score_history,\n            \"full_response\": score_response  # Include the final response for analysis\n        }\n        \n    async def _evaluate_samre(self, question: str, answer_1: str, answer_2: str) -> Dict:\n        \"\"\"Implements SAMRE evaluation with multi-round debate process\n        \n        Flow:\n        1. Get defenses from both advocates\n        2. Judge provides feedback and scores\n        3. Repeat until max rounds or convergence\n        4. Return averaged results\n        \"\"\"\n        local_memory = Memory()\n        \n        self.logger.info(\"\\n=== Starting SAMRE Evaluation ===\\n\")\n        \n        for round_num in range(self.max_rounds):\n            self.logger.info(f\"\\n--- Round {round_num + 1} ---\")\n            \n            scores = await self._run_debate_round(\n                question,\n                answer_1, \n                answer_2, \n                round_num,\n                local_memory\n            )\n            \n            if self._has_scores_converged(round_num, local_memory):\n                self.logger.info(\"\\nScores have converged - ending debate early.\")\n                break\n        \n        return self._prepare_results(local_memory)\n\n    async def defend_answer(self, question: str, answer_1: str, answer_2: str, \n                        advocate_id: int, feedback: str = \"\", \n                        opponent_argument: str = \"\",\n                        team_arguments: List[str] = None) -> str:\n        \"\"\"Get defense from an advocate.\n        \n        Args:\n            question: The question being debated\n            answer_1: First answer in the debate\n            answer_2: Second answer in the debate\n            advocate_id: Which advocate (1 or 2) is defending\n            feedback: Previous feedback from judge\n            opponent_argument: Last argument from opponent\n            team_arguments: List of previous arguments from this advocate's team\n        \"\"\"\n        if team_arguments is None:\n            team_arguments = []\n            \n        # Map answers based on advocate_id\n        answer = answer_1 if advocate_id == 1 else answer_2\n        opponent_answer = answer_2 if advocate_id == 1 else answer_1\n            \n        prompt = self.defend_prompt.format(\n            question=question,\n            advocate_id=advocate_id,\n            answer=answer,  # The answer this advocate is defending\n            opponent_answer=opponent_answer,  # The opposing answer\n            feedback=feedback,\n            opponent_argument=opponent_argument,\n            team_arguments=\"\\n\".join(team_arguments)\n        )\n        return await self.get_completion(prompt)\n\n    async def judge_debate(self, question: str, answer_1: str, answer_2: str,\n                          defense_1: str, defense_2: str, \n                          current_round: int,\n                          memory: Memory) -> Tuple[str, Tuple[float, float]]:\n        \"\"\"Judge the debate between two answers.\"\"\"\n        feedback_prompt = self.judge_prompt.format(\n            question=question,\n            answer_1=answer_1,\n            answer_2=answer_2,\n            current_round=current_round,\n            total_rounds=self.max_rounds,\n            previous_scores=memory.scores,\n            defense_1=defense_1,\n            defense_2=defense_2\n        )\n        feedback = await self.get_completion(feedback_prompt)\n        \n        score_prompt = PROMPTS[\"score_prompt_samre\"].format(\n            question=question,\n            answer_1=answer_1,\n            answer_2=answer_2,\n            defense_1=defense_1,\n            defense_2=defense_2,\n            total_rounds=self.max_rounds,\n            previous_scores=memory.scores,\n            feedback=feedback\n        )\n        score_response = await self.get_completion(score_prompt)    \n        self.logger.info(f\"Score response: {score_response}\")\n        \n        try:\n            scores = self._extract_final_scores(score_response)\n        except Exception as e:\n            self.logger.error(f\"Score parsing error: {e}\")\n            self.logger.error(f\"Raw score response: {score_response}\")\n            scores = (10.0, 10.0)\n        \n        return feedback, scores\n\n    async def _run_debate_round(self, question: str, answer_1: str, answer_2: str, \n                               round_num: int, memory: Memory) -> Tuple[float, float]:\n        \"\"\"Executes single debate round in SAMRE evaluation\"\"\"\n        defenses = await self._get_advocate_defenses(question, answer_1, answer_2, memory)\n        memory.arguments.append(defenses)\n        \n        feedback, scores = await self.judge_debate(\n            question, answer_1, answer_2, defenses[0], defenses[1], round_num + 1, memory\n        )\n        \n        self._store_round_results(feedback, scores, memory)\n        self._display_round_results(defenses, feedback, scores)\n        \n        return scores\n\n    async def _get_advocate_defenses(self, question: str, answer_1: str, answer_2: str,\n                                   memory: Memory) -> Tuple[str, str]:\n        \"\"\"Get defenses from both advocates.\"\"\"\n        defense_1 = await self.defend_answer(\n            question, answer_1, answer_2, 1,\n            feedback=memory.feedback[-1] if memory.feedback else \"\",\n            opponent_argument=memory.arguments[-1][1] if memory.arguments else \"\",\n            team_arguments=[args[0] for args in memory.arguments]\n        )\n        \n        defense_2 = await self.defend_answer(\n            question, answer_1, answer_2, 2,\n            feedback=memory.feedback[-1] if memory.feedback else \"\",\n            opponent_argument=memory.arguments[-1][0] if memory.arguments else \"\",\n            team_arguments=[args[1] for args in memory.arguments]\n        )\n        \n        return (defense_1, defense_2)\n\n    def _store_round_results(self, feedback: str, scores: Tuple[float, float],\n                           memory: Memory) -> None:\n        \"\"\"Store feedback and scores from the round.\"\"\"\n        memory.feedback.append(feedback)\n        memory.scores.append(scores)\n\n    def _display_round_results(self, defenses: Tuple[str, str], \n                             feedback: str, scores: Tuple[float, float]) -> None:\n        \"\"\"Display the results of the current round.\"\"\"\n        self.logger.info(f\"\\nAdvocate 1's defense:\\n{defenses[0]}\")\n        self.logger.info(f\"\\nAdvocate 2's defense:\\n{defenses[1]}\")\n        self.logger.info(f\"\\nJudge's feedback:\\n{feedback}\")\n        self.logger.info(f\"Scores for this round: Answer 1 = {round(scores[0], 2)}, Answer 2 = {round(scores[1], 2)}\")\n\n    def _has_scores_converged(self, round_num: int, memory: Memory) -> bool:\n        \"\"\"Checks if debate scores have converged by comparing last two rounds\"\"\"\n        if round_num > 0:\n            prev_diff = memory.scores[-2][0] - memory.scores[-2][1]\n            curr_diff = memory.scores[-1][0] - memory.scores[-1][1]\n            return (prev_diff * curr_diff) > 0\n        return False\n\n    def _prepare_results(self, memory: Memory) -> Dict:\n        \"\"\"Prepare the final results dictionary.\"\"\"\n        avg_scores = [\n            round(sum(scores[i] for scores in memory.scores) / len(memory.scores), 2)\n            for i in range(2)\n        ]\n        \n        winner = (\n            'model_a' if avg_scores[0] > avg_scores[1]\n            else 'model_b' if avg_scores[0] < avg_scores[1]\n            else 'tie'\n        )\n        \n        return {\n            \"winner\": winner,\n            \"average_scores\": avg_scores,\n            \"rounds\": len(memory.scores),\n            \"score_history\": [[round(s[0], 2), round(s[1], 2)] for s in memory.scores],\n            \"argument_history\": memory.arguments,\n            \"feedback_history\": memory.feedback\n        }\n```\n:::\n\n\n# Load the MT-bench dataset\n\nNext I will read in the MT-bench dataset from disk and prepare it for evaluation. I will use [MtBenchHumanJudgementDataset](https://llamahub.ai/l/llama_datasets/MT%20Bench%20Human%20Judgement%20Dataset?from=) from Llamahub.\n\n::: {#a2c1b23c .cell execution_count=2}\n``` {.python .cell-code}\n# Commented out since the dataset is already downloaded\n#!llamaindex-cli download-llamadataset MtBenchHumanJudgementDataset --download-dir ./data\n```\n:::\n\n\nNext, I will load the dataset into a pandas dataframe and take a random sample of 300 rows.\n\n::: {#052c7fe4 .cell code-fold-show='false' execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code that loads the dataset\"}\nimport json\nimport pandas as pd\nfrom llama_index.core.llama_dataset import LabelledPairwiseEvaluatorDataset\n\ndf = LabelledPairwiseEvaluatorDataset.from_json(\n    \"./data/pairwise_evaluator_dataset.json\"\n).to_pandas()\n\ndf = df[['query', 'answer', 'second_answer', 'answer_by', 'second_answer_by', 'reference_score']]\n\n# Rename as follows: query => question, answer => model_a_answer, second_answer => model_b_answer, answer_by => model_a, second_answer_by => model_b, reference_score => human_winner\ndf.rename(columns={'query': 'question', 'answer': 'model_a_answer', 'second_answer': 'model_b_answer', 'answer_by': 'model_a', 'second_answer_by': 'model_b', 'reference_score': 'human_winner'}, inplace=True)\n\n# Reencode human winner as \"model_a\" if 1, \"model_b\" if 0, and \"tie\" if 0.5\ndf['human_winner'] = df['human_winner'].apply(lambda x: 'model_a' if x == 1 else 'model_b' if x == 0 else 'tie')\n\n# Take a random sample of 300 rows\ndf = df.sample(n=300, random_state=42)\n\ndf.head()\n\n# Take first 180 rows\ndf = df.iloc[:180]\n```\n:::\n\n\n# Use methods to evaluate MT-bench dataset\n\nUsing the MT-bench dataset, I will run the three LLM models (Baseline-Weak, Baseline-Strong, and SAMRE) on each set of question and answers.\n\nThe code below is the main evaluation loop, designed to run multiple evaluations asynchronously (to save time). It will evaluate each item in the dataset, and save the results to disk as a checkpoint. If the evaluation is interrupted, the code can be resumed from the last checkpoint.\n\n::: {#6bb14f79 .cell code-fold-show='false' execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code that runs the evaluations\"}\nimport asyncio\nfrom asyncio import Semaphore\nimport logging\nimport os\nimport hashlib\nimport json\nlogging.basicConfig(level=logging.WARNING)\n\nasync def evaluate_conversation_pair(row, evaluators, semaphore, idx, total):\n    \"\"\"Evaluate a single conversation pair with all evaluators\"\"\"\n    async with semaphore:\n        # Add delay between API calls\n        #await asyncio.sleep(1)  # Add small delay between conversations\n        \n        # Generate pair_id from conversation hash\n        pair_id = f\"{row['model_a']}_{row['model_b']}_{hashlib.sha256(str(row['question']).encode()).hexdigest()[:12]}\"\n        checkpoint_file = f'checkpoints/{pair_id}.json'\n        \n        # Return existing checkpoint if available\n        if os.path.exists(checkpoint_file):\n            logging.info(f\"Found existing checkpoint file for {pair_id}\")\n            return json.load(open(checkpoint_file))\n        \n        logging.info(f\"No checkpoint file found for {pair_id}\")\n        result = {\n            'model_a': row['model_a'],\n            'model_b': row['model_b'],\n            'human_winner': row['human_winner'],\n            'pair_id': pair_id\n        }\n        \n        try:\n            # First run SAMRE evaluation with retries\n            for attempt in range(3):  # Try up to 3 times\n                try:\n                    samre_evaluator = evaluators['samre']\n                    samre_result = await samre_evaluator.evaluate(\n                        row['question'], \n                        row['model_a_answer'], \n                        row['model_b_answer']\n                    )\n                    result['samre_winner'] = samre_result['winner']\n                    result.update({f'samre_{k}': samre_result[k] for k in ['average_scores', 'rounds', 'score_history']})\n                    result.update({\n                        'samre_argument_history': samre_result['argument_history'],\n                        'samre_feedback_history': samre_result['feedback_history']\n                    })\n                    break  # If successful, break retry loop\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1  # Exponential backoff\n                        print(f\"Rate limit hit on SAMRE, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:  # Last attempt failed\n                            raise\n                    else:\n                        raise  # Re-raise non-rate-limit errors\n\n            await asyncio.sleep(0.5)  # Add small delay between evaluator calls\n            \n            # Run baseline strong with same number of rounds as SAMRE\n            for attempt in range(3):\n                try:\n                    baseline_strong_evaluator = evaluators['baseline_strong']\n                    baseline_strong_result = await baseline_strong_evaluator.evaluate(\n                        row['question'],\n                        row['model_a_answer'],\n                        row['model_b_answer'],\n                        num_rounds=result['samre_rounds']\n                    )\n                    result['baseline_strong_winner'] = baseline_strong_result['winner']\n                    result.update({f'baseline_strong_{k}': baseline_strong_result[k] \n                                 for k in ['average_scores', 'rounds', 'score_history']})\n                    result['baseline_strong_full_response'] = baseline_strong_result['full_response']\n                    break\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1\n                        print(f\"Rate limit hit on baseline strong, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:\n                            raise\n                    else:\n                        raise\n\n            await asyncio.sleep(0.5)  # Add small delay between evaluator calls\n\n            # Run baseline weak with 1 round\n            for attempt in range(3):\n                try:\n                    baseline_weak_evaluator = evaluators['baseline_weak']\n                    baseline_weak_result = await baseline_weak_evaluator.evaluate(\n                        row['question'],\n                        row['model_a_answer'],\n                        row['model_b_answer'],\n                        num_rounds=1\n                    )\n                    result['baseline_weak_winner'] = baseline_weak_result['winner']\n                    result.update({f'baseline_weak_{k}': baseline_weak_result[k] \n                                 for k in ['average_scores', 'rounds', 'score_history']})\n                    result['baseline_weak_full_response'] = baseline_weak_result['full_response']\n                    break\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1\n                        print(f\"Rate limit hit on baseline weak, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:\n                            raise\n                    else:\n                        raise\n                        \n        except Exception as e:\n            print(f\"Error evaluating row {idx}: {str(e)}\")\n            result['samre_winner'] = None\n            result['baseline_strong_winner'] = None\n            result['baseline_weak_winner'] = None\n            result['error'] = str(e)\n        \n        # Save checkpoint after each evaluation\n        os.makedirs('checkpoints', exist_ok=True)\n        json.dump(result, open(checkpoint_file, 'w'))\n        \n        if (idx + 1) % 10 == 0:\n            print(f\"Processed {idx + 1}/{total} conversations\")\n            \n        return result\n\nasync def evaluate_conversations_async(df, evaluators, semaphore_limit=3):\n    \"\"\"Evaluate conversations asynchronously\"\"\"\n    # Reduce semaphore limit\n    semaphore_limit = 1  # Process one at a time to avoid rate limits\n    \n    # Process in smaller batches\n    batch_size = 10\n    results = []\n    \n    for i in range(0, len(df), batch_size):\n        batch = df.iloc[i:i+batch_size]\n        tasks = [\n            evaluate_conversation_pair(row[1], evaluators, Semaphore(semaphore_limit), idx, len(df))\n            for idx, row in enumerate(batch.iterrows(), start=i)\n        ]\n        batch_results = await asyncio.gather(*tasks)\n        results.extend(batch_results)\n        \n        # Add delay between batches\n        if i + batch_size < len(df):\n            print(f\"Completed batch {i//batch_size + 1}, waiting before next batch...\")\n            #await asyncio.sleep(5)  # 5 second delay between batches\n            \n    return pd.DataFrame(results)\n\nasync def main():\n    async with ModelEvaluator.create(mode=\"samre\") as samre_evaluator, \\\n               ModelEvaluator.create(mode=\"baseline_strong\") as baseline_strong_evaluator, \\\n               ModelEvaluator.create(mode=\"baseline_weak\") as baseline_weak_evaluator:\n        return await evaluate_conversations_async(\n            df,\n            {\n                'samre': samre_evaluator, \n                'baseline_strong': baseline_strong_evaluator,\n                'baseline_weak': baseline_weak_evaluator\n            },\n            semaphore_limit=1\n        )\n\n# Run evaluation with checkpoint recovery\ntry:\n    eval_df = await main()\nexcept Exception as e:\n    print(f\"Error during evaluation: {str(e)}\\nRecovering from checkpoints...\")\n    eval_df = pd.DataFrame([json.load(open(f'checkpoints/{f}')) \n                           for f in os.listdir('checkpoints') \n                           if f.endswith('.json')])\nfinally:\n    eval_df.to_csv('eval_df.csv', index=False)\n    eval_df.head()\n\n# Drop rows with any null values on the model winner columns\neval_df = eval_df.dropna(subset=['baseline_strong_winner', 'baseline_weak_winner', 'samre_winner'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCompleted batch 1, waiting before next batch...\nCompleted batch 2, waiting before next batch...\nCompleted batch 3, waiting before next batch...\nCompleted batch 4, waiting before next batch...\nCompleted batch 5, waiting before next batch...\nCompleted batch 6, waiting before next batch...\nCompleted batch 7, waiting before next batch...\nCompleted batch 8, waiting before next batch...\nCompleted batch 9, waiting before next batch...\nCompleted batch 10, waiting before next batch...\nCompleted batch 11, waiting before next batch...\nCompleted batch 12, waiting before next batch...\nCompleted batch 13, waiting before next batch...\nCompleted batch 14, waiting before next batch...\nCompleted batch 15, waiting before next batch...\nCompleted batch 16, waiting before next batch...\nCompleted batch 17, waiting before next batch...\n```\n:::\n:::\n\n\n# Results\n\nNow that the evaluation is complete, I will evaluate the performance of each of the three methods by looking at how well each method agreed with the human judgments. I'll use Krippendorff's alpha to measure agreement, since it is a robust measure of agreement that can handle non-binary ratings (among other things).\n\n::: {#ce3bb01e .cell code-fold-show='false' execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code that calculates agreement\"}\nfrom krippendorff import alpha\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef calculate_agreement(df, rater1_col, rater2_col):\n    \"\"\"\n    Calculate Krippendorff's alpha between two raters.\n    \n    Args:\n        df: DataFrame containing the ratings\n        rater1_col: Name of first rater's column\n        rater2_col: Name of second rater's column\n    \n    Returns:\n        float: Krippendorff's alpha score\n    \"\"\"\n    # Create label encoder\n    le = LabelEncoder()\n    \n    # Combine all unique values from both columns\n    all_values = pd.concat([df[rater1_col], df[rater2_col]]).unique()\n    le.fit(all_values)\n    \n    # Transform the ratings to numeric values\n    ratings1 = le.transform(df[rater1_col].fillna('missing'))\n    ratings2 = le.transform(df[rater2_col].fillna('missing'))\n    \n    # Reshape data for krippendorff alpha calculation\n    # Each row represents one item, each column represents one rater\n    reliability_data = np.vstack([ratings1, ratings2])\n    \n    return alpha(reliability_data=reliability_data, level_of_measurement='nominal')\n\n# Calculate agreement scores for all methods\nhuman_baseline_strong_agreement = calculate_agreement(eval_df, 'human_winner', 'baseline_strong_winner')\nhuman_baseline_weak_agreement = calculate_agreement(eval_df, 'human_winner', 'baseline_weak_winner')\nhuman_samre_agreement = calculate_agreement(eval_df, 'human_winner', 'samre_winner')\n\n# Create a DataFrame with the agreement scores\nagreement_df = pd.DataFrame({\n    'Evaluator Pair': ['Human-Baseline Strong', 'Human-Baseline Weak', 'Human-SAMRE'],\n    'Krippendorff Alpha': [human_baseline_strong_agreement, human_baseline_weak_agreement, human_samre_agreement]\n})\n\n# Round the scores to 3 decimal places\nagreement_df['Krippendorff Alpha'] = agreement_df['Krippendorff Alpha'].round(3)\n\n# Display the DataFrame\nagreement_df\n\n# Calculate the percent difference between Baseline-Strong and Baseline-Weak, and SAMRE and Baseline-Strong\nbaseline_strong_baseline_weak_diff = (human_baseline_strong_agreement - human_baseline_weak_agreement) / human_baseline_strong_agreement\nbaseline_strong_samre_diff = (human_baseline_strong_agreement - human_samre_agreement) / human_baseline_strong_agreement\nsamre_baseline_weak_diff = (human_samre_agreement - human_baseline_weak_agreement) / human_samre_agreement\n\n# Display the percent difference\nprint(f\"SAMRE vs. Baseline-Weak: {samre_baseline_weak_diff:.0%}\")\nprint(f\"Baseline-Strong vs. Baseline-Weak: {baseline_strong_baseline_weak_diff:.0%}\")\nprint(f\"Baseline-Strong vs. SAMRE: {baseline_strong_samre_diff:.0%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSAMRE vs. Baseline-Weak: 18%\nBaseline-Strong vs. Baseline-Weak: 36%\nBaseline-Strong vs. SAMRE: 21%\n```\n:::\n:::\n\n\nAlthough none of the methods yielded particularly strong agreement with the human judges, we can observe a few things:\n1. As reported in the paper, SAMRE yielded significantly better agreement than Baseline-Weak (0.307 vs. 0.252, an increase of ~18%).\n2. Baseline-Strong yielded significantly better agreement than Baseline-Weak (0.391 vs. 0.252, an increase of ~36%).\n3. Importantly, Baseline-Strong also yielded significantly better agreement than SAMRE (0.391 vs. 0.252, an increase of ~21%)!\n\nThus, SAMRE does not perform better than a baseline that is designed with best practices and similar multi-round aggregation.\n\n# Conclusion\n\nIn this post, I have shown that SAMRE does not perform better than a well-engineered baseline method. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}