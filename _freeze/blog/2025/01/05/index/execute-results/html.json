{
  "hash": "18456e62c4861df1791e97231360b7b5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Single Advocate Multi-Round Evaluation (SAMRE)'\ndate: 2025-01-05\ndescription: \"In this post, I show how to build a Single Advocate Multi-Round Evaluation (SAMRE) system for LLM-based evaluations using python and OpenAI's API. This is an advanced method of LLM-as-judge evaluation.\"\ncategories:\n  - prompt-engineering\n  - python\n  - LLM-as-judge\n  - LLM-evals\nfreeze: true\n---\n\n\nI've been doing a lot of work with LLM-based evaluations lately, and I've been thinking about how to improve the quality of these evaluations.\n\nI recently came across a paper published in October 2024 in arXiv, [Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates](https://arxiv.org/abs/2410.04663), which explores different approaches for using LLMs to evaluate the quality of two alternative responses using methods inspired by judicial process. (Note: Using LLMs to evaluate responses is sometimes called \"LLM as Judge\" evaluation.)\n\nOne of the methods explored in the paper is called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process. \n\n- For each answer being evaluated, there is one advocate who defends it, along with a judge who provides feedback and scoring across multiple criteria (like relevance, accuracy, and clarity).\n- The debate runs for up to 4 rounds, with advocates refining their arguments based on the judge's feedback each round. (Note: The process can end early if the same answer wins twice in a row, saving computational resources.) \n- Each round, the judge scores both answers on a scale of 1-20, and the final winner is determined by the highest average score across all rounds. \n\nThis approach is interesting because it allows the LLM to make a strong case for each answer (as its arguments for each answer are refined over multiple rounds), and then judge the quality of the arguments before picking a winner.\n\nIn this blog post, I'll show how to implement the SAMRE method in python, and I'll use a test case to demonstrate how it works -- printing the results of each round in the debate, as well as the final winner and scores.\n\n# Implementation\n\nThe implementation is straightforward. We define a `SAMREEvaluator` class that initializes the evaluator with a model, and then defines methods for each step of the SAMRE process.\n\nWe also define a `Memory` class to store the results of each round, and a `PROMPTS` dictionary to store the prompts for the different steps of the SAMRE process.\n\nA logger is used to print the results of each round in the debate.\n\n::: {#d0c1c637 .cell execution_count=1}\n``` {.python .cell-code .code-overflow-wrap}\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple, Dict\nimport os\nfrom openai import OpenAI\nimport logging\n\n\n# Define the prompts\nPROMPTS = {\n\"defend_prompt\": \"\"\"\nYou're an eloquent advocate (Advocate {advocate_id}) defending this answer: ```{answer}``` to the question: ```{question}```. Your opponent defends this answer: ```{opponent_answer}```.\n\nUse the latest feedback, opponent's last argument, and your team's previous arguments to improve your case.\n\nLatest feedback: ```{feedback}``` \nOpponent's last argument: ```{opponent_argument}```\nYour team's previous arguments: ```{team_arguments}```\n\nRespond in under 80 words.\n\nYour defense:\n\"\"\",\n\n\"judge_prompt\": \"\"\"\nYou're a fair, impartial judge in a debate on the following question: ```{question}```\n\nAnswer 1: ```{answer1}```\nAnswer 2: ```{answer2}```\n\nYour goal is to provide feedback that will help advocates improve and differentiate their arguments more clearly.\n\nCurrent round: ```{current_round}```\nTotal rounds: ```{total_rounds}```\nPrevious scores: ```{previous_scores}```\nDefense for 1st answer: ```{defense1}```\nDefense for 2nd answer: ```{defense2}```\n\nProvide specific, constructive feedback in under 50 words:\n\"\"\",\n\n\"score_prompt\": \"\"\"You're a critical, impartial judge scoring these debate responses on a scale of 1-20 for each criterion. Return scores as tuple (score1, score2).\n\nQuestion: ```{question}```\nAnswer 1: ```{answer1}```\nAnswer 2: ```{answer2}```\nDefense 1: ```{defense1}```\nDefense 2: ```{defense2}```\n\nCriteria:\n1. Relevance to question\n2. Accuracy and credible sources\n3. Depth and completeness\n4. Clarity and logical flow\n5. Reasoning and factual support\n6. Effectiveness addressing opponent\n\nReturn final scores as tuple, e.g. (18, 9)\"\"\"\n}\n\n\n@dataclass\nclass Memory:\n    # List of (arg1, arg2) tuples\n    arguments: List[Tuple[str, str]] = field(default_factory=list)\n    # List of (score1, score2) tuples\n    scores: List[Tuple[float, float]] = field(default_factory=list)\n    # List of feedback strings\n    feedback: List[str] = field(default_factory=list)\n\n\nclass SAMREEvaluator:\n    def __init__(self, model=\"gpt-4o-mini\"):\n        \"\"\"Initialize the SAMRE evaluator.\"\"\"\n        self.model = model\n        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n        self.max_rounds = 4\n        self.memory = Memory()\n        self.defend_prompt = PROMPTS[\"defend_prompt\"]\n        self.judge_prompt = PROMPTS[\"judge_prompt\"]\n        self.score_prompt = PROMPTS[\"score_prompt\"]\n        \n        # Setup logger with word wrapping\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)\n        if not self.logger.handlers:\n            handler = logging.StreamHandler()\n            class WrapFormatter(logging.Formatter):\n                def format(self, record):\n                    import textwrap\n                    message = super().format(record)\n                    return '\\n'.join(textwrap.fill(line, width=80) \n                                   for line in message.split('\\n'))\n            \n            formatter = WrapFormatter('%(message)s')  # Simple format with wrapping\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n\n\n    def get_completion(self, prompt: str) -> str:\n        \"\"\"Get a completion from the OpenAI API.\"\"\"\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"system\", \"content\": prompt}],\n            temperature=0\n        )\n        return response.choices[0].message.content\n\n\n    def defend_answer(self, question: str, answer: str, opponent_answer: str, \n                     advocate_id: int, feedback: str = \"\", opponent_argument: str = \"\",\n                     team_arguments: List[str] = None) -> str:\n        if team_arguments is None:\n            team_arguments = []\n            \n        prompt = self.defend_prompt.format(\n            advocate_id=advocate_id,\n            answer=answer,\n            question=question,\n            opponent_answer=opponent_answer,\n            feedback=feedback,\n            opponent_argument=opponent_argument,\n            team_arguments=\"\\n\".join(team_arguments)\n        )\n        return self.get_completion(prompt)\n\n\n    def judge_debate(self, \n                     question: str, \n                     answer1: str, \n                     answer2: str,\n                     defense1: str, \n                     defense2: str, \n                     current_round: int) -> Tuple[str, Tuple[float, float]]:\n        \"\"\"Judge the debate between two answers.\"\"\"\n        # Get feedback\n        feedback_prompt = self.judge_prompt.format(\n            question=question,\n            answer1=answer1,\n            answer2=answer2,\n            current_round=current_round,\n            total_rounds=self.max_rounds,\n            previous_scores=self.memory.scores,\n            defense1=defense1,\n            defense2=defense2\n        )\n        feedback = self.get_completion(feedback_prompt)\n        \n        # Get scores\n        score_prompt = self.score_prompt.format(\n            question=question,\n            answer1=answer1,\n            answer2=answer2,\n            defense1=defense1,\n            defense2=defense2\n        )\n        score_response = self.get_completion(score_prompt)\n        \n        # More robust score parsing\n        try:\n            # Try to find any tuple-like pattern in the response\n            import re\n            tuple_match = re.search(r'\\((\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\)', score_response)\n            if tuple_match:\n                scores = (float(tuple_match.group(1)), \n                          float(tuple_match.group(2)))\n            else:\n                # Fallback: try to find any two numbers in the response\n                numbers = re.findall(r'\\d+\\.?\\d*', score_response)\n                if len(numbers) >= 2:\n                    scores = (float(numbers[0]), float(numbers[1]))\n                else:\n                    raise ValueError(\"Could not find two scores in response\")\n        except Exception as e:\n            self.logger.error(f\"Score parsing error: {e}\")\n            self.logger.error(f\"Raw score response: {score_response}\")\n            scores = (10.0, 10.0)\n        \n        return feedback, scores\n\n\n    def evaluate(self, \n                 question: str, \n                 answer1: str, \n                 answer2: str) -> Dict:\n        \"\"\"Evaluate the debate between two answers.\"\"\"\n        self.logger.info(\"\\n=== Starting SAMRE Evaluation ===\\n\")\n        \n        for round_num in range(self.max_rounds):\n            self.logger.info(f\"\\n--- Round {round_num + 1} ---\")\n            \n            scores = self._run_debate_round(question, answer1, answer2, round_num)\n            \n            if self._has_scores_converged(round_num):\n                self.logger.info(\"\\nScores have converged - ending debate early.\")\n                break\n        \n        return self._prepare_results()\n\n\n    def _run_debate_round(self, \n                          question: str, \n                          answer1: str, \n                          answer2: str, \n                          round_num: int) -> Tuple[float, float]:\n\n        \"\"\"Execute a single round of debate between advocates.\"\"\"\n        # Get advocate defenses\n        defenses = self._get_advocate_defenses(question, answer1, answer2)\n        self.memory.arguments.append(defenses)\n        \n        # Get judge's feedback and scores\n        feedback, scores = self.judge_debate(\n            question, answer1, answer2, defenses[0], defenses[1], round_num + 1\n        )\n        \n        # Store and display results\n        self._store_round_results(feedback, scores)\n        self._display_round_results(defenses, feedback, scores)\n        \n        return scores\n\n\n    def _get_advocate_defenses(self, \n                               question: str, \n                               answer1: str, \n                               answer2: str) -> Tuple[str, str]:\n        \"\"\"Get defenses from both advocates.\"\"\"\n        defense1 = self.defend_answer(\n            question, answer1, answer2, 1,\n            feedback=self.memory.feedback[-1] if self.memory.feedback else \"\",\n            opponent_argument=self.memory.arguments[-1][1] if self.memory.arguments else \"\",\n            team_arguments=[args[0] for args in self.memory.arguments]\n        )\n        \n        defense2 = self.defend_answer(\n            question, answer2, answer1, 2,\n            feedback=self.memory.feedback[-1] if self.memory.feedback else \"\",\n            opponent_argument=self.memory.arguments[-1][0] if self.memory.arguments else \"\",\n            team_arguments=[args[1] for args in self.memory.arguments]\n        )\n        \n        return (defense1, defense2)\n\n\n    def _store_round_results(self, \n                             feedback: str, \n                             scores: Tuple[float, float]) -> None:\n        \"\"\"Store feedback and scores from the round.\"\"\"\n        self.memory.feedback.append(feedback)\n        self.memory.scores.append(scores)\n\n\n    def _display_round_results(self, \n                               defenses: Tuple[str, str], \n                               feedback: str, \n                               scores: Tuple[float, float]) -> None:\n        \"\"\"Display the results of the current round.\"\"\"\n        self.logger.info(f\"\\nAdvocate 1's defense:\\n{defenses[0]}\")\n        self.logger.info(f\"\\nAdvocate 2's defense:\\n{defenses[1]}\")\n        self.logger.info(f\"\\nJudge's feedback:\\n{feedback}\")\n        self.logger.info(f\"Scores for this round: Answer 1 = {scores[0]}, Answer 2 = {scores[1]}\")\n\n\n    def _has_scores_converged(self, \n                              round_num: int) -> bool:                              \n        \"\"\"Check if scores have converged (same winner twice in a row).\"\"\"\n        if round_num > 0:\n            prev_diff = self.memory.scores[-2][0] - self.memory.scores[-2][1]\n            curr_diff = self.memory.scores[-1][0] - self.memory.scores[-1][1]\n            return (prev_diff * curr_diff) > 0\n        return False\n\n\n    def _prepare_results(self) -> Dict:\n        \"\"\"Prepare the final results dictionary.\"\"\"\n        avg_scores = tuple(\n            sum(scores[i] for scores in self.memory.scores) / len(self.memory.scores)\n            for i in range(2)\n        )\n        \n        return {\n            \"winner\": 1 if avg_scores[0] > avg_scores[1] else 2,\n            \"average_scores\": avg_scores,\n            \"rounds\": len(self.memory.scores),\n            \"score_history\": self.memory.scores,\n            \"argument_history\": self.memory.arguments,\n            \"feedback_history\": self.memory.feedback\n        }\n```\n:::\n\n\n# Demonstration\n\nHere's a demonstration of the SAMRE method using a test case wherein the question is \"What are the implications of quantum computing for cybersecurity?\" to which two responses were generated by GPT-4o.\n\n::: {#a394df16 .cell execution_count=2}\n``` {.python .cell-code .code-overflow-wrap}\n# Initialize evaluator\nevaluator = SAMREEvaluator(model=\"gpt-4o\")\n\n# Define the question and answers\nquestion = \"What are the implications of quantum computing for cybersecurity? Respond in 80 words or less.\"\n\nanswer1 = \"Quantum computing poses a major threat to cybersecurity by potentially breaking widely used encryption methods, such as RSA and ECC, through algorithms like Shor's. It could render current cryptographic protections obsolete, exposing sensitive data. However, it also enables quantum-resistant cryptography and advanced security techniques, such as quantum key distribution (QKD), which offer near-unbreakable encryption. Preparing for this shift requires adopting post-quantum cryptography and upgrading current systems to remain secure against future quantum threats.\"\n\nanswer2 = \"Quantum computing could revolutionize cybersecurity by enabling faster detection of cyber threats and improving optimization in defensive measures. However, it also risks undermining blockchain integrity and breaking current hashing algorithms, threatening digital signatures and data integrity. Quantum algorithms may facilitate advanced cyberattacks, making traditional defenses inadequate. Organizations must prioritize research in quantum-safe solutions, such as lattice-based cryptography, while leveraging quantum technology for enhanced threat modeling and secure communication systems, ensuring a balance between risks and opportunities.\"\n\nresult = evaluator.evaluate(question, answer1, answer2)\n\nprint(f\"Winner: Answer {result['winner']}\")\nprint(f\"Average scores: {result['average_scores']}\")\nprint(f\"Number of rounds: {result['rounds']}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n=== Starting SAMRE Evaluation ===\n\n\n--- Round 1 ---\n\nAdvocate 1's defense:\nQuantum computing threatens cybersecurity by potentially breaking encryption\nlike RSA and ECC with Shor's algorithm, risking data exposure. However, it also\noffers solutions like quantum-resistant cryptography and quantum key\ndistribution (QKD) for near-unbreakable security. Preparing for this shift\ninvolves adopting post-quantum cryptography and upgrading systems to counter\nfuture quantum threats, ensuring data remains secure. This dual nature\nhighlights both the risks and the potential for enhanced security in the quantum\nera.\n\nAdvocate 2's defense:\nQuantum computing's dual impact on cybersecurity includes both risks and\nopportunities. While it threatens current encryption and blockchain integrity,\nit also offers enhanced threat detection and optimization of defenses. Unlike\nthe narrow focus on encryption threats, our perspective emphasizes a balanced\napproach, advocating for quantum-safe solutions like lattice-based cryptography\nand leveraging quantum technology for secure communication. This comprehensive\nview ensures organizations are prepared for both the challenges and benefits of\nquantum advancements.\n\nJudge's feedback:\nAnswer 1: Clarify how quantum-resistant cryptography differs from current\nmethods and provide examples. Emphasize the urgency of transitioning to post-\nquantum cryptography. Highlight the potential timeline for quantum threats to\nbecome significant.\n\nAnswer 2: Expand on how quantum computing enhances threat detection and\noptimization. Provide specific examples of quantum-safe solutions. Discuss the\ntimeline for when quantum computing might realistically impact cybersecurity.\nScores for this round: Answer 1 = 18.0, Answer 2 = 17.0\n\n--- Round 2 ---\n\nAdvocate 1's defense:\nQuantum computing threatens cybersecurity by potentially breaking current\nencryption methods like RSA and ECC with Shor's algorithm, risking data\nexposure. Quantum-resistant cryptography, such as lattice-based methods, and\nquantum key distribution (QKD) offer near-unbreakable security. The urgency to\ntransition to post-quantum cryptography is critical as quantum threats could\nemerge within the next decade. This dual nature highlights both the risks and\nthe potential for enhanced security, necessitating immediate upgrades to\nsafeguard against future quantum threats.\n\nAdvocate 2's defense:\nQuantum computing's dual impact on cybersecurity includes both risks and\nopportunities. It enhances threat detection and optimizes defenses, while also\nthreatening current encryption and blockchain integrity. Unlike a narrow focus\non encryption threats, our perspective emphasizes a balanced approach,\nadvocating for quantum-safe solutions like lattice-based cryptography and\nleveraging quantum technology for secure communication. Organizations must\nprioritize research in these areas to ensure preparedness for both the\nchallenges and benefits of quantum advancements, with significant impacts\nexpected within the next decade.\n\nJudge's feedback:\nAnswer 1: Emphasize the urgency of transitioning to post-quantum cryptography by\nproviding a timeline for potential quantum threats. Highlight specific quantum-\nresistant cryptographic methods beyond QKD to strengthen your argument. Clarify\nhow organizations can practically implement these solutions to prepare for\nfuture challenges.\n\nAnswer 2: Expand on how quantum computing can specifically enhance threat\ndetection and optimization. Provide examples of quantum-safe solutions and their\ncurrent development status. Clarify how organizations can balance leveraging\nquantum technology while mitigating risks, offering practical steps for\nimplementation.\nScores for this round: Answer 1 = 18.0, Answer 2 = 17.0\n\nScores have converged - ending debate early.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWinner: Answer 1\nAverage scores: (18.0, 17.0)\nNumber of rounds: 2\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}