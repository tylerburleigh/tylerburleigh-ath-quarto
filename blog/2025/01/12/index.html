<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tyler Burleigh">
<meta name="description" content="In this post, I re-evaluate a method that was recently published in arXiv, critiquing their baseline model and then designing a new baseline model that implements standard best practices for comparison with the new method. I find that the new evaluation method proposed in the paper does not perform better than this robust baseline. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as being skeptical of claims in research papers that compare new methods to baseline.">

<title>Challenging SAMRE: Comparing multi-round debate-style LLM evaluation to a robust (and much simpler) baseline | Tyler Burleigh – Tyler Burleigh</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../..//files/favico.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dd08061cb7210c315e315379d94beb87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-1f84303a6b4423b6cbd3f2cacdf6faa2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PRHQZ8HPLB"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-PRHQZ8HPLB', { 'anonymize_ip': true});
</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background: #170C3A;
      }
</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Tyler Burleigh</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../cv/index.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../research/index.html"> 
<span class="menu-text">Research</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://fosstodon.org/users/tylerburleigh" rel="me"> <i class="bi bi-mastodon" role="img" aria-label="mastodon">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/tylerburleigh.bsky.social" rel="me"> <i class="bi bi-square" role="img" aria-label="bluesky">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tylerburleigh" rel="me"> <i class="bi bi-github" role="img" aria-label="github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tylerburleigh" rel="me"> <i class="bi bi-linkedin" role="img" aria-label="linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tylerburleigh@gmail.com" rel="me"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default blog-post page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Challenging SAMRE: Comparing multi-round debate-style LLM evaluation to a robust (and much simpler) baseline</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          In this post, I re-evaluate a method that was recently published in arXiv, critiquing their baseline model and then designing a new baseline model that implements standard best practices for comparison with the new method. I find that the new evaluation method proposed in the paper does not perform better than this robust baseline. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as being skeptical of claims in research papers that compare new methods to baseline.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">prompt-engineering</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">LLM-as-judge</div>
                <div class="quarto-category">LLM-evals</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://www.tylerburleigh.com/">Tyler Burleigh</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Sunday, January 12, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#tldr-what-i-did-and-what-i-found" id="toc-tldr-what-i-did-and-what-i-found" class="nav-link active" data-scroll-target="#tldr-what-i-did-and-what-i-found">TL;DR: What I did and what I found</a></li>
  <li><a href="#baseline-model-prompt-inadequacies" id="toc-baseline-model-prompt-inadequacies" class="nav-link" data-scroll-target="#baseline-model-prompt-inadequacies">Baseline model prompt inadequacies</a></li>
  <li><a href="#hypothesis-and-predictions" id="toc-hypothesis-and-predictions" class="nav-link" data-scroll-target="#hypothesis-and-predictions">Hypothesis and predictions</a></li>
  <li><a href="#my-implementation-of-samre-and-baseline" id="toc-my-implementation-of-samre-and-baseline" class="nav-link" data-scroll-target="#my-implementation-of-samre-and-baseline">My implementation of SAMRE and Baseline</a></li>
  <li><a href="#load-the-mt-bench-dataset" id="toc-load-the-mt-bench-dataset" class="nav-link" data-scroll-target="#load-the-mt-bench-dataset">Load the MT-Bench dataset</a></li>
  <li><a href="#use-methods-to-evaluate-mt-bench-dataset" id="toc-use-methods-to-evaluate-mt-bench-dataset" class="nav-link" data-scroll-target="#use-methods-to-evaluate-mt-bench-dataset">Use methods to evaluate MT-Bench dataset</a></li>
  <li><a href="#performance-evaluation" id="toc-performance-evaluation" class="nav-link" data-scroll-target="#performance-evaluation">Performance evaluation</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>I’ve been doing a lot of work with LLM-based evaluations lately, and I’ve been thinking about how to improve the quality of these evaluations.</p>
<p>I like to read research papers from arXiv for inspiration, and I recently came across a paper called <a href="https://arxiv.org/abs/2410.04663">Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates</a>, which introduces a new method inspired by judicial process called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process.</p>
<p>I was initially impressed by the results, which reported a gain of ~6-8% over baseline. Below I’ve reproduced an excerpt from one of the tables in the paper showing their results.</p>
<table class="caption-top table">
<caption>Excerpt from “Table 2: Performance Gains Compared to Baseline”</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>SAMRE w/o Juries</th>
<th>SAMRE w/o Juries (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Llama-3-8B</td>
<td>0.05</td>
<td>6.3%</td>
</tr>
<tr class="even">
<td>Qwen</td>
<td>0.06</td>
<td>7.3%</td>
</tr>
<tr class="odd">
<td>Gemini</td>
<td>0.06</td>
<td>7.2%</td>
</tr>
<tr class="even">
<td>GPT-4-o</td>
<td>0.07</td>
<td>8.3%</td>
</tr>
<tr class="odd">
<td>GPT-4-turbo</td>
<td>0.07</td>
<td>8.2%</td>
</tr>
<tr class="even">
<td>GPT-3.5-turbo</td>
<td>0.05</td>
<td>6.2%</td>
</tr>
</tbody>
</table>
<p><em>Note that the authors had tested versions of SAMRE with and without the addition of “juries”. In the table I’ve included only the version without juries, as it was both simpler and more performant. It is also this more performant version without juries that I am interested in testing. So with that said, in this blog post when I mention “SAMRE”, I will be referring to the version without juries.</em></p>
<p>Despite the impressive results reported in the paper, I am often skeptical when researchers claim to have found that new methods outperform “baseline” models. I have observed that researchers often fail to implement standard best practices in their baseline models, and so their results are therefore not represenative of true gains over baseline. It is as if they are knocking down a straw man.</p>
<p>Given this skepticism of mine, I decided that it might be interesting to put it this skepticism the test: What if I implemented the SAMRE method (again, note that I am referring to the version without juries), and compared it to a baseline model that does implement standard best practices for prompt engineering? Would I find that the SAMRE method is indeed an improvement over the baseline? Or would I find that SAMRE is inferior to a properly implemented baseline?</p>
<section id="tldr-what-i-did-and-what-i-found" class="level2">
<h2 class="anchored" data-anchor-id="tldr-what-i-did-and-what-i-found">TL;DR: What I did and what I found</h2>
<p>I tested three model variants:</p>
<ol type="1">
<li>SAMRE, as implemented in the paper (without juries)</li>
<li>Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)</li>
<li>Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.</li>
</ol>
<p>I evaluated each of these models using a sample of 300 conversations from MT-Bench for testing and evaluation. (MT-Bench was used in the original paper as well.)</p>
<p>After running the evaluations and calculating Krippendorff alpha agreement with human judge ground truth, I found that although SAMRE did yield better agreement than Baseline-Weak more importantly it was inferior to Baseline-Strong – and by a fair margin. A similar result was found when examining binary classification accuracy using Matthews Correlation Coefficient (MCC).</p>
<p>These results serve to highlight the importance of implementing standard best practices in baseline models, as well as being skeptical of claims in research papers that compare new methods to a “baseline model”. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.</p>
</section>
<section id="baseline-model-prompt-inadequacies" class="level1">
<h1>Baseline model prompt inadequacies</h1>
<p>Here I will consider some of the inadequacies in the Baseline model’s prompt reported in the paper, and share a version of the prompt that addresses these inadequacies and implements standard best practices.</p>
<p>The “baseline” prompt used by the authors of the paper was as follows:</p>
<pre class="prompt"><code>You are a fair, impartial judge scoring a debate on the following question:
question.
Answer 1: answer_1
Answer 2: answer_2
Score each answer on a scale of 1-20 for each of the following criteria:
1. Relevance to the question
2. Accuracy of information and use of credible sources
3. Depth of analysis and completeness of argument
4. Clarity of expression and logical flow
5. Strength of reasoning and factual support
6. Effectiveness in addressing opponent’s points
Provide scores as [answer_1_score, answer_2_score] for each criterion in a list format, then sum for final scores. Please keep an eye on the slightest difference that should make a difference in the scoring. Don’t overthink!
Relevance:
Accuracy:
Depth:
Clarity:
Logic and Factuality:
Addressing opponent’s points:
Final Scores (sum of above) as a tuple (example: (18, 9)):
Explain your scoring, focusing on why one answer is better than the other based on the criteria above. Keep your explanation concise but informative.
Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).
Example: (18, 9)
Your scores and explanation:</code></pre>
<p>Here are the issues I see with this prompt:</p>
<ol type="1">
<li><p>The prompt does not use delimiters for most of the inputs. I would enclose the inputs inside XML tags like <code>&lt;Question&gt;&lt;/Question&gt;</code>, <code>&lt;Answer1&gt;&lt;/Answer1&gt;</code>, and <code>&lt;Answer2&gt;&lt;/Answer2&gt;</code>, but in a pinch delimiters like triple backticks can be used.</p></li>
<li><p>The prompt instructs the model to first generate scores in list format, and then to sum them. But as we know, language models models often make arithmetic mistakes. It would be better to ask the model to generate scores for each criterion, and then to programmatically extract and summarize them in python (or another programming language) from which the routine is run.</p></li>
<li><p>Although the prompt asks the model to “explain your scoring”, it is not clear if the model should be reasoning about each criterion before it scores them, or if it should provide reasoning at the end when giving its final score. I would ask the model to provide reasoning for each criterion that it is asked to score, and ask it to reason before scoring.</p></li>
<li><p>It’s unclear why a scale of 1-20 is used. This is not a standard scale for scoring. I would use a scale of 1-10 which is likely more familiar to the model and can be expected to be used more consistently.</p></li>
<li><p>Although the prompt does suggest that the model provide its scores in tuple format, it would be better to provide more explicit format instructions.</p></li>
<li><p>The prompt includes an “Effectiveness in addressing opponent’s points” criterion, but this is almost certainly irrelevant given that the answers to the question were not generated with the goal of addressing an opponent.</p></li>
<li><p>Finally, although this goes beyond the prompt itself, the authors of the paper are comparing a multi-round method to a single-round method. This is obviously an unfair comparison. Instead, it would be better to compare the SAMRE method to a baseline that uses the same number of rounds and then similarly averages its scores.</p></li>
</ol>
<p>With all of that in mind, here’s how I would rewrite the prompt:</p>
<pre class="prompt"><code>You are a fair, impartial judge scoring a debate on Question.

&lt;Question&gt;
{question}
&lt;/Question&gt;

Two Answers have been given to the Question.

&lt;Answer1&gt;
{answer_1}
&lt;/Answer1&gt;

&lt;Answer2&gt;
{answer_2}
&lt;/Answer2&gt;

The Answers are being judged on the following Criteria:

&lt;Criteria&gt;
&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;
&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;
&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;
&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;
&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;
&lt;/Criteria&gt;

For each Criterion, briefly analyze the performance of 
the two Answers, then give a score between 1 and 10.

Respond as follows:
&lt;Criterion1&gt;
&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;
&lt;Analysis&gt;
Answer 1: [Analysis of Answer 1 performance on the Criterion]
Answer 2: [Analysis of Answer 2 performance on the Criterion]
&lt;/Analysis&gt;
&lt;Scores&gt;
&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;
&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;
&lt;/Scores&gt;
&lt;/Criterion1&gt;
&lt;Criterion2&gt;
&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;
&lt;Analysis&gt;
Answer 1: [Analysis of Answer 1 performance on the Criterion]
Answer 2: [Analysis of Answer 2 performance on the Criterion]
&lt;/Analysis&gt;
&lt;Scores&gt;
&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;
&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;
&lt;/Scores&gt;
&lt;/Criterion2&gt;
...</code></pre>
<p>Notice that the prompt now uses XML tags to structure the instructions, that it asks the model to provide reasoning for each criterion before scoring, and that it gives the model a clear format for its response that reinforces analysis before scoring for each criterion.</p>
<p>I’ve also changed the scale from 1-20 to 1-10, removed the unnecessary “Effectiveness in addressing opponent’s points” criterion, and removed the instruction to summarize the scores, as I would handle this within the code.</p>
<p><em>Note the baseline could be improved even further by requesting the structured output using a mode like OpenAI’s Structured Outputs. This would increase the likelihood of the model responding in the desired format. For this test, I will not be using structured outputs.</em></p>
</section>
<section id="hypothesis-and-predictions" class="level1">
<h1>Hypothesis and predictions</h1>
<p>I hypothesize that SAMRE will NOT perform better than a baseline model that implements standard best practices for prompt engineering.</p>
<p>My predictions are as follows:</p>
<ol type="1">
<li>SAMRE will perform better than Baseline-Weak, as this was what the authors of the paper found and by implementing these methods faithfully from the paper, I can expect to replicate their results.</li>
<li>Baseline-Strong will perform better than Baseline-Weak, since the “Strong” variant implements best practices and can be expected to perform better than the “Weak” variant.</li>
<li>Baseline-Strong will perform equal to or better than SAMRE, as the best practices implemented in Baseline-Strong will close the gap that exists between SAMRE and Baseline-Weak due to the inadequacies in the Baseline-Weak prompt.</li>
</ol>
</section>
<section id="my-implementation-of-samre-and-baseline" class="level1">
<h1>My implementation of SAMRE and Baseline</h1>
<p>Okay, so with those criticisms out of the way, let’s design evaluators to implement three methods:</p>
<ol type="1">
<li>SAMRE, as implemented in the paper (without juries)</li>
<li>Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)</li>
<li>Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.</li>
</ol>
<p>Below is my python implementation of these evaluators. To the best of my ability, I have implemented the SAMRE and Baseline methods as described in the paper (I call the paper’s Baseline method “Baseline-Weak”). And I have implemented a Baseline-Strong evaluator that implements what I would consider to be standard best practices for prompt engineering, as discussed in an earlier section.</p>
<div id="a0e049f9" class="cell" data-code-fold-show="false" data-execution_count="1">
<details class="code-fold">
<summary>Click to view the implementation code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass, field</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple, Dict</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> AsyncOpenAI</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> asynccontextmanager</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the prompts</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>PROMPTS <span class="op">=</span> {</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for advocates to defend their assigned answer, used in SAMRE</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="st">"defend_prompt"</span>: <span class="st">"""</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="st">You’re an eloquent advocate (Advocate </span><span class="sc">{advocate_id}</span><span class="st">) in a group defending this answer:</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer}</span><span class="st"> to the question: </span><span class="sc">{question}</span><span class="st">. Your opponent group defends: </span><span class="sc">{opponent_answer}</span><span class="st">.</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="st">Collaborate with your fellow advocates to convince the audience your argument is</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="st">better. Use the latest feedback, your opponent’s last argument, and your team’s previous</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="st">arguments to improve your case.</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="st">Latest feedback: </span><span class="sc">{feedback}</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="st">Opponent’s last argument: </span><span class="sc">{opponent_argument}</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="st">Your team’s previous arguments: team_arguments</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="st">Respond in under 80 words.</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="st">Your defense:</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for judge to provide feedback on debate progress, used in SAMRE</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="st">"judge_prompt"</span>: <span class="st">"""</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="st">You’re a fair, impartial judge in a debate on: "</span><span class="sc">{question}</span><span class="st">". Answer 1: "</span><span class="sc">{answer_1}</span><span class="st">".</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: "</span><span class="sc">{answer_2}</span><span class="st">". Your goal is to provide feedback that will help advocate groups</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="st">improve and differentiate their arguments more clearly.</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="st">Current round: </span><span class="sc">{current_round}</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="st">Total rounds: </span><span class="sc">{total_rounds}</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="st">Previous scores: </span><span class="sc">{previous_scores}</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 1st answer: </span><span class="sc">{defense_1}</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 2nd answer: </span><span class="sc">{defense_2}</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="st">Provide specific, constructive feedback to help each advocate group strengthen their</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="st">unique position. Encourage them to address weaknesses and highlight distinctions. Aim</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="st">for your feedback to lead to more divergent scores in future rounds.</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="st">Give your feedback in under 50 words:</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for SAMRE method scoring</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_samre"</span>: <span class="st">"""</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="st">You’re a critical, impartial judge in a high-stakes debate on: "</span><span class="sc">{question}</span><span class="st">". Answer</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="st">1: "</span><span class="sc">{answer_1}</span><span class="st">". Answer 2: "</span><span class="sc">{answer_2}</span><span class="st">". Your goal is to provide detailed, constructive</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="st">feedback that will push advocates to significantly improve their arguments.</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="st">Total rounds: </span><span class="sc">{total_rounds}</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="st">Previous scores: </span><span class="sc">{previous_scores}</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 1st answer: </span><span class="sc">{defense_1}</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 2nd answer: </span><span class="sc">{defense_2}</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="st">Analyze each argument meticulously. Be thorough and unbiased in your assessment of:</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="st">1. Relevance to the question</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="st">2. Accuracy of information and use of credible sources</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="st">3. Depth of analysis and completeness of argument</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="st">4. Clarity of expression and logical flow</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="st">5. Strength of reasoning and factual support</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="st">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="st">For each criterion, provide a score on a scale of 1-20 and detailed justification.</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="st">Scores should be given as [answer_1_score, answer_2_score] for each criterion.</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="st">Your comprehensive feedback for each advocate (50 words each):</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="st">Feedback for Advocate 1:</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="st">Feedback for Advocate 2:</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a><span class="st">Sum up the scores and return the final score tuple (score1, score2). Example: (95, 87)</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a><span class="st">Your detailed scores and final tally:</span></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for Baseline-Weak method scoring, which represents the baseline model used in the paper</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_baseline_weak"</span>: <span class="st">"""</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="st">You are a fair, impartial judge scoring a debate on the following question:</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="st">question.</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: </span><span class="sc">{answer_1}</span></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: </span><span class="sc">{answer_2}</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="st">Score each answer on a scale of 1-20 for each of the following criteria:</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="st">1. Relevance to the question</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="st">2. Accuracy of information and use of credible sources</span></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a><span class="st">3. Depth of analysis and completeness of argument</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a><span class="st">4. Clarity of expression and logical flow</span></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a><span class="st">5. Strength of reasoning and factual support</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a><span class="st">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a><span class="st">Provide scores as [Answer1_score, Answer2_score] for each criterion in a list format,</span></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a><span class="st">then sum for final scores. Please keep an eye on the slightest difference that should</span></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a><span class="st">make a difference in the scoring. Don’t overthink!</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a><span class="st">Relevance:</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a><span class="st">Accuracy:</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a><span class="st">Depth:</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a><span class="st">Clarity:</span></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a><span class="st">Logic and Factuality:</span></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="st">Addressing opponent’s points:</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a><span class="st">Final Scores (sum of above) as a tuple (example: (18, 9)):</span></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a><span class="st">Explain your scoring, focusing on why one answer is better than the other based on the</span></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a><span class="st">criteria above. Keep your explanation concise but informative.</span></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a><span class="st">Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).</span></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a><span class="st">Example: (18, 9)</span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a><span class="st">Your scores and explanation:</span></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for Baseline-Strong method scoring, which implements what I consider to be standard best practices for prompt engineering</span></span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_baseline_strong"</span>: <span class="st">"""</span></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a><span class="st">You are a fair, impartial judge scoring a debate on Question.</span></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Question&gt;</span></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a><span class="sc">{question}</span></span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Question&gt;</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a><span class="st">Two Answers have been given to the Question.</span></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1&gt;</span></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_1}</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Answer1&gt;</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2&gt;</span></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_2}</span></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Answer2&gt;</span></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="st">The Answers are being judged on the following Criteria:</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criteria&gt;</span></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;</span></span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;</span></span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;</span></span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;</span></span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criteria&gt;</span></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a><span class="st">For each Criterion, briefly analyze the performance of </span></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a><span class="st">the two Answers, then give a score between 1 and 10.</span></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a><span class="st">Respond as follows:</span></span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion1&gt;</span></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;</span></span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Analysis&gt;</span></span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Analysis&gt;</span></span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Scores&gt;</span></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Scores&gt;</span></span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criterion1&gt;</span></span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion2&gt;</span></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;</span></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Analysis&gt;</span></span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Analysis&gt;</span></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Scores&gt;</span></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Scores&gt;</span></span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criterion2&gt;</span></span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a><span class="st">...</span></span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Memory:</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stores debate history including arguments, scores, and feedback for each round, used in SAMRE"""</span></span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>    arguments: List[Tuple[<span class="bu">str</span>, <span class="bu">str</span>]] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>    scores: List[Tuple[<span class="bu">float</span>, <span class="bu">float</span>]] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>    feedback: List[<span class="bu">str</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelEvaluator:</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>    <span class="at">@classmethod</span></span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>    <span class="at">@asynccontextmanager</span></span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> create(cls, mode<span class="op">=</span><span class="st">"samre"</span>, model<span class="op">=</span><span class="st">"gpt-4o-mini"</span>, logging_level<span class="op">=</span>logging.WARNING):</span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Factory method to create evaluator instance with proper async context management"""</span></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>        instance <span class="op">=</span> cls(mode<span class="op">=</span>mode, model<span class="op">=</span>model, logging_level<span class="op">=</span>logging_level)</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>        instance.client <span class="op">=</span> AsyncOpenAI()</span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> instance</span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>        <span class="cf">finally</span>:</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> instance.client.close()</span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _setup_logger(<span class="va">self</span>, logging_level):</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Setup logger with word wrapping."""</span></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>        logger <span class="op">=</span> logging.getLogger(<span class="va">__name__</span>)</span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>        logger.setLevel(logging_level)</span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> logger.handlers:</span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a>            handler <span class="op">=</span> logging.StreamHandler()</span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>            <span class="kw">class</span> WrapFormatter(logging.Formatter):</span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>                <span class="kw">def</span> <span class="bu">format</span>(<span class="va">self</span>, record):</span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>                    <span class="im">import</span> textwrap</span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>                    message <span class="op">=</span> <span class="bu">super</span>().<span class="bu">format</span>(record)</span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(textwrap.fill(line, width<span class="op">=</span><span class="dv">80</span>) </span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>                                <span class="cf">for</span> line <span class="kw">in</span> message.split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>))</span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a>            formatter <span class="op">=</span> WrapFormatter(<span class="st">'</span><span class="sc">%(message)s</span><span class="st">'</span>)</span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a>            handler.setFormatter(formatter)</span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>            logger.addHandler(handler)</span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logger</span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mode<span class="op">=</span><span class="st">"samre"</span>, model<span class="op">=</span><span class="st">"gpt-4o-mini"</span>, logging_level<span class="op">=</span>logging.WARNING):</span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mode <span class="op">=</span> mode</span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Modify to handle both baseline modes</span></span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_rounds <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> mode.startswith(<span class="st">"baseline"</span>) <span class="cf">else</span> <span class="dv">4</span></span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger <span class="op">=</span> <span class="va">self</span>._setup_logger(logging_level)</span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize all prompts</span></span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.defend_prompt <span class="op">=</span> PROMPTS[<span class="st">"defend_prompt"</span>]</span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.judge_prompt <span class="op">=</span> PROMPTS[<span class="st">"judge_prompt"</span>]</span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> get_completion(<span class="va">self</span>, prompt: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get a completion from the OpenAI API."""</span></span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.client:</span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'"</span>)</span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.client.chat.completions.create(</span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span><span class="va">self</span>.model,</span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: prompt}],</span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span><span class="dv">0</span></span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _extract_final_scores(<span class="va">self</span>, score_response: <span class="bu">str</span>) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extracts final scores from model response based on evaluation mode"""</span></span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"samre"</span>:</span>
<span id="cb3-214"><a href="#cb3-214" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Look for final tuple in format (score1, score2)</span></span>
<span id="cb3-215"><a href="#cb3-215" aria-hidden="true" tabindex="-1"></a>            tuple_pattern <span class="op">=</span> <span class="vs">r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'</span></span>
<span id="cb3-216"><a href="#cb3-216" aria-hidden="true" tabindex="-1"></a>            match <span class="op">=</span> re.search(tuple_pattern, score_response)</span>
<span id="cb3-217"><a href="#cb3-217" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match:</span>
<span id="cb3-218"><a href="#cb3-218" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> (<span class="bu">float</span>(match.group(<span class="dv">1</span>)), <span class="bu">float</span>(match.group(<span class="dv">2</span>)))</span>
<span id="cb3-219"><a href="#cb3-219" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find score tuple in SAMRE response"</span>)</span>
<span id="cb3-220"><a href="#cb3-220" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-221"><a href="#cb3-221" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_weak"</span>:</span>
<span id="cb3-222"><a href="#cb3-222" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Look for final tuple in format (score1, score2)</span></span>
<span id="cb3-223"><a href="#cb3-223" aria-hidden="true" tabindex="-1"></a>            tuple_pattern <span class="op">=</span> <span class="vs">r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'</span></span>
<span id="cb3-224"><a href="#cb3-224" aria-hidden="true" tabindex="-1"></a>            match <span class="op">=</span> re.search(tuple_pattern, score_response)</span>
<span id="cb3-225"><a href="#cb3-225" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match:</span>
<span id="cb3-226"><a href="#cb3-226" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> (<span class="bu">float</span>(match.group(<span class="dv">1</span>)), <span class="bu">float</span>(match.group(<span class="dv">2</span>)))</span>
<span id="cb3-227"><a href="#cb3-227" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find score tuple in weak baseline response"</span>)</span>
<span id="cb3-228"><a href="#cb3-228" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-229"><a href="#cb3-229" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_strong"</span>:</span>
<span id="cb3-230"><a href="#cb3-230" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use XML parsing for strong baseline</span></span>
<span id="cb3-231"><a href="#cb3-231" aria-hidden="true" tabindex="-1"></a>            score_a_pattern <span class="op">=</span> <span class="vs">r'&lt;Answer1Score&gt;\s*(\d+\.?\d*)\s*&lt;/Answer1Score&gt;'</span></span>
<span id="cb3-232"><a href="#cb3-232" aria-hidden="true" tabindex="-1"></a>            score_b_pattern <span class="op">=</span> <span class="vs">r'&lt;Answer2Score&gt;\s*(\d+\.?\d*)\s*&lt;/Answer2Score&gt;'</span></span>
<span id="cb3-233"><a href="#cb3-233" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-234"><a href="#cb3-234" aria-hidden="true" tabindex="-1"></a>            scores_a <span class="op">=</span> [<span class="bu">float</span>(match.group(<span class="dv">1</span>)) <span class="cf">for</span> match <span class="kw">in</span> re.finditer(score_a_pattern, score_response)]</span>
<span id="cb3-235"><a href="#cb3-235" aria-hidden="true" tabindex="-1"></a>            scores_b <span class="op">=</span> [<span class="bu">float</span>(match.group(<span class="dv">1</span>)) <span class="cf">for</span> match <span class="kw">in</span> re.finditer(score_b_pattern, score_response)]</span>
<span id="cb3-236"><a href="#cb3-236" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-237"><a href="#cb3-237" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> scores_a <span class="kw">or</span> <span class="kw">not</span> scores_b:</span>
<span id="cb3-238"><a href="#cb3-238" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find scores for both candidates"</span>)</span>
<span id="cb3-239"><a href="#cb3-239" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-240"><a href="#cb3-240" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(scores_a) <span class="op">!=</span> <span class="bu">len</span>(scores_b):</span>
<span id="cb3-241"><a href="#cb3-241" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Mismatched number of scores: A=</span><span class="sc">{</span><span class="bu">len</span>(scores_a)<span class="sc">}</span><span class="ss">, B=</span><span class="sc">{</span><span class="bu">len</span>(scores_b)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-242"><a href="#cb3-242" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-243"><a href="#cb3-243" aria-hidden="true" tabindex="-1"></a>            final_score_a <span class="op">=</span> <span class="bu">sum</span>(scores_a) <span class="op">/</span> <span class="bu">len</span>(scores_a)</span>
<span id="cb3-244"><a href="#cb3-244" aria-hidden="true" tabindex="-1"></a>            final_score_b <span class="op">=</span> <span class="bu">sum</span>(scores_b) <span class="op">/</span> <span class="bu">len</span>(scores_b)</span>
<span id="cb3-245"><a href="#cb3-245" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-246"><a href="#cb3-246" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (final_score_a, final_score_b)</span>
<span id="cb3-247"><a href="#cb3-247" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-248"><a href="#cb3-248" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-249"><a href="#cb3-249" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Unknown mode: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>mode<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-250"><a href="#cb3-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-251"><a href="#cb3-251" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> evaluate(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, num_rounds: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb3-252"><a href="#cb3-252" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Main evaluation entry point that routes to appropriate evaluation method based on mode"""</span></span>
<span id="cb3-253"><a href="#cb3-253" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.client:</span>
<span id="cb3-254"><a href="#cb3-254" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'"</span>)</span>
<span id="cb3-255"><a href="#cb3-255" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-256"><a href="#cb3-256" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mode.startswith(<span class="st">"baseline"</span>):</span>
<span id="cb3-257"><a href="#cb3-257" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">=== Starting </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>mode<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Evaluation ===</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb3-258"><a href="#cb3-258" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>._evaluate_baseline(question, answer_1, answer_2, num_rounds)</span>
<span id="cb3-259"><a href="#cb3-259" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-260"><a href="#cb3-260" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting SAMRE Evaluation ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-261"><a href="#cb3-261" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>._evaluate_samre(question, answer_1, answer_2)</span>
<span id="cb3-262"><a href="#cb3-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-263"><a href="#cb3-263" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _evaluate_baseline(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, num_rounds: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb3-264"><a href="#cb3-264" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implements baseline evaluation methods (both weak and strong)"""</span></span>
<span id="cb3-265"><a href="#cb3-265" aria-hidden="true" tabindex="-1"></a>        score_history <span class="op">=</span> []</span>
<span id="cb3-266"><a href="#cb3-266" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-267"><a href="#cb3-267" aria-hidden="true" tabindex="-1"></a>        num_rounds <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_weak"</span> <span class="cf">else</span> num_rounds</span>
<span id="cb3-268"><a href="#cb3-268" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_rounds):</span>
<span id="cb3-269"><a href="#cb3-269" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Select appropriate prompt based on mode</span></span>
<span id="cb3-270"><a href="#cb3-270" aria-hidden="true" tabindex="-1"></a>            prompt_key <span class="op">=</span> <span class="st">"score_prompt_"</span> <span class="op">+</span> <span class="va">self</span>.mode</span>
<span id="cb3-271"><a href="#cb3-271" aria-hidden="true" tabindex="-1"></a>            score_prompt <span class="op">=</span> PROMPTS[prompt_key].<span class="bu">format</span>(</span>
<span id="cb3-272"><a href="#cb3-272" aria-hidden="true" tabindex="-1"></a>                question<span class="op">=</span>question,</span>
<span id="cb3-273"><a href="#cb3-273" aria-hidden="true" tabindex="-1"></a>                answer_1<span class="op">=</span>answer_1,</span>
<span id="cb3-274"><a href="#cb3-274" aria-hidden="true" tabindex="-1"></a>                answer_2<span class="op">=</span>answer_2</span>
<span id="cb3-275"><a href="#cb3-275" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-276"><a href="#cb3-276" aria-hidden="true" tabindex="-1"></a>            score_response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(score_prompt)</span>
<span id="cb3-277"><a href="#cb3-277" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"Score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-278"><a href="#cb3-278" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-279"><a href="#cb3-279" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb3-280"><a href="#cb3-280" aria-hidden="true" tabindex="-1"></a>                round_scores <span class="op">=</span> <span class="va">self</span>._extract_final_scores(score_response)</span>
<span id="cb3-281"><a href="#cb3-281" aria-hidden="true" tabindex="-1"></a>                score_history.append(<span class="bu">list</span>(round_scores))</span>
<span id="cb3-282"><a href="#cb3-282" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb3-283"><a href="#cb3-283" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.error(<span class="ss">f"Score parsing error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-284"><a href="#cb3-284" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.error(<span class="ss">f"Raw score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-285"><a href="#cb3-285" aria-hidden="true" tabindex="-1"></a>                score_history.append([<span class="fl">10.0</span>, <span class="fl">10.0</span>])</span>
<span id="cb3-286"><a href="#cb3-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-287"><a href="#cb3-287" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate average scores across all rounds</span></span>
<span id="cb3-288"><a href="#cb3-288" aria-hidden="true" tabindex="-1"></a>        avg_scores <span class="op">=</span> [</span>
<span id="cb3-289"><a href="#cb3-289" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span>(scores[i] <span class="cf">for</span> scores <span class="kw">in</span> score_history) <span class="op">/</span> <span class="bu">len</span>(score_history)</span>
<span id="cb3-290"><a href="#cb3-290" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb3-291"><a href="#cb3-291" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-292"><a href="#cb3-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-293"><a href="#cb3-293" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Determine winner based on average scores</span></span>
<span id="cb3-294"><a href="#cb3-294" aria-hidden="true" tabindex="-1"></a>        winner <span class="op">=</span> (</span>
<span id="cb3-295"><a href="#cb3-295" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&gt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb3-296"><a href="#cb3-296" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&lt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb3-297"><a href="#cb3-297" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'tie'</span></span>
<span id="cb3-298"><a href="#cb3-298" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-299"><a href="#cb3-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-300"><a href="#cb3-300" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb3-301"><a href="#cb3-301" aria-hidden="true" tabindex="-1"></a>            <span class="st">"winner"</span>: winner,</span>
<span id="cb3-302"><a href="#cb3-302" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_scores"</span>: [<span class="bu">round</span>(score, <span class="dv">2</span>) <span class="cf">for</span> score <span class="kw">in</span> avg_scores] ,</span>
<span id="cb3-303"><a href="#cb3-303" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rounds"</span>: <span class="bu">len</span>(score_history),</span>
<span id="cb3-304"><a href="#cb3-304" aria-hidden="true" tabindex="-1"></a>            <span class="st">"score_history"</span>: score_history,</span>
<span id="cb3-305"><a href="#cb3-305" aria-hidden="true" tabindex="-1"></a>            <span class="st">"full_response"</span>: score_response  <span class="co"># Include the final response for analysis</span></span>
<span id="cb3-306"><a href="#cb3-306" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-307"><a href="#cb3-307" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-308"><a href="#cb3-308" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _evaluate_samre(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb3-309"><a href="#cb3-309" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implements SAMRE evaluation with multi-round debate process</span></span>
<span id="cb3-310"><a href="#cb3-310" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-311"><a href="#cb3-311" aria-hidden="true" tabindex="-1"></a><span class="co">        Flow:</span></span>
<span id="cb3-312"><a href="#cb3-312" aria-hidden="true" tabindex="-1"></a><span class="co">        1. Get defenses from both advocates</span></span>
<span id="cb3-313"><a href="#cb3-313" aria-hidden="true" tabindex="-1"></a><span class="co">        2. Judge provides feedback and scores</span></span>
<span id="cb3-314"><a href="#cb3-314" aria-hidden="true" tabindex="-1"></a><span class="co">        3. Repeat until max rounds or convergence</span></span>
<span id="cb3-315"><a href="#cb3-315" aria-hidden="true" tabindex="-1"></a><span class="co">        4. Return averaged results</span></span>
<span id="cb3-316"><a href="#cb3-316" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-317"><a href="#cb3-317" aria-hidden="true" tabindex="-1"></a>        local_memory <span class="op">=</span> Memory()</span>
<span id="cb3-318"><a href="#cb3-318" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-319"><a href="#cb3-319" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting SAMRE Evaluation ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-320"><a href="#cb3-320" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-321"><a href="#cb3-321" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> round_num <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.max_rounds):</span>
<span id="cb3-322"><a href="#cb3-322" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Round </span><span class="sc">{</span>round_num <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb3-323"><a href="#cb3-323" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-324"><a href="#cb3-324" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>._run_debate_round(</span>
<span id="cb3-325"><a href="#cb3-325" aria-hidden="true" tabindex="-1"></a>                question,</span>
<span id="cb3-326"><a href="#cb3-326" aria-hidden="true" tabindex="-1"></a>                answer_1, </span>
<span id="cb3-327"><a href="#cb3-327" aria-hidden="true" tabindex="-1"></a>                answer_2, </span>
<span id="cb3-328"><a href="#cb3-328" aria-hidden="true" tabindex="-1"></a>                round_num,</span>
<span id="cb3-329"><a href="#cb3-329" aria-hidden="true" tabindex="-1"></a>                local_memory</span>
<span id="cb3-330"><a href="#cb3-330" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-331"><a href="#cb3-331" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-332"><a href="#cb3-332" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._has_scores_converged(round_num, local_memory):</span>
<span id="cb3-333"><a href="#cb3-333" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">Scores have converged - ending debate early."</span>)</span>
<span id="cb3-334"><a href="#cb3-334" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb3-335"><a href="#cb3-335" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-336"><a href="#cb3-336" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._prepare_results(local_memory)</span>
<span id="cb3-337"><a href="#cb3-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-338"><a href="#cb3-338" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> defend_answer(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, </span>
<span id="cb3-339"><a href="#cb3-339" aria-hidden="true" tabindex="-1"></a>                        advocate_id: <span class="bu">int</span>, feedback: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>, </span>
<span id="cb3-340"><a href="#cb3-340" aria-hidden="true" tabindex="-1"></a>                        opponent_argument: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>,</span>
<span id="cb3-341"><a href="#cb3-341" aria-hidden="true" tabindex="-1"></a>                        team_arguments: List[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb3-342"><a href="#cb3-342" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get defense from an advocate.</span></span>
<span id="cb3-343"><a href="#cb3-343" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-344"><a href="#cb3-344" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb3-345"><a href="#cb3-345" aria-hidden="true" tabindex="-1"></a><span class="co">            question: The question being debated</span></span>
<span id="cb3-346"><a href="#cb3-346" aria-hidden="true" tabindex="-1"></a><span class="co">            answer_1: First answer in the debate</span></span>
<span id="cb3-347"><a href="#cb3-347" aria-hidden="true" tabindex="-1"></a><span class="co">            answer_2: Second answer in the debate</span></span>
<span id="cb3-348"><a href="#cb3-348" aria-hidden="true" tabindex="-1"></a><span class="co">            advocate_id: Which advocate (1 or 2) is defending</span></span>
<span id="cb3-349"><a href="#cb3-349" aria-hidden="true" tabindex="-1"></a><span class="co">            feedback: Previous feedback from judge</span></span>
<span id="cb3-350"><a href="#cb3-350" aria-hidden="true" tabindex="-1"></a><span class="co">            opponent_argument: Last argument from opponent</span></span>
<span id="cb3-351"><a href="#cb3-351" aria-hidden="true" tabindex="-1"></a><span class="co">            team_arguments: List of previous arguments from this advocate's team</span></span>
<span id="cb3-352"><a href="#cb3-352" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-353"><a href="#cb3-353" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> team_arguments <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-354"><a href="#cb3-354" aria-hidden="true" tabindex="-1"></a>            team_arguments <span class="op">=</span> []</span>
<span id="cb3-355"><a href="#cb3-355" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-356"><a href="#cb3-356" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Map answers based on advocate_id</span></span>
<span id="cb3-357"><a href="#cb3-357" aria-hidden="true" tabindex="-1"></a>        answer <span class="op">=</span> answer_1 <span class="cf">if</span> advocate_id <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> answer_2</span>
<span id="cb3-358"><a href="#cb3-358" aria-hidden="true" tabindex="-1"></a>        opponent_answer <span class="op">=</span> answer_2 <span class="cf">if</span> advocate_id <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> answer_1</span>
<span id="cb3-359"><a href="#cb3-359" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-360"><a href="#cb3-360" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> <span class="va">self</span>.defend_prompt.<span class="bu">format</span>(</span>
<span id="cb3-361"><a href="#cb3-361" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb3-362"><a href="#cb3-362" aria-hidden="true" tabindex="-1"></a>            advocate_id<span class="op">=</span>advocate_id,</span>
<span id="cb3-363"><a href="#cb3-363" aria-hidden="true" tabindex="-1"></a>            answer<span class="op">=</span>answer,  <span class="co"># The answer this advocate is defending</span></span>
<span id="cb3-364"><a href="#cb3-364" aria-hidden="true" tabindex="-1"></a>            opponent_answer<span class="op">=</span>opponent_answer,  <span class="co"># The opposing answer</span></span>
<span id="cb3-365"><a href="#cb3-365" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>feedback,</span>
<span id="cb3-366"><a href="#cb3-366" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>opponent_argument,</span>
<span id="cb3-367"><a href="#cb3-367" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(team_arguments)</span>
<span id="cb3-368"><a href="#cb3-368" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-369"><a href="#cb3-369" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(prompt)</span>
<span id="cb3-370"><a href="#cb3-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-371"><a href="#cb3-371" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> judge_debate(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>,</span>
<span id="cb3-372"><a href="#cb3-372" aria-hidden="true" tabindex="-1"></a>                          defense_1: <span class="bu">str</span>, defense_2: <span class="bu">str</span>, </span>
<span id="cb3-373"><a href="#cb3-373" aria-hidden="true" tabindex="-1"></a>                          current_round: <span class="bu">int</span>,</span>
<span id="cb3-374"><a href="#cb3-374" aria-hidden="true" tabindex="-1"></a>                          memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, Tuple[<span class="bu">float</span>, <span class="bu">float</span>]]:</span>
<span id="cb3-375"><a href="#cb3-375" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Judge the debate between two answers."""</span></span>
<span id="cb3-376"><a href="#cb3-376" aria-hidden="true" tabindex="-1"></a>        feedback_prompt <span class="op">=</span> <span class="va">self</span>.judge_prompt.<span class="bu">format</span>(</span>
<span id="cb3-377"><a href="#cb3-377" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb3-378"><a href="#cb3-378" aria-hidden="true" tabindex="-1"></a>            answer_1<span class="op">=</span>answer_1,</span>
<span id="cb3-379"><a href="#cb3-379" aria-hidden="true" tabindex="-1"></a>            answer_2<span class="op">=</span>answer_2,</span>
<span id="cb3-380"><a href="#cb3-380" aria-hidden="true" tabindex="-1"></a>            current_round<span class="op">=</span>current_round,</span>
<span id="cb3-381"><a href="#cb3-381" aria-hidden="true" tabindex="-1"></a>            total_rounds<span class="op">=</span><span class="va">self</span>.max_rounds,</span>
<span id="cb3-382"><a href="#cb3-382" aria-hidden="true" tabindex="-1"></a>            previous_scores<span class="op">=</span>memory.scores,</span>
<span id="cb3-383"><a href="#cb3-383" aria-hidden="true" tabindex="-1"></a>            defense_1<span class="op">=</span>defense_1,</span>
<span id="cb3-384"><a href="#cb3-384" aria-hidden="true" tabindex="-1"></a>            defense_2<span class="op">=</span>defense_2</span>
<span id="cb3-385"><a href="#cb3-385" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-386"><a href="#cb3-386" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(feedback_prompt)</span>
<span id="cb3-387"><a href="#cb3-387" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-388"><a href="#cb3-388" aria-hidden="true" tabindex="-1"></a>        score_prompt <span class="op">=</span> PROMPTS[<span class="st">"score_prompt_samre"</span>].<span class="bu">format</span>(</span>
<span id="cb3-389"><a href="#cb3-389" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb3-390"><a href="#cb3-390" aria-hidden="true" tabindex="-1"></a>            answer_1<span class="op">=</span>answer_1,</span>
<span id="cb3-391"><a href="#cb3-391" aria-hidden="true" tabindex="-1"></a>            answer_2<span class="op">=</span>answer_2,</span>
<span id="cb3-392"><a href="#cb3-392" aria-hidden="true" tabindex="-1"></a>            defense_1<span class="op">=</span>defense_1,</span>
<span id="cb3-393"><a href="#cb3-393" aria-hidden="true" tabindex="-1"></a>            defense_2<span class="op">=</span>defense_2,</span>
<span id="cb3-394"><a href="#cb3-394" aria-hidden="true" tabindex="-1"></a>            total_rounds<span class="op">=</span><span class="va">self</span>.max_rounds,</span>
<span id="cb3-395"><a href="#cb3-395" aria-hidden="true" tabindex="-1"></a>            previous_scores<span class="op">=</span>memory.scores,</span>
<span id="cb3-396"><a href="#cb3-396" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>feedback</span>
<span id="cb3-397"><a href="#cb3-397" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-398"><a href="#cb3-398" aria-hidden="true" tabindex="-1"></a>        score_response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(score_prompt)    </span>
<span id="cb3-399"><a href="#cb3-399" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-400"><a href="#cb3-400" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-401"><a href="#cb3-401" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb3-402"><a href="#cb3-402" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="va">self</span>._extract_final_scores(score_response)</span>
<span id="cb3-403"><a href="#cb3-403" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb3-404"><a href="#cb3-404" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Score parsing error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-405"><a href="#cb3-405" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Raw score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-406"><a href="#cb3-406" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> (<span class="fl">10.0</span>, <span class="fl">10.0</span>)</span>
<span id="cb3-407"><a href="#cb3-407" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-408"><a href="#cb3-408" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> feedback, scores</span>
<span id="cb3-409"><a href="#cb3-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-410"><a href="#cb3-410" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _run_debate_round(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, </span>
<span id="cb3-411"><a href="#cb3-411" aria-hidden="true" tabindex="-1"></a>                               round_num: <span class="bu">int</span>, memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb3-412"><a href="#cb3-412" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Executes single debate round in SAMRE evaluation"""</span></span>
<span id="cb3-413"><a href="#cb3-413" aria-hidden="true" tabindex="-1"></a>        defenses <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>._get_advocate_defenses(question, answer_1, answer_2, memory)</span>
<span id="cb3-414"><a href="#cb3-414" aria-hidden="true" tabindex="-1"></a>        memory.arguments.append(defenses)</span>
<span id="cb3-415"><a href="#cb3-415" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-416"><a href="#cb3-416" aria-hidden="true" tabindex="-1"></a>        feedback, scores <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.judge_debate(</span>
<span id="cb3-417"><a href="#cb3-417" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, defenses[<span class="dv">0</span>], defenses[<span class="dv">1</span>], round_num <span class="op">+</span> <span class="dv">1</span>, memory</span>
<span id="cb3-418"><a href="#cb3-418" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-419"><a href="#cb3-419" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-420"><a href="#cb3-420" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._store_round_results(feedback, scores, memory)</span>
<span id="cb3-421"><a href="#cb3-421" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._display_round_results(defenses, feedback, scores)</span>
<span id="cb3-422"><a href="#cb3-422" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-423"><a href="#cb3-423" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scores</span>
<span id="cb3-424"><a href="#cb3-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-425"><a href="#cb3-425" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _get_advocate_defenses(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>,</span>
<span id="cb3-426"><a href="#cb3-426" aria-hidden="true" tabindex="-1"></a>                                   memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, <span class="bu">str</span>]:</span>
<span id="cb3-427"><a href="#cb3-427" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get defenses from both advocates."""</span></span>
<span id="cb3-428"><a href="#cb3-428" aria-hidden="true" tabindex="-1"></a>        defense_1 <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.defend_answer(</span>
<span id="cb3-429"><a href="#cb3-429" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, <span class="dv">1</span>,</span>
<span id="cb3-430"><a href="#cb3-430" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>memory.feedback[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory.feedback <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb3-431"><a href="#cb3-431" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>memory.arguments[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>] <span class="cf">if</span> memory.arguments <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb3-432"><a href="#cb3-432" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span>[args[<span class="dv">0</span>] <span class="cf">for</span> args <span class="kw">in</span> memory.arguments]</span>
<span id="cb3-433"><a href="#cb3-433" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-434"><a href="#cb3-434" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-435"><a href="#cb3-435" aria-hidden="true" tabindex="-1"></a>        defense_2 <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.defend_answer(</span>
<span id="cb3-436"><a href="#cb3-436" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, <span class="dv">2</span>,</span>
<span id="cb3-437"><a href="#cb3-437" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>memory.feedback[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory.feedback <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb3-438"><a href="#cb3-438" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>memory.arguments[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="cf">if</span> memory.arguments <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb3-439"><a href="#cb3-439" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span>[args[<span class="dv">1</span>] <span class="cf">for</span> args <span class="kw">in</span> memory.arguments]</span>
<span id="cb3-440"><a href="#cb3-440" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-441"><a href="#cb3-441" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-442"><a href="#cb3-442" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (defense_1, defense_2)</span>
<span id="cb3-443"><a href="#cb3-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-444"><a href="#cb3-444" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _store_round_results(<span class="va">self</span>, feedback: <span class="bu">str</span>, scores: Tuple[<span class="bu">float</span>, <span class="bu">float</span>],</span>
<span id="cb3-445"><a href="#cb3-445" aria-hidden="true" tabindex="-1"></a>                           memory: Memory) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-446"><a href="#cb3-446" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Store feedback and scores from the round."""</span></span>
<span id="cb3-447"><a href="#cb3-447" aria-hidden="true" tabindex="-1"></a>        memory.feedback.append(feedback)</span>
<span id="cb3-448"><a href="#cb3-448" aria-hidden="true" tabindex="-1"></a>        memory.scores.append(scores)</span>
<span id="cb3-449"><a href="#cb3-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-450"><a href="#cb3-450" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _display_round_results(<span class="va">self</span>, defenses: Tuple[<span class="bu">str</span>, <span class="bu">str</span>], </span>
<span id="cb3-451"><a href="#cb3-451" aria-hidden="true" tabindex="-1"></a>                             feedback: <span class="bu">str</span>, scores: Tuple[<span class="bu">float</span>, <span class="bu">float</span>]) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-452"><a href="#cb3-452" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Display the results of the current round."""</span></span>
<span id="cb3-453"><a href="#cb3-453" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Advocate 1's defense:</span><span class="ch">\n</span><span class="sc">{</span>defenses[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-454"><a href="#cb3-454" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Advocate 2's defense:</span><span class="ch">\n</span><span class="sc">{</span>defenses[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-455"><a href="#cb3-455" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Judge's feedback:</span><span class="ch">\n</span><span class="sc">{</span>feedback<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-456"><a href="#cb3-456" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Scores for this round: Answer 1 = </span><span class="sc">{</span><span class="bu">round</span>(scores[<span class="dv">0</span>], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">, Answer 2 = </span><span class="sc">{</span><span class="bu">round</span>(scores[<span class="dv">1</span>], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-457"><a href="#cb3-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-458"><a href="#cb3-458" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _has_scores_converged(<span class="va">self</span>, round_num: <span class="bu">int</span>, memory: Memory) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb3-459"><a href="#cb3-459" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Checks if debate scores have converged by comparing last two rounds"""</span></span>
<span id="cb3-460"><a href="#cb3-460" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> round_num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb3-461"><a href="#cb3-461" aria-hidden="true" tabindex="-1"></a>            prev_diff <span class="op">=</span> memory.scores[<span class="op">-</span><span class="dv">2</span>][<span class="dv">0</span>] <span class="op">-</span> memory.scores[<span class="op">-</span><span class="dv">2</span>][<span class="dv">1</span>]</span>
<span id="cb3-462"><a href="#cb3-462" aria-hidden="true" tabindex="-1"></a>            curr_diff <span class="op">=</span> memory.scores[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="op">-</span> memory.scores[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>]</span>
<span id="cb3-463"><a href="#cb3-463" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (prev_diff <span class="op">*</span> curr_diff) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb3-464"><a href="#cb3-464" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb3-465"><a href="#cb3-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-466"><a href="#cb3-466" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _prepare_results(<span class="va">self</span>, memory: Memory) <span class="op">-&gt;</span> Dict:</span>
<span id="cb3-467"><a href="#cb3-467" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Prepare the final results dictionary."""</span></span>
<span id="cb3-468"><a href="#cb3-468" aria-hidden="true" tabindex="-1"></a>        avg_scores <span class="op">=</span> [</span>
<span id="cb3-469"><a href="#cb3-469" aria-hidden="true" tabindex="-1"></a>            <span class="bu">round</span>(<span class="bu">sum</span>(scores[i] <span class="cf">for</span> scores <span class="kw">in</span> memory.scores) <span class="op">/</span> <span class="bu">len</span>(memory.scores), <span class="dv">2</span>)</span>
<span id="cb3-470"><a href="#cb3-470" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb3-471"><a href="#cb3-471" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-472"><a href="#cb3-472" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-473"><a href="#cb3-473" aria-hidden="true" tabindex="-1"></a>        winner <span class="op">=</span> (</span>
<span id="cb3-474"><a href="#cb3-474" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&gt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb3-475"><a href="#cb3-475" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&lt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb3-476"><a href="#cb3-476" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'tie'</span></span>
<span id="cb3-477"><a href="#cb3-477" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-478"><a href="#cb3-478" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-479"><a href="#cb3-479" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb3-480"><a href="#cb3-480" aria-hidden="true" tabindex="-1"></a>            <span class="st">"winner"</span>: winner,</span>
<span id="cb3-481"><a href="#cb3-481" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_scores"</span>: avg_scores,</span>
<span id="cb3-482"><a href="#cb3-482" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rounds"</span>: <span class="bu">len</span>(memory.scores),</span>
<span id="cb3-483"><a href="#cb3-483" aria-hidden="true" tabindex="-1"></a>            <span class="st">"score_history"</span>: [[<span class="bu">round</span>(s[<span class="dv">0</span>], <span class="dv">2</span>), <span class="bu">round</span>(s[<span class="dv">1</span>], <span class="dv">2</span>)] <span class="cf">for</span> s <span class="kw">in</span> memory.scores],</span>
<span id="cb3-484"><a href="#cb3-484" aria-hidden="true" tabindex="-1"></a>            <span class="st">"argument_history"</span>: memory.arguments,</span>
<span id="cb3-485"><a href="#cb3-485" aria-hidden="true" tabindex="-1"></a>            <span class="st">"feedback_history"</span>: memory.feedback</span>
<span id="cb3-486"><a href="#cb3-486" aria-hidden="true" tabindex="-1"></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="load-the-mt-bench-dataset" class="level1">
<h1>Load the MT-Bench dataset</h1>
<p>For evaluation, I’ll use MT-Bench which is the dataset used in the paper. MT-Bench is a dataset that contains human annotator judgments of preference between two alternative LLM responses.</p>
<p>I’ll read the dataset from Llamahub <a href="https://llamahub.ai/l/llama_datasets/MT%20Bench%20Human%20Judgement%20Dataset?from=">MtBenchHumanJudgementDataset</a>, which has simplified the dataset by aggregating human judgments for repeated observations of the same model competitions.</p>
<blockquote class="blockquote">
<p>In the original version, there can be more than one human evaluator for a given example (query, two model responses). In this adapted version however, we aggregate these ‘repeated’ entries entries and convert the ‘winner’ column of the original schema to instead represent the proportion of times ‘model_a’ wins across all of the human evaluators. To adapt this to a llama-dataset, and to better consider ties (albeit with small samples) we set an uncertainty threshold for this proportion in that if it is between [0.4, 0.6] then we consider there to be no winner between the two models.</p>
</blockquote>
<p>Although it’s not entirely clear from this datacard description, the human evaluator judgments were encoded as “1” (model_a wins), “0” (model_b wins), or “0.5” (tie). Essentially, they were aggregated to represent the majority winner across repeated observations.</p>
<div id="71391835" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Commented out since the dataset is already downloaded</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#!llamaindex-cli download-llamadataset MtBenchHumanJudgementDataset --download-dir ./data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b6e8f171" class="cell" data-code-fold-show="false" data-execution_count="3">
<details class="code-fold">
<summary>Code to load the dataset</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.llama_dataset <span class="im">import</span> LabelledPairwiseEvaluatorDataset</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> LabelledPairwiseEvaluatorDataset.from_json(</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"./data/pairwise_evaluator_dataset.json"</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>).to_pandas()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the dataset</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Dataset shape: </span><span class="sc">{</span>df<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the reference_score value counts, just to confirm that this column is encoding the winner as I expect</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\n</span><span class="ss">Reference score (winner) value counts: </span><span class="sc">{</span>df[<span class="st">"reference_score"</span>]<span class="sc">.</span>value_counts()<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset shape: (1204, 12)

Reference score (winner) value counts: reference_score
0.0    709
1.0    433
0.5     62
Name: count, dtype: int64</code></pre>
</div>
</div>
<p>I’ll rename some of the columns, and also encode a “human_winner” column to indicate whether model_a was preferred, model_b, or if there was a tie. (Note: This is just my own preference for how to represent the data).</p>
<div id="c22b4194" class="cell" data-code-fold-show="false" data-execution_count="4">
<details class="code-fold">
<summary>Code to rename variables and encode a winner column</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[[<span class="st">'query'</span>, <span class="st">'answer'</span>, <span class="st">'second_answer'</span>, <span class="st">'answer_by'</span>, <span class="st">'second_answer_by'</span>, <span class="st">'reference_score'</span>]]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename as follows: query =&gt; question, answer =&gt; model_a_answer, second_answer =&gt; model_b_answer, answer_by =&gt; model_a, second_answer_by =&gt; model_b, reference_score =&gt; human_winner</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>df.rename(columns<span class="op">=</span>{<span class="st">'query'</span>: <span class="st">'question'</span>, <span class="st">'answer'</span>: <span class="st">'model_a_answer'</span>, <span class="st">'second_answer'</span>: <span class="st">'model_b_answer'</span>, <span class="st">'answer_by'</span>: <span class="st">'model_a'</span>, <span class="st">'second_answer_by'</span>: <span class="st">'model_b'</span>, <span class="st">'reference_score'</span>: <span class="st">'human_winner'</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Reencode human winner as "model_a" if 1, "model_b" if 0, and "tie" if 0.5</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'human_winner'</span>] <span class="op">=</span> df[<span class="st">'human_winner'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">'model_a'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'tie'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The original dataset contains ~1200 rows. I’ll take a random sample of 300 rows for my testing to save on time and API costs.</p>
<div id="d2f9cc35" class="cell" data-code-fold-show="false" data-execution_count="5">
<details class="code-fold">
<summary>Code to take a random sample of 300 rows</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a random sample of 300 rows</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.sample(n<span class="op">=</span><span class="dv">300</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">question</th>
<th data-quarto-table-cell-role="th">model_a_answer</th>
<th data-quarto-table-cell-role="th">model_b_answer</th>
<th data-quarto-table-cell-role="th">model_a</th>
<th data-quarto-table-cell-role="th">model_b</th>
<th data-quarto-table-cell-role="th">human_winner</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">76</td>
<td>Write a descriptive paragraph about a bustling...</td>
<td>The bustling marketplace teemed with life, a v...</td>
<td>Here is a descriptive paragraph about a bustli...</td>
<td>ai (gpt-4)</td>
<td>ai (claude-v1)</td>
<td>model_a</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">158</td>
<td>Embrace the role of Sheldon from "The Big Bang...</td>
<td>As Sheldon, I am a staunch proponent of hand d...</td>
<td>Well, I must say, I find hand dryers to be a r...</td>
<td>ai (alpaca-13b)</td>
<td>ai (vicuna-13b-v1.2)</td>
<td>model_b</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">717</td>
<td>You are given two sorted lists of size m and n...</td>
<td>One approach to solve this problem with linear...</td>
<td>To find the kth smallest element in the union ...</td>
<td>ai (gpt-3.5-turbo)</td>
<td>ai (vicuna-13b-v1.2)</td>
<td>model_a</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">602</td>
<td>Develop a Python program that reads all the te...</td>
<td>Here is a Python program that reads all text f...</td>
<td>Here's a Python program that reads all the tex...</td>
<td>ai (claude-v1)</td>
<td>ai (vicuna-13b-v1.2)</td>
<td>model_b</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">247</td>
<td>Act as a math teacher. I will provide some mat...</td>
<td>Probability is a branch of mathematics that de...</td>
<td>Probability is the likelihood that an event wi...</td>
<td>ai (vicuna-13b-v1.2)</td>
<td>ai (alpaca-13b)</td>
<td>tie</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="use-methods-to-evaluate-mt-bench-dataset" class="level1">
<h1>Use methods to evaluate MT-Bench dataset</h1>
<p>Using this sample of 300 rows from the MT-Bench dataset, I will run the three LLM models (Baseline-Weak, Baseline-Strong, and SAMRE) on each set of question and answers.</p>
<p>The code below is the main evaluation loop, designed to run multiple evaluations asynchronously (to save time). It will evaluate each item in the dataset, and save the results to disk as a checkpoint. If the evaluation is interrupted, the code can be resumed from the last checkpoint.</p>
<p>I’ll use <code>gpt-4o-mini</code> for the evaluations. In the paper they had tested models like <code>gpt-4o</code> and <code>gpt-3.5-turbo</code>, and I would not expect <code>gpt-4o-mini</code> to be an exception.</p>
<div id="858797b7" class="cell" data-code-fold-show="false" data-execution_count="6">
<details class="code-fold">
<summary>Click to view the code that runs the evaluations</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> asyncio</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> asyncio <span class="im">import</span> Semaphore</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> hashlib</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.WARNING)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> evaluate_conversation_pair(row, evaluators, semaphore, idx, total):</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate a single conversation pair with all evaluators"""</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="cf">with</span> semaphore:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add delay between API calls</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">#await asyncio.sleep(1)  # Add small delay between conversations</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate pair_id from conversation hash</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        pair_id <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>row[<span class="st">'model_a'</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>row[<span class="st">'model_b'</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>hashlib<span class="sc">.</span>sha256(<span class="bu">str</span>(row[<span class="st">'question'</span>]).encode())<span class="sc">.</span>hexdigest()[:<span class="dv">12</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        checkpoint_file <span class="op">=</span> <span class="ss">f'checkpoints/</span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">.json'</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return existing checkpoint if available</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> os.path.exists(checkpoint_file):</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>            logging.info(<span class="ss">f"Found existing checkpoint file for </span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> json.load(<span class="bu">open</span>(checkpoint_file))</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        logging.info(<span class="ss">f"No checkpoint file found for </span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> {</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span>: row[<span class="st">'model_a'</span>],</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_b'</span>: row[<span class="st">'model_b'</span>],</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">'human_winner'</span>: row[<span class="st">'human_winner'</span>],</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">'pair_id'</span>: pair_id</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># First run SAMRE evaluation with retries</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):  <span class="co"># Try up to 3 times</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>                    samre_evaluator <span class="op">=</span> evaluators[<span class="st">'samre'</span>]</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>                    samre_result <span class="op">=</span> <span class="cf">await</span> samre_evaluator.evaluate(</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>], </span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>], </span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>]</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'samre_winner'</span>] <span class="op">=</span> samre_result[<span class="st">'winner'</span>]</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'samre_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: samre_result[k] <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>                    result.update({</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'samre_argument_history'</span>: samre_result[<span class="st">'argument_history'</span>],</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'samre_feedback_history'</span>: samre_result[<span class="st">'feedback_history'</span>]</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>                    })</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span>  <span class="co"># If successful, break retry loop</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span>  <span class="co"># Exponential backoff</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on SAMRE, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:  <span class="co"># Last attempt failed</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span>  <span class="co"># Re-raise non-rate-limit errors</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> asyncio.sleep(<span class="fl">0.5</span>)  <span class="co"># Add small delay between evaluator calls</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run baseline strong with same number of rounds as SAMRE</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>                    baseline_strong_evaluator <span class="op">=</span> evaluators[<span class="st">'baseline_strong'</span>]</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>                    baseline_strong_result <span class="op">=</span> <span class="cf">await</span> baseline_strong_evaluator.evaluate(</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>],</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>],</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>],</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>                        num_rounds<span class="op">=</span>result[<span class="st">'samre_rounds'</span>]</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_strong_winner'</span>] <span class="op">=</span> baseline_strong_result[<span class="st">'winner'</span>]</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'baseline_strong_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: baseline_strong_result[k] </span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>                                 <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_strong_full_response'</span>] <span class="op">=</span> baseline_strong_result[<span class="st">'full_response'</span>]</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on baseline strong, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> asyncio.sleep(<span class="fl">0.5</span>)  <span class="co"># Add small delay between evaluator calls</span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run baseline weak with 1 round</span></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>                    baseline_weak_evaluator <span class="op">=</span> evaluators[<span class="st">'baseline_weak'</span>]</span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>                    baseline_weak_result <span class="op">=</span> <span class="cf">await</span> baseline_weak_evaluator.evaluate(</span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>],</span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>],</span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>],</span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a>                        num_rounds<span class="op">=</span><span class="dv">1</span></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_weak_winner'</span>] <span class="op">=</span> baseline_weak_result[<span class="st">'winner'</span>]</span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'baseline_weak_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: baseline_weak_result[k] </span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a>                                 <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_weak_full_response'</span>] <span class="op">=</span> baseline_weak_result[<span class="st">'full_response'</span>]</span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span></span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on baseline weak, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span></span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Error evaluating row </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'samre_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'baseline_strong_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'baseline_weak_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-118"><a href="#cb9-118" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'error'</span>] <span class="op">=</span> <span class="bu">str</span>(e)</span>
<span id="cb9-119"><a href="#cb9-119" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-120"><a href="#cb9-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save checkpoint after each evaluation</span></span>
<span id="cb9-121"><a href="#cb9-121" aria-hidden="true" tabindex="-1"></a>        os.makedirs(<span class="st">'checkpoints'</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-122"><a href="#cb9-122" aria-hidden="true" tabindex="-1"></a>        json.dump(result, <span class="bu">open</span>(checkpoint_file, <span class="st">'w'</span>))</span>
<span id="cb9-123"><a href="#cb9-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-124"><a href="#cb9-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (idx <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-125"><a href="#cb9-125" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Processed </span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>total<span class="sc">}</span><span class="ss"> conversations"</span>)</span>
<span id="cb9-126"><a href="#cb9-126" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-127"><a href="#cb9-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result</span>
<span id="cb9-128"><a href="#cb9-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-129"><a href="#cb9-129" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> evaluate_conversations_async(df, evaluators, semaphore_limit<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb9-130"><a href="#cb9-130" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate conversations asynchronously"""</span></span>
<span id="cb9-131"><a href="#cb9-131" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reduce semaphore limit</span></span>
<span id="cb9-132"><a href="#cb9-132" aria-hidden="true" tabindex="-1"></a>    semaphore_limit <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Process one at a time to avoid rate limits</span></span>
<span id="cb9-133"><a href="#cb9-133" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-134"><a href="#cb9-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process in smaller batches</span></span>
<span id="cb9-135"><a href="#cb9-135" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb9-136"><a href="#cb9-136" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb9-137"><a href="#cb9-137" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-138"><a href="#cb9-138" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(df), batch_size):</span>
<span id="cb9-139"><a href="#cb9-139" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> df.iloc[i:i<span class="op">+</span>batch_size]</span>
<span id="cb9-140"><a href="#cb9-140" aria-hidden="true" tabindex="-1"></a>        tasks <span class="op">=</span> [</span>
<span id="cb9-141"><a href="#cb9-141" aria-hidden="true" tabindex="-1"></a>            evaluate_conversation_pair(row[<span class="dv">1</span>], evaluators, Semaphore(semaphore_limit), idx, <span class="bu">len</span>(df))</span>
<span id="cb9-142"><a href="#cb9-142" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, row <span class="kw">in</span> <span class="bu">enumerate</span>(batch.iterrows(), start<span class="op">=</span>i)</span>
<span id="cb9-143"><a href="#cb9-143" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb9-144"><a href="#cb9-144" aria-hidden="true" tabindex="-1"></a>        batch_results <span class="op">=</span> <span class="cf">await</span> asyncio.gather(<span class="op">*</span>tasks)</span>
<span id="cb9-145"><a href="#cb9-145" aria-hidden="true" tabindex="-1"></a>        results.extend(batch_results)</span>
<span id="cb9-146"><a href="#cb9-146" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-147"><a href="#cb9-147" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add delay between batches</span></span>
<span id="cb9-148"><a href="#cb9-148" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">+</span> batch_size <span class="op">&lt;</span> <span class="bu">len</span>(df):</span>
<span id="cb9-149"><a href="#cb9-149" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Completed batch </span><span class="sc">{</span>i<span class="op">//</span>batch_size <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, waiting before next batch..."</span>)</span>
<span id="cb9-150"><a href="#cb9-150" aria-hidden="true" tabindex="-1"></a>            <span class="co">#await asyncio.sleep(5)  # 5 second delay between batches</span></span>
<span id="cb9-151"><a href="#cb9-151" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-152"><a href="#cb9-152" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(results)</span>
<span id="cb9-153"><a href="#cb9-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-154"><a href="#cb9-154" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> main():</span>
<span id="cb9-155"><a href="#cb9-155" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="cf">with</span> ModelEvaluator.create(mode<span class="op">=</span><span class="st">"samre"</span>) <span class="im">as</span> samre_evaluator, <span class="op">\</span></span>
<span id="cb9-156"><a href="#cb9-156" aria-hidden="true" tabindex="-1"></a>               ModelEvaluator.create(mode<span class="op">=</span><span class="st">"baseline_strong"</span>) <span class="im">as</span> baseline_strong_evaluator, <span class="op">\</span></span>
<span id="cb9-157"><a href="#cb9-157" aria-hidden="true" tabindex="-1"></a>               ModelEvaluator.create(mode<span class="op">=</span><span class="st">"baseline_weak"</span>) <span class="im">as</span> baseline_weak_evaluator:</span>
<span id="cb9-158"><a href="#cb9-158" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="cf">await</span> evaluate_conversations_async(</span>
<span id="cb9-159"><a href="#cb9-159" aria-hidden="true" tabindex="-1"></a>            df,</span>
<span id="cb9-160"><a href="#cb9-160" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb9-161"><a href="#cb9-161" aria-hidden="true" tabindex="-1"></a>                <span class="st">'samre'</span>: samre_evaluator, </span>
<span id="cb9-162"><a href="#cb9-162" aria-hidden="true" tabindex="-1"></a>                <span class="st">'baseline_strong'</span>: baseline_strong_evaluator,</span>
<span id="cb9-163"><a href="#cb9-163" aria-hidden="true" tabindex="-1"></a>                <span class="st">'baseline_weak'</span>: baseline_weak_evaluator</span>
<span id="cb9-164"><a href="#cb9-164" aria-hidden="true" tabindex="-1"></a>            },</span>
<span id="cb9-165"><a href="#cb9-165" aria-hidden="true" tabindex="-1"></a>            semaphore_limit<span class="op">=</span><span class="dv">1</span></span>
<span id="cb9-166"><a href="#cb9-166" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-167"><a href="#cb9-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-168"><a href="#cb9-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Run evaluation with checkpoint recovery</span></span>
<span id="cb9-169"><a href="#cb9-169" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb9-170"><a href="#cb9-170" aria-hidden="true" tabindex="-1"></a>    eval_df <span class="op">=</span> <span class="cf">await</span> main()</span>
<span id="cb9-171"><a href="#cb9-171" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb9-172"><a href="#cb9-172" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Error during evaluation: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ch">\n</span><span class="ss">Recovering from checkpoints..."</span>)</span>
<span id="cb9-173"><a href="#cb9-173" aria-hidden="true" tabindex="-1"></a>    eval_df <span class="op">=</span> pd.DataFrame([json.load(<span class="bu">open</span>(<span class="ss">f'checkpoints/</span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">'</span>)) </span>
<span id="cb9-174"><a href="#cb9-174" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">for</span> f <span class="kw">in</span> os.listdir(<span class="st">'checkpoints'</span>) </span>
<span id="cb9-175"><a href="#cb9-175" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">if</span> f.endswith(<span class="st">'.json'</span>)])</span>
<span id="cb9-176"><a href="#cb9-176" aria-hidden="true" tabindex="-1"></a><span class="cf">finally</span>:</span>
<span id="cb9-177"><a href="#cb9-177" aria-hidden="true" tabindex="-1"></a>    eval_df.to_csv(<span class="st">'eval_df.csv'</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-178"><a href="#cb9-178" aria-hidden="true" tabindex="-1"></a>    eval_df.head()</span>
<span id="cb9-179"><a href="#cb9-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-180"><a href="#cb9-180" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop rows with any null values on the model winner columns</span></span>
<span id="cb9-181"><a href="#cb9-181" aria-hidden="true" tabindex="-1"></a>eval_df <span class="op">=</span> eval_df.dropna(subset<span class="op">=</span>[<span class="st">'baseline_strong_winner'</span>, <span class="st">'baseline_weak_winner'</span>, <span class="st">'samre_winner'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Completed batch 1, waiting before next batch...
Completed batch 2, waiting before next batch...
Completed batch 3, waiting before next batch...
Completed batch 4, waiting before next batch...
Completed batch 5, waiting before next batch...
Completed batch 6, waiting before next batch...
Completed batch 7, waiting before next batch...
Completed batch 8, waiting before next batch...
Completed batch 9, waiting before next batch...
Completed batch 10, waiting before next batch...
Completed batch 11, waiting before next batch...
Completed batch 12, waiting before next batch...
Completed batch 13, waiting before next batch...
Completed batch 14, waiting before next batch...
Completed batch 15, waiting before next batch...
Completed batch 16, waiting before next batch...
Completed batch 17, waiting before next batch...
Completed batch 18, waiting before next batch...
Completed batch 19, waiting before next batch...
Completed batch 20, waiting before next batch...
Completed batch 21, waiting before next batch...
Completed batch 22, waiting before next batch...
Completed batch 23, waiting before next batch...
Completed batch 24, waiting before next batch...
Completed batch 25, waiting before next batch...
Completed batch 26, waiting before next batch...
Completed batch 27, waiting before next batch...
Completed batch 28, waiting before next batch...
Completed batch 29, waiting before next batch...</code></pre>
</div>
</div>
</section>
<section id="performance-evaluation" class="level1">
<h1>Performance evaluation</h1>
<p>Now that the evaluation is complete, I will evaluate the performance of each of the three methods by first looking at how well each method agreed with the human judgments.</p>
<p>I’ll use Krippendorff’s alpha to measure agreement, since it is a robust measure of agreement that can handle non-binary ratings (among other things).</p>
<div id="7f293d8e" class="cell" data-code-fold-show="false" data-execution_count="7">
<details class="code-fold">
<summary>Click to view the code that calculates agreement</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> krippendorff <span class="im">import</span> alpha</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_agreement(df, rater1_col, rater2_col):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate Krippendorff's alpha between two raters.</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">        df: DataFrame containing the ratings</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">        rater1_col: Name of first rater's column</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">        rater2_col: Name of second rater's column</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">        float: Krippendorff's alpha score</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create label encoder</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    le <span class="op">=</span> LabelEncoder()</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine all unique values from both columns</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    all_values <span class="op">=</span> pd.concat([df[rater1_col], df[rater2_col]]).unique()</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    le.fit(all_values)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Transform the ratings to numeric values</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    ratings1 <span class="op">=</span> le.transform(df[rater1_col].fillna(<span class="st">'missing'</span>))</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    ratings2 <span class="op">=</span> le.transform(df[rater2_col].fillna(<span class="st">'missing'</span>))</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape data for krippendorff alpha calculation</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each row represents one item, each column represents one rater</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    reliability_data <span class="op">=</span> np.vstack([ratings1, ratings2])</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha(reliability_data<span class="op">=</span>reliability_data, level_of_measurement<span class="op">=</span><span class="st">'nominal'</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate agreement scores for all methods</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>human_baseline_strong_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_strong_winner'</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>human_baseline_weak_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_weak_winner'</span>)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>human_samre_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'samre_winner'</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame with the agreement scores</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>agreement_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Evaluator Pair'</span>: [<span class="st">'Baseline-Strong Agreement with Humans'</span>, <span class="st">'Baseline-Weak Agreement with Humans'</span>, <span class="st">'SAMRE Agreement with Humans'</span>],</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Krippendorff Alpha'</span>: [human_baseline_strong_agreement, human_baseline_weak_agreement, human_samre_agreement]</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Round the scores to 3 decimal places</span></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>agreement_df[<span class="st">'Krippendorff Alpha'</span>] <span class="op">=</span> agreement_df[<span class="st">'Krippendorff Alpha'</span>].<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percent difference between Baseline-Strong and Baseline-Weak, and SAMRE and Baseline-Strong</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_diff <span class="op">=</span> (human_baseline_strong_agreement <span class="op">-</span> human_baseline_weak_agreement) <span class="op">/</span> human_baseline_strong_agreement</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_diff <span class="op">=</span> (human_baseline_strong_agreement <span class="op">-</span> human_samre_agreement) <span class="op">/</span> human_baseline_strong_agreement</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_diff <span class="op">=</span> (human_samre_agreement <span class="op">-</span> human_baseline_weak_agreement) <span class="op">/</span> human_samre_agreement</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(agreement_df)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the percent difference</span></span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Krippendorff Alpha Improvements:"</span>)</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                          Evaluator Pair  Krippendorff Alpha
0  Baseline-Strong Agreement with Humans               0.411
1    Baseline-Weak Agreement with Humans               0.321
2            SAMRE Agreement with Humans               0.369

Krippendorff Alpha Improvements:
SAMRE vs. Baseline-Weak: 13%
Baseline-Strong vs. Baseline-Weak: 22%
Baseline-Strong vs. SAMRE: 10%</code></pre>
</div>
</div>
<p>Although none of the methods yielded particularly strong agreement with the human judges in an absolute sense, their relative performance is in line with my predictions:</p>
<ol type="1">
<li>As reported in the paper, SAMRE yielded significantly better agreement than Baseline-Weak (0.369 vs.&nbsp;0.321, an increase of ~13%).</li>
<li>Baseline-Strong yielded significantly better agreement than Baseline-Weak (0.411 vs.&nbsp;0.321, an increase of ~22%).</li>
<li>Importantly, Baseline-Strong also yielded significantly better agreement than SAMRE (0.411 vs.&nbsp;0.321, an increase of ~10%)!</li>
</ol>
<p>Next, we can also measure performance in terms of binary classification accuracy using Matthews Correlation Coefficient (MCC) as a balanced accuracy metric, while re-encoding the “winner” columns to indicate whether model_a was selected as better (1) or not better (0) in each case.</p>
<div id="981d302a" class="cell" data-code-fold-show="false" data-execution_count="8">
<details class="code-fold">
<summary>Click to view the code that calculates Matthews Correlation Coefficient (MCC)</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode winner as binary</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_winner_as_binary(winner):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> winner <span class="op">==</span> <span class="st">'model_a'</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create binary columns for each evaluator</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'human_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'human_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'baseline_strong_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'baseline_strong_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'baseline_weak_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'baseline_weak_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'samre_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'samre_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> matthews_corrcoef</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate MCC for each method</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>metrics_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Method'</span>: [<span class="st">'Baseline-Strong'</span>, <span class="st">'Baseline-Weak'</span>, <span class="st">'SAMRE'</span>],</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'MCC'</span>: [</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'baseline_strong_model_a_better'</span>]</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'baseline_weak_model_a_better'</span>]</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'samre_model_a_better'</span>]</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Round the scores to 3 decimal places</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>metrics_df[<span class="st">'MCC'</span>] <span class="op">=</span> metrics_df[<span class="st">'MCC'</span>].<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percent differences</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_percent_diff(new, old):</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (new <span class="op">-</span> old) <span class="op">/</span> old <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co"># MCC differences</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(metrics_df)</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">MCC Improvements:"</span>)</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Method    MCC
0  Baseline-Strong  0.482
1    Baseline-Weak  0.417
2            SAMRE  0.401

MCC Improvements:
SAMRE vs. Baseline-Weak: -4%
Baseline-Strong vs. Baseline-Weak: 16%
Baseline-Strong vs. SAMRE: 20%</code></pre>
</div>
</div>
<p>Looking at MCC values, we observe a similar pattern of findings to the Krippendorff alphas:</p>
<ol type="1">
<li>SAMRE did not perform better than Baseline-Weak, in fact it performed slightly worse (0.401 vs.&nbsp;0.417, a decrease of 4%). This is a bit different than what we saw with Krippendorff alpha.</li>
<li>Baseline-Strong performed better than Baseline-Weak (0.482 vs.&nbsp;0.401, an increase of 16%).</li>
<li>Baseline-Strong performed better than SAMRE (0.464 vs.&nbsp;0.401, an increase of 20%).</li>
</ol>
<p><em>Side-note: Why does MCC disagree with the Krippendorff alpha on the SAMRE vs.&nbsp;Baseline-Weak comparison? I would guess this is due to how ties were resolved when encoding the winner as binary.</em></p>
<p>Finally, we can look at accuracy in terms of percentage agreement. Percentage agreement is not a “balanced” accuracy metric and therefore needs to be used with caution (for example, if the classes are imbalanced, then percentage agreement accuracy can be misleading). But it is the metric used in the paper.</p>
<div id="41f74433" class="cell" data-code-fold-show="false" data-execution_count="9">
<details class="code-fold">
<summary>Click to view the code that calculates percentage agreement</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate percentage agreement for each method</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_percent_agreement(df, rater1_col, rater2_col):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate percentage agreement between two raters"""</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (df[rater1_col] <span class="op">==</span> df[rater2_col]).mean()</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate agreement percentages</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>agreement_percentages <span class="op">=</span> pd.DataFrame({</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Method'</span>: [<span class="st">'Baseline-Strong'</span>, <span class="st">'Baseline-Weak'</span>, <span class="st">'SAMRE'</span>],</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Agreement'</span>: [</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        calculate_percent_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_strong_winner'</span>),</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        calculate_percent_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_weak_winner'</span>),</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        calculate_percent_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'samre_winner'</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Round to 3 decimal places and convert to percentage</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>agreement_percentages[<span class="st">'Agreement'</span>] <span class="op">=</span> (agreement_percentages[<span class="st">'Agreement'</span>] <span class="op">*</span> <span class="dv">100</span>).<span class="bu">round</span>(<span class="dv">1</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percentage point differences</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_diff <span class="op">=</span> (</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>] <span class="op">-</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_diff <span class="op">=</span> (</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>] <span class="op">-</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_diff <span class="op">=</span> (</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>] <span class="op">-</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Percentage Agreement with Human Judgments:"</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(agreement_percentages)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Percentage Point Differences:"</span>)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_diff<span class="sc">:+.1f}</span><span class="ss">"</span>)</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_diff<span class="sc">:+.1f}</span><span class="ss">"</span>)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_diff<span class="sc">:+.1f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Percentage Agreement with Human Judgments:
            Method  Agreement
0  Baseline-Strong       68.4
1    Baseline-Weak       62.8
2            SAMRE       67.0

Percentage Point Differences:
SAMRE vs. Baseline-Weak: +4.2
Baseline-Strong vs. Baseline-Weak: +5.6
Baseline-Strong vs. SAMRE: +1.4</code></pre>
</div>
</div>
<p>Overall across these three metrics, the story is the same: SAMRE did not perform better than a baseline that is designed with best practices.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, I have shown that SAMRE does not perform better than a well-engineered baseline method. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tylerburleigh\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="tylerburleigh/tylerburleigh.github.io" data-repo-id="R_kgDOKMo8ww" data-category="Blog comments" data-category-id="DIC_kwDOIg6EJc4CSz92" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Challenging SAMRE: Comparing multi-round debate-style LLM evaluation to a robust (and much simpler) baseline"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2025-01-12</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "In this post, I re-evaluate a method that was recently published in arXiv, critiquing their baseline model and then designing a new baseline model that implements standard best practices for comparison with the new method. I find that the new evaluation method proposed in the paper does not perform better than this robust baseline. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as being skeptical of claims in research papers that compare new methods to baseline."</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - prompt-engineering</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - python</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - LLM-as-judge</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - LLM-evals</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="an">freeze:</span><span class="co"> true</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>I've been doing a lot of work with LLM-based evaluations lately, and I've been thinking about how to improve the quality of these evaluations.</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>I like to read research papers from arXiv for inspiration, and I recently came across a paper called <span class="co">[</span><span class="ot">Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates</span><span class="co">](https://arxiv.org/abs/2410.04663)</span>, which introduces a new method inspired by judicial process called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process.</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>I was initially impressed by the results, which reported a gain of ~6-8% over baseline. Below I've reproduced an excerpt from one of the tables in the paper showing their results.</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>| Model | SAMRE w/o Juries | SAMRE w/o Juries (%) |</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>|-------|------------------|---------------------|</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>| Llama-3-8B | 0.05 | 6.3% |</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>| Qwen | 0.06 | 7.3% |</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>| Gemini | 0.06 | 7.2% |</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>| GPT-4-o | 0.07 | 8.3% |</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>| GPT-4-turbo | 0.07 | 8.2% |</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>| GPT-3.5-turbo | 0.05 | 6.2% |</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>: Excerpt from "Table 2: Performance Gains Compared to Baseline"</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>_Note that the authors had tested versions of SAMRE with and without the addition of "juries". In the table I've included only the version without juries, as it was both simpler and more performant. It is also this more performant version without juries that I am interested in testing. So with that said, in this blog post when I mention "SAMRE", I will be referring to the version without juries._</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>Despite the impressive results reported in the paper, I am often skeptical when researchers claim to have found that new methods outperform "baseline" models. I have observed that researchers often fail to implement standard best practices in their baseline models, and so their results are therefore not represenative of true gains over baseline. It is as if they are knocking down a straw man.</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>Given this skepticism of mine, I decided that it might be interesting to put it this skepticism the test: What if I implemented the SAMRE method (again, note that I am referring to the version without juries), and compared it to a baseline model that does implement standard best practices for prompt engineering? Would I find that the SAMRE method is indeed an improvement over the baseline? Or would I find that SAMRE is inferior to a properly implemented baseline?</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## TL;DR: What I did and what I found</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>I tested three model variants:</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>SAMRE, as implemented in the paper (without juries)</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>I evaluated each of these models using a sample of 300 conversations from MT-Bench for testing and evaluation. (MT-Bench was used in the original paper as well.)</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>After running the evaluations and calculating Krippendorff alpha agreement with human judge ground truth, I found that although SAMRE did yield better agreement than Baseline-Weak more importantly it was inferior to Baseline-Strong -- and by a fair margin. A similar result was found when examining binary classification accuracy using Matthews Correlation Coefficient (MCC).</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>These results serve to highlight the importance of implementing standard best practices in baseline models, as well as being skeptical of claims in research papers that compare new methods to a "baseline model". Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a><span class="fu"># Baseline model prompt inadequacies</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>Here I will consider some of the inadequacies in the Baseline model's prompt reported in the paper, and share a version of the prompt that addresses these inadequacies and implements standard best practices. </span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>The "baseline" prompt used by the authors of the paper was as follows:</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a><span class="in">```prompt</span></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a><span class="in">You are a fair, impartial judge scoring a debate on the following question:</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a><span class="in">question.</span></span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 1: answer_1</span></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 2: answer_2</span></span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a><span class="in">Score each answer on a scale of 1-20 for each of the following criteria:</span></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a><span class="in">1. Relevance to the question</span></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a><span class="in">2. Accuracy of information and use of credible sources</span></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a><span class="in">3. Depth of analysis and completeness of argument</span></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a><span class="in">4. Clarity of expression and logical flow</span></span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a><span class="in">5. Strength of reasoning and factual support</span></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a><span class="in">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a><span class="in">Provide scores as [answer_1_score, answer_2_score] for each criterion in a list format, then sum for final scores. Please keep an eye on the slightest difference that should make a difference in the scoring. Don’t overthink!</span></span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a><span class="in">Relevance:</span></span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a><span class="in">Accuracy:</span></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a><span class="in">Depth:</span></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a><span class="in">Clarity:</span></span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a><span class="in">Logic and Factuality:</span></span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a><span class="in">Addressing opponent’s points:</span></span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a><span class="in">Final Scores (sum of above) as a tuple (example: (18, 9)):</span></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a><span class="in">Explain your scoring, focusing on why one answer is better than the other based on the criteria above. Keep your explanation concise but informative.</span></span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a><span class="in">Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).</span></span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a><span class="in">Example: (18, 9)</span></span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a><span class="in">Your scores and explanation:</span></span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a>Here are the issues I see with this prompt:</span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The prompt does not use delimiters for most of the inputs. I would enclose the inputs inside XML tags like <span class="in">`&lt;Question&gt;&lt;/Question&gt;`</span>, <span class="in">`&lt;Answer1&gt;&lt;/Answer1&gt;`</span>, and <span class="in">`&lt;Answer2&gt;&lt;/Answer2&gt;`</span>, but in a pinch delimiters like triple backticks can be used.</span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The prompt instructs the model to first generate scores in list format, and then to sum them. But as we know, language models models often make arithmetic mistakes. It would be better to ask the model to generate scores for each criterion, and then to programmatically extract and summarize them in python (or another programming language) from which the routine is run.</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Although the prompt asks the model to "explain your scoring", it is not clear if the model should be reasoning about each criterion before it scores them, or if it should provide reasoning at the end when giving its final score. I would ask the model to provide reasoning for each criterion that it is asked to score, and ask it to reason before scoring.</span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>It's unclear why a scale of 1-20 is used. This is not a standard scale for scoring. I would use a scale of 1-10 which is likely more familiar to the model and can be expected to be used more consistently.</span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Although the prompt does suggest that the model provide its scores in tuple format, it would be better to provide more explicit format instructions.</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>The prompt includes an "Effectiveness in addressing opponent's points" criterion, but this is almost certainly irrelevant given that the answers to the question were not generated with the goal of addressing an opponent.</span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Finally, although this goes beyond the prompt itself, the authors of the paper are comparing a multi-round method to a single-round method. This is obviously an unfair comparison. Instead, it would be better to compare the SAMRE method to a baseline that uses the same number of rounds and then similarly averages its scores.</span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a>With all of that in mind, here's how I would rewrite the prompt:</span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a><span class="in">```prompt</span></span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a><span class="in">You are a fair, impartial judge scoring a debate on Question.</span></span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Question&gt;</span></span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a><span class="in">{question}</span></span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Question&gt;</span></span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a><span class="in">Two Answers have been given to the Question.</span></span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer1&gt;</span></span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a><span class="in">{answer_1}</span></span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Answer1&gt;</span></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer2&gt;</span></span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a><span class="in">{answer_2}</span></span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Answer2&gt;</span></span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a><span class="in">The Answers are being judged on the following Criteria:</span></span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criteria&gt;</span></span>
<span id="cb17-121"><a href="#cb17-121" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;</span></span>
<span id="cb17-122"><a href="#cb17-122" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;</span></span>
<span id="cb17-123"><a href="#cb17-123" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;</span></span>
<span id="cb17-124"><a href="#cb17-124" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;</span></span>
<span id="cb17-125"><a href="#cb17-125" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;</span></span>
<span id="cb17-126"><a href="#cb17-126" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Criteria&gt;</span></span>
<span id="cb17-127"><a href="#cb17-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-128"><a href="#cb17-128" aria-hidden="true" tabindex="-1"></a><span class="in">For each Criterion, briefly analyze the performance of </span></span>
<span id="cb17-129"><a href="#cb17-129" aria-hidden="true" tabindex="-1"></a><span class="in">the two Answers, then give a score between 1 and 10.</span></span>
<span id="cb17-130"><a href="#cb17-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-131"><a href="#cb17-131" aria-hidden="true" tabindex="-1"></a><span class="in">Respond as follows:</span></span>
<span id="cb17-132"><a href="#cb17-132" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion1&gt;</span></span>
<span id="cb17-133"><a href="#cb17-133" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;</span></span>
<span id="cb17-134"><a href="#cb17-134" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Analysis&gt;</span></span>
<span id="cb17-135"><a href="#cb17-135" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb17-136"><a href="#cb17-136" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb17-137"><a href="#cb17-137" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Analysis&gt;</span></span>
<span id="cb17-138"><a href="#cb17-138" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Scores&gt;</span></span>
<span id="cb17-139"><a href="#cb17-139" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb17-140"><a href="#cb17-140" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb17-141"><a href="#cb17-141" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Scores&gt;</span></span>
<span id="cb17-142"><a href="#cb17-142" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Criterion1&gt;</span></span>
<span id="cb17-143"><a href="#cb17-143" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion2&gt;</span></span>
<span id="cb17-144"><a href="#cb17-144" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;</span></span>
<span id="cb17-145"><a href="#cb17-145" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Analysis&gt;</span></span>
<span id="cb17-146"><a href="#cb17-146" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb17-147"><a href="#cb17-147" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb17-148"><a href="#cb17-148" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Analysis&gt;</span></span>
<span id="cb17-149"><a href="#cb17-149" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Scores&gt;</span></span>
<span id="cb17-150"><a href="#cb17-150" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb17-151"><a href="#cb17-151" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb17-152"><a href="#cb17-152" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Scores&gt;</span></span>
<span id="cb17-153"><a href="#cb17-153" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Criterion2&gt;</span></span>
<span id="cb17-154"><a href="#cb17-154" aria-hidden="true" tabindex="-1"></a><span class="in">...</span></span>
<span id="cb17-155"><a href="#cb17-155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-156"><a href="#cb17-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-157"><a href="#cb17-157" aria-hidden="true" tabindex="-1"></a>Notice that the prompt now uses XML tags to structure the instructions, that it asks the model to provide reasoning for each criterion before scoring, and that it gives the model a clear format for its response that reinforces analysis before scoring for each criterion.</span>
<span id="cb17-158"><a href="#cb17-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-159"><a href="#cb17-159" aria-hidden="true" tabindex="-1"></a>I've also changed the scale from 1-20 to 1-10, removed the unnecessary "Effectiveness in addressing opponent's points" criterion, and removed the instruction to summarize the scores, as I would handle this within the code.</span>
<span id="cb17-160"><a href="#cb17-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-161"><a href="#cb17-161" aria-hidden="true" tabindex="-1"></a>_Note the baseline could be improved even further by requesting the structured output using a mode like OpenAI's Structured Outputs. This would increase the likelihood of the model responding in the desired format. For this test, I will not be using structured outputs._</span>
<span id="cb17-162"><a href="#cb17-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-163"><a href="#cb17-163" aria-hidden="true" tabindex="-1"></a><span class="fu"># Hypothesis and predictions</span></span>
<span id="cb17-164"><a href="#cb17-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-165"><a href="#cb17-165" aria-hidden="true" tabindex="-1"></a>I hypothesize that SAMRE will NOT perform better than a baseline model that implements standard best practices for prompt engineering.</span>
<span id="cb17-166"><a href="#cb17-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-167"><a href="#cb17-167" aria-hidden="true" tabindex="-1"></a>My predictions are as follows:</span>
<span id="cb17-168"><a href="#cb17-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-169"><a href="#cb17-169" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>SAMRE will perform better than Baseline-Weak, as this was what the authors of the paper found and by implementing these methods faithfully from the paper, I can expect to replicate their results.</span>
<span id="cb17-170"><a href="#cb17-170" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Strong will perform better than Baseline-Weak, since the "Strong" variant implements best practices and can be expected to perform better than the "Weak" variant.</span>
<span id="cb17-171"><a href="#cb17-171" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Baseline-Strong will perform equal to or better than SAMRE, as the best practices implemented in Baseline-Strong will close the gap that exists between SAMRE and Baseline-Weak due to the inadequacies in the Baseline-Weak prompt.</span>
<span id="cb17-172"><a href="#cb17-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-173"><a href="#cb17-173" aria-hidden="true" tabindex="-1"></a><span class="fu"># My implementation of SAMRE and Baseline</span></span>
<span id="cb17-174"><a href="#cb17-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-175"><a href="#cb17-175" aria-hidden="true" tabindex="-1"></a>Okay, so with those criticisms out of the way, let's design evaluators to implement three methods:</span>
<span id="cb17-176"><a href="#cb17-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-177"><a href="#cb17-177" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>SAMRE, as implemented in the paper (without juries)</span>
<span id="cb17-178"><a href="#cb17-178" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)</span>
<span id="cb17-179"><a href="#cb17-179" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.</span>
<span id="cb17-180"><a href="#cb17-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-181"><a href="#cb17-181" aria-hidden="true" tabindex="-1"></a>Below is my python implementation of these evaluators. To the best of my ability, I have implemented the SAMRE and Baseline methods as described in the paper (I call the paper's Baseline method "Baseline-Weak"). And I have implemented a Baseline-Strong evaluator that implements what I would consider to be standard best practices for prompt engineering, as discussed in an earlier section.</span>
<span id="cb17-182"><a href="#cb17-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-185"><a href="#cb17-185" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-186"><a href="#cb17-186" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-187"><a href="#cb17-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb17-188"><a href="#cb17-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the implementation code"</span></span>
<span id="cb17-189"><a href="#cb17-189" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass, field</span>
<span id="cb17-190"><a href="#cb17-190" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple, Dict</span>
<span id="cb17-191"><a href="#cb17-191" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb17-192"><a href="#cb17-192" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> AsyncOpenAI</span>
<span id="cb17-193"><a href="#cb17-193" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb17-194"><a href="#cb17-194" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb17-195"><a href="#cb17-195" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> asynccontextmanager</span>
<span id="cb17-196"><a href="#cb17-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-197"><a href="#cb17-197" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the prompts</span></span>
<span id="cb17-198"><a href="#cb17-198" aria-hidden="true" tabindex="-1"></a>PROMPTS <span class="op">=</span> {</span>
<span id="cb17-199"><a href="#cb17-199" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for advocates to defend their assigned answer, used in SAMRE</span></span>
<span id="cb17-200"><a href="#cb17-200" aria-hidden="true" tabindex="-1"></a><span class="st">"defend_prompt"</span>: <span class="st">"""</span></span>
<span id="cb17-201"><a href="#cb17-201" aria-hidden="true" tabindex="-1"></a><span class="st">You’re an eloquent advocate (Advocate </span><span class="sc">{advocate_id}</span><span class="st">) in a group defending this answer:</span></span>
<span id="cb17-202"><a href="#cb17-202" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer}</span><span class="st"> to the question: </span><span class="sc">{question}</span><span class="st">. Your opponent group defends: </span><span class="sc">{opponent_answer}</span><span class="st">.</span></span>
<span id="cb17-203"><a href="#cb17-203" aria-hidden="true" tabindex="-1"></a><span class="st">Collaborate with your fellow advocates to convince the audience your argument is</span></span>
<span id="cb17-204"><a href="#cb17-204" aria-hidden="true" tabindex="-1"></a><span class="st">better. Use the latest feedback, your opponent’s last argument, and your team’s previous</span></span>
<span id="cb17-205"><a href="#cb17-205" aria-hidden="true" tabindex="-1"></a><span class="st">arguments to improve your case.</span></span>
<span id="cb17-206"><a href="#cb17-206" aria-hidden="true" tabindex="-1"></a><span class="st">Latest feedback: </span><span class="sc">{feedback}</span></span>
<span id="cb17-207"><a href="#cb17-207" aria-hidden="true" tabindex="-1"></a><span class="st">Opponent’s last argument: </span><span class="sc">{opponent_argument}</span></span>
<span id="cb17-208"><a href="#cb17-208" aria-hidden="true" tabindex="-1"></a><span class="st">Your team’s previous arguments: team_arguments</span></span>
<span id="cb17-209"><a href="#cb17-209" aria-hidden="true" tabindex="-1"></a><span class="st">Respond in under 80 words.</span></span>
<span id="cb17-210"><a href="#cb17-210" aria-hidden="true" tabindex="-1"></a><span class="st">Your defense:</span></span>
<span id="cb17-211"><a href="#cb17-211" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb17-212"><a href="#cb17-212" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for judge to provide feedback on debate progress, used in SAMRE</span></span>
<span id="cb17-213"><a href="#cb17-213" aria-hidden="true" tabindex="-1"></a><span class="st">"judge_prompt"</span>: <span class="st">"""</span></span>
<span id="cb17-214"><a href="#cb17-214" aria-hidden="true" tabindex="-1"></a><span class="st">You’re a fair, impartial judge in a debate on: "</span><span class="sc">{question}</span><span class="st">". Answer 1: "</span><span class="sc">{answer_1}</span><span class="st">".</span></span>
<span id="cb17-215"><a href="#cb17-215" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: "</span><span class="sc">{answer_2}</span><span class="st">". Your goal is to provide feedback that will help advocate groups</span></span>
<span id="cb17-216"><a href="#cb17-216" aria-hidden="true" tabindex="-1"></a><span class="st">improve and differentiate their arguments more clearly.</span></span>
<span id="cb17-217"><a href="#cb17-217" aria-hidden="true" tabindex="-1"></a><span class="st">Current round: </span><span class="sc">{current_round}</span></span>
<span id="cb17-218"><a href="#cb17-218" aria-hidden="true" tabindex="-1"></a><span class="st">Total rounds: </span><span class="sc">{total_rounds}</span></span>
<span id="cb17-219"><a href="#cb17-219" aria-hidden="true" tabindex="-1"></a><span class="st">Previous scores: </span><span class="sc">{previous_scores}</span></span>
<span id="cb17-220"><a href="#cb17-220" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 1st answer: </span><span class="sc">{defense_1}</span></span>
<span id="cb17-221"><a href="#cb17-221" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 2nd answer: </span><span class="sc">{defense_2}</span></span>
<span id="cb17-222"><a href="#cb17-222" aria-hidden="true" tabindex="-1"></a><span class="st">Provide specific, constructive feedback to help each advocate group strengthen their</span></span>
<span id="cb17-223"><a href="#cb17-223" aria-hidden="true" tabindex="-1"></a><span class="st">unique position. Encourage them to address weaknesses and highlight distinctions. Aim</span></span>
<span id="cb17-224"><a href="#cb17-224" aria-hidden="true" tabindex="-1"></a><span class="st">for your feedback to lead to more divergent scores in future rounds.</span></span>
<span id="cb17-225"><a href="#cb17-225" aria-hidden="true" tabindex="-1"></a><span class="st">Give your feedback in under 50 words:</span></span>
<span id="cb17-226"><a href="#cb17-226" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb17-227"><a href="#cb17-227" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for SAMRE method scoring</span></span>
<span id="cb17-228"><a href="#cb17-228" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_samre"</span>: <span class="st">"""</span></span>
<span id="cb17-229"><a href="#cb17-229" aria-hidden="true" tabindex="-1"></a><span class="st">You’re a critical, impartial judge in a high-stakes debate on: "</span><span class="sc">{question}</span><span class="st">". Answer</span></span>
<span id="cb17-230"><a href="#cb17-230" aria-hidden="true" tabindex="-1"></a><span class="st">1: "</span><span class="sc">{answer_1}</span><span class="st">". Answer 2: "</span><span class="sc">{answer_2}</span><span class="st">". Your goal is to provide detailed, constructive</span></span>
<span id="cb17-231"><a href="#cb17-231" aria-hidden="true" tabindex="-1"></a><span class="st">feedback that will push advocates to significantly improve their arguments.</span></span>
<span id="cb17-232"><a href="#cb17-232" aria-hidden="true" tabindex="-1"></a><span class="st">Total rounds: </span><span class="sc">{total_rounds}</span></span>
<span id="cb17-233"><a href="#cb17-233" aria-hidden="true" tabindex="-1"></a><span class="st">Previous scores: </span><span class="sc">{previous_scores}</span></span>
<span id="cb17-234"><a href="#cb17-234" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 1st answer: </span><span class="sc">{defense_1}</span></span>
<span id="cb17-235"><a href="#cb17-235" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 2nd answer: </span><span class="sc">{defense_2}</span></span>
<span id="cb17-236"><a href="#cb17-236" aria-hidden="true" tabindex="-1"></a><span class="st">Analyze each argument meticulously. Be thorough and unbiased in your assessment of:</span></span>
<span id="cb17-237"><a href="#cb17-237" aria-hidden="true" tabindex="-1"></a><span class="st">1. Relevance to the question</span></span>
<span id="cb17-238"><a href="#cb17-238" aria-hidden="true" tabindex="-1"></a><span class="st">2. Accuracy of information and use of credible sources</span></span>
<span id="cb17-239"><a href="#cb17-239" aria-hidden="true" tabindex="-1"></a><span class="st">3. Depth of analysis and completeness of argument</span></span>
<span id="cb17-240"><a href="#cb17-240" aria-hidden="true" tabindex="-1"></a><span class="st">4. Clarity of expression and logical flow</span></span>
<span id="cb17-241"><a href="#cb17-241" aria-hidden="true" tabindex="-1"></a><span class="st">5. Strength of reasoning and factual support</span></span>
<span id="cb17-242"><a href="#cb17-242" aria-hidden="true" tabindex="-1"></a><span class="st">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb17-243"><a href="#cb17-243" aria-hidden="true" tabindex="-1"></a><span class="st">For each criterion, provide a score on a scale of 1-20 and detailed justification.</span></span>
<span id="cb17-244"><a href="#cb17-244" aria-hidden="true" tabindex="-1"></a><span class="st">Scores should be given as [answer_1_score, answer_2_score] for each criterion.</span></span>
<span id="cb17-245"><a href="#cb17-245" aria-hidden="true" tabindex="-1"></a><span class="st">Your comprehensive feedback for each advocate (50 words each):</span></span>
<span id="cb17-246"><a href="#cb17-246" aria-hidden="true" tabindex="-1"></a><span class="st">Feedback for Advocate 1:</span></span>
<span id="cb17-247"><a href="#cb17-247" aria-hidden="true" tabindex="-1"></a><span class="st">Feedback for Advocate 2:</span></span>
<span id="cb17-248"><a href="#cb17-248" aria-hidden="true" tabindex="-1"></a><span class="st">Sum up the scores and return the final score tuple (score1, score2). Example: (95, 87)</span></span>
<span id="cb17-249"><a href="#cb17-249" aria-hidden="true" tabindex="-1"></a><span class="st">Your detailed scores and final tally:</span></span>
<span id="cb17-250"><a href="#cb17-250" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb17-251"><a href="#cb17-251" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for Baseline-Weak method scoring, which represents the baseline model used in the paper</span></span>
<span id="cb17-252"><a href="#cb17-252" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_baseline_weak"</span>: <span class="st">"""</span></span>
<span id="cb17-253"><a href="#cb17-253" aria-hidden="true" tabindex="-1"></a><span class="st">You are a fair, impartial judge scoring a debate on the following question:</span></span>
<span id="cb17-254"><a href="#cb17-254" aria-hidden="true" tabindex="-1"></a><span class="st">question.</span></span>
<span id="cb17-255"><a href="#cb17-255" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: </span><span class="sc">{answer_1}</span></span>
<span id="cb17-256"><a href="#cb17-256" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: </span><span class="sc">{answer_2}</span></span>
<span id="cb17-257"><a href="#cb17-257" aria-hidden="true" tabindex="-1"></a><span class="st">Score each answer on a scale of 1-20 for each of the following criteria:</span></span>
<span id="cb17-258"><a href="#cb17-258" aria-hidden="true" tabindex="-1"></a><span class="st">1. Relevance to the question</span></span>
<span id="cb17-259"><a href="#cb17-259" aria-hidden="true" tabindex="-1"></a><span class="st">2. Accuracy of information and use of credible sources</span></span>
<span id="cb17-260"><a href="#cb17-260" aria-hidden="true" tabindex="-1"></a><span class="st">3. Depth of analysis and completeness of argument</span></span>
<span id="cb17-261"><a href="#cb17-261" aria-hidden="true" tabindex="-1"></a><span class="st">4. Clarity of expression and logical flow</span></span>
<span id="cb17-262"><a href="#cb17-262" aria-hidden="true" tabindex="-1"></a><span class="st">5. Strength of reasoning and factual support</span></span>
<span id="cb17-263"><a href="#cb17-263" aria-hidden="true" tabindex="-1"></a><span class="st">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb17-264"><a href="#cb17-264" aria-hidden="true" tabindex="-1"></a><span class="st">Provide scores as [Answer1_score, Answer2_score] for each criterion in a list format,</span></span>
<span id="cb17-265"><a href="#cb17-265" aria-hidden="true" tabindex="-1"></a><span class="st">then sum for final scores. Please keep an eye on the slightest difference that should</span></span>
<span id="cb17-266"><a href="#cb17-266" aria-hidden="true" tabindex="-1"></a><span class="st">make a difference in the scoring. Don’t overthink!</span></span>
<span id="cb17-267"><a href="#cb17-267" aria-hidden="true" tabindex="-1"></a><span class="st">Relevance:</span></span>
<span id="cb17-268"><a href="#cb17-268" aria-hidden="true" tabindex="-1"></a><span class="st">Accuracy:</span></span>
<span id="cb17-269"><a href="#cb17-269" aria-hidden="true" tabindex="-1"></a><span class="st">Depth:</span></span>
<span id="cb17-270"><a href="#cb17-270" aria-hidden="true" tabindex="-1"></a><span class="st">Clarity:</span></span>
<span id="cb17-271"><a href="#cb17-271" aria-hidden="true" tabindex="-1"></a><span class="st">Logic and Factuality:</span></span>
<span id="cb17-272"><a href="#cb17-272" aria-hidden="true" tabindex="-1"></a><span class="st">Addressing opponent’s points:</span></span>
<span id="cb17-273"><a href="#cb17-273" aria-hidden="true" tabindex="-1"></a><span class="st">Final Scores (sum of above) as a tuple (example: (18, 9)):</span></span>
<span id="cb17-274"><a href="#cb17-274" aria-hidden="true" tabindex="-1"></a><span class="st">Explain your scoring, focusing on why one answer is better than the other based on the</span></span>
<span id="cb17-275"><a href="#cb17-275" aria-hidden="true" tabindex="-1"></a><span class="st">criteria above. Keep your explanation concise but informative.</span></span>
<span id="cb17-276"><a href="#cb17-276" aria-hidden="true" tabindex="-1"></a><span class="st">Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).</span></span>
<span id="cb17-277"><a href="#cb17-277" aria-hidden="true" tabindex="-1"></a><span class="st">Example: (18, 9)</span></span>
<span id="cb17-278"><a href="#cb17-278" aria-hidden="true" tabindex="-1"></a><span class="st">Your scores and explanation:</span></span>
<span id="cb17-279"><a href="#cb17-279" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb17-280"><a href="#cb17-280" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for Baseline-Strong method scoring, which implements what I consider to be standard best practices for prompt engineering</span></span>
<span id="cb17-281"><a href="#cb17-281" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_baseline_strong"</span>: <span class="st">"""</span></span>
<span id="cb17-282"><a href="#cb17-282" aria-hidden="true" tabindex="-1"></a><span class="st">You are a fair, impartial judge scoring a debate on Question.</span></span>
<span id="cb17-283"><a href="#cb17-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-284"><a href="#cb17-284" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Question&gt;</span></span>
<span id="cb17-285"><a href="#cb17-285" aria-hidden="true" tabindex="-1"></a><span class="sc">{question}</span></span>
<span id="cb17-286"><a href="#cb17-286" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Question&gt;</span></span>
<span id="cb17-287"><a href="#cb17-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-288"><a href="#cb17-288" aria-hidden="true" tabindex="-1"></a><span class="st">Two Answers have been given to the Question.</span></span>
<span id="cb17-289"><a href="#cb17-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-290"><a href="#cb17-290" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1&gt;</span></span>
<span id="cb17-291"><a href="#cb17-291" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_1}</span></span>
<span id="cb17-292"><a href="#cb17-292" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Answer1&gt;</span></span>
<span id="cb17-293"><a href="#cb17-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-294"><a href="#cb17-294" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2&gt;</span></span>
<span id="cb17-295"><a href="#cb17-295" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_2}</span></span>
<span id="cb17-296"><a href="#cb17-296" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Answer2&gt;</span></span>
<span id="cb17-297"><a href="#cb17-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-298"><a href="#cb17-298" aria-hidden="true" tabindex="-1"></a><span class="st">The Answers are being judged on the following Criteria:</span></span>
<span id="cb17-299"><a href="#cb17-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-300"><a href="#cb17-300" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criteria&gt;</span></span>
<span id="cb17-301"><a href="#cb17-301" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;</span></span>
<span id="cb17-302"><a href="#cb17-302" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;</span></span>
<span id="cb17-303"><a href="#cb17-303" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;</span></span>
<span id="cb17-304"><a href="#cb17-304" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;</span></span>
<span id="cb17-305"><a href="#cb17-305" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;</span></span>
<span id="cb17-306"><a href="#cb17-306" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criteria&gt;</span></span>
<span id="cb17-307"><a href="#cb17-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-308"><a href="#cb17-308" aria-hidden="true" tabindex="-1"></a><span class="st">For each Criterion, briefly analyze the performance of </span></span>
<span id="cb17-309"><a href="#cb17-309" aria-hidden="true" tabindex="-1"></a><span class="st">the two Answers, then give a score between 1 and 10.</span></span>
<span id="cb17-310"><a href="#cb17-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-311"><a href="#cb17-311" aria-hidden="true" tabindex="-1"></a><span class="st">Respond as follows:</span></span>
<span id="cb17-312"><a href="#cb17-312" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion1&gt;</span></span>
<span id="cb17-313"><a href="#cb17-313" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;</span></span>
<span id="cb17-314"><a href="#cb17-314" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Analysis&gt;</span></span>
<span id="cb17-315"><a href="#cb17-315" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb17-316"><a href="#cb17-316" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb17-317"><a href="#cb17-317" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Analysis&gt;</span></span>
<span id="cb17-318"><a href="#cb17-318" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Scores&gt;</span></span>
<span id="cb17-319"><a href="#cb17-319" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb17-320"><a href="#cb17-320" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb17-321"><a href="#cb17-321" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Scores&gt;</span></span>
<span id="cb17-322"><a href="#cb17-322" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criterion1&gt;</span></span>
<span id="cb17-323"><a href="#cb17-323" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion2&gt;</span></span>
<span id="cb17-324"><a href="#cb17-324" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;</span></span>
<span id="cb17-325"><a href="#cb17-325" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Analysis&gt;</span></span>
<span id="cb17-326"><a href="#cb17-326" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb17-327"><a href="#cb17-327" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb17-328"><a href="#cb17-328" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Analysis&gt;</span></span>
<span id="cb17-329"><a href="#cb17-329" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Scores&gt;</span></span>
<span id="cb17-330"><a href="#cb17-330" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb17-331"><a href="#cb17-331" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb17-332"><a href="#cb17-332" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Scores&gt;</span></span>
<span id="cb17-333"><a href="#cb17-333" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criterion2&gt;</span></span>
<span id="cb17-334"><a href="#cb17-334" aria-hidden="true" tabindex="-1"></a><span class="st">...</span></span>
<span id="cb17-335"><a href="#cb17-335" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb17-336"><a href="#cb17-336" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-337"><a href="#cb17-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-338"><a href="#cb17-338" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb17-339"><a href="#cb17-339" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Memory:</span>
<span id="cb17-340"><a href="#cb17-340" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stores debate history including arguments, scores, and feedback for each round, used in SAMRE"""</span></span>
<span id="cb17-341"><a href="#cb17-341" aria-hidden="true" tabindex="-1"></a>    arguments: List[Tuple[<span class="bu">str</span>, <span class="bu">str</span>]] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb17-342"><a href="#cb17-342" aria-hidden="true" tabindex="-1"></a>    scores: List[Tuple[<span class="bu">float</span>, <span class="bu">float</span>]] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb17-343"><a href="#cb17-343" aria-hidden="true" tabindex="-1"></a>    feedback: List[<span class="bu">str</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb17-344"><a href="#cb17-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-345"><a href="#cb17-345" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelEvaluator:</span>
<span id="cb17-346"><a href="#cb17-346" aria-hidden="true" tabindex="-1"></a>    <span class="at">@classmethod</span></span>
<span id="cb17-347"><a href="#cb17-347" aria-hidden="true" tabindex="-1"></a>    <span class="at">@asynccontextmanager</span></span>
<span id="cb17-348"><a href="#cb17-348" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> create(cls, mode<span class="op">=</span><span class="st">"samre"</span>, model<span class="op">=</span><span class="st">"gpt-4o-mini"</span>, logging_level<span class="op">=</span>logging.WARNING):</span>
<span id="cb17-349"><a href="#cb17-349" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Factory method to create evaluator instance with proper async context management"""</span></span>
<span id="cb17-350"><a href="#cb17-350" aria-hidden="true" tabindex="-1"></a>        instance <span class="op">=</span> cls(mode<span class="op">=</span>mode, model<span class="op">=</span>model, logging_level<span class="op">=</span>logging_level)</span>
<span id="cb17-351"><a href="#cb17-351" aria-hidden="true" tabindex="-1"></a>        instance.client <span class="op">=</span> AsyncOpenAI()</span>
<span id="cb17-352"><a href="#cb17-352" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb17-353"><a href="#cb17-353" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> instance</span>
<span id="cb17-354"><a href="#cb17-354" aria-hidden="true" tabindex="-1"></a>        <span class="cf">finally</span>:</span>
<span id="cb17-355"><a href="#cb17-355" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> instance.client.close()</span>
<span id="cb17-356"><a href="#cb17-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-357"><a href="#cb17-357" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _setup_logger(<span class="va">self</span>, logging_level):</span>
<span id="cb17-358"><a href="#cb17-358" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Setup logger with word wrapping."""</span></span>
<span id="cb17-359"><a href="#cb17-359" aria-hidden="true" tabindex="-1"></a>        logger <span class="op">=</span> logging.getLogger(<span class="va">__name__</span>)</span>
<span id="cb17-360"><a href="#cb17-360" aria-hidden="true" tabindex="-1"></a>        logger.setLevel(logging_level)</span>
<span id="cb17-361"><a href="#cb17-361" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> logger.handlers:</span>
<span id="cb17-362"><a href="#cb17-362" aria-hidden="true" tabindex="-1"></a>            handler <span class="op">=</span> logging.StreamHandler()</span>
<span id="cb17-363"><a href="#cb17-363" aria-hidden="true" tabindex="-1"></a>            <span class="kw">class</span> WrapFormatter(logging.Formatter):</span>
<span id="cb17-364"><a href="#cb17-364" aria-hidden="true" tabindex="-1"></a>                <span class="kw">def</span> <span class="bu">format</span>(<span class="va">self</span>, record):</span>
<span id="cb17-365"><a href="#cb17-365" aria-hidden="true" tabindex="-1"></a>                    <span class="im">import</span> textwrap</span>
<span id="cb17-366"><a href="#cb17-366" aria-hidden="true" tabindex="-1"></a>                    message <span class="op">=</span> <span class="bu">super</span>().<span class="bu">format</span>(record)</span>
<span id="cb17-367"><a href="#cb17-367" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(textwrap.fill(line, width<span class="op">=</span><span class="dv">80</span>) </span>
<span id="cb17-368"><a href="#cb17-368" aria-hidden="true" tabindex="-1"></a>                                <span class="cf">for</span> line <span class="kw">in</span> message.split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>))</span>
<span id="cb17-369"><a href="#cb17-369" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-370"><a href="#cb17-370" aria-hidden="true" tabindex="-1"></a>            formatter <span class="op">=</span> WrapFormatter(<span class="st">'</span><span class="sc">%(message)s</span><span class="st">'</span>)</span>
<span id="cb17-371"><a href="#cb17-371" aria-hidden="true" tabindex="-1"></a>            handler.setFormatter(formatter)</span>
<span id="cb17-372"><a href="#cb17-372" aria-hidden="true" tabindex="-1"></a>            logger.addHandler(handler)</span>
<span id="cb17-373"><a href="#cb17-373" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logger</span>
<span id="cb17-374"><a href="#cb17-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-375"><a href="#cb17-375" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mode<span class="op">=</span><span class="st">"samre"</span>, model<span class="op">=</span><span class="st">"gpt-4o-mini"</span>, logging_level<span class="op">=</span>logging.WARNING):</span>
<span id="cb17-376"><a href="#cb17-376" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mode <span class="op">=</span> mode</span>
<span id="cb17-377"><a href="#cb17-377" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb17-378"><a href="#cb17-378" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Modify to handle both baseline modes</span></span>
<span id="cb17-379"><a href="#cb17-379" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_rounds <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> mode.startswith(<span class="st">"baseline"</span>) <span class="cf">else</span> <span class="dv">4</span></span>
<span id="cb17-380"><a href="#cb17-380" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger <span class="op">=</span> <span class="va">self</span>._setup_logger(logging_level)</span>
<span id="cb17-381"><a href="#cb17-381" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-382"><a href="#cb17-382" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize all prompts</span></span>
<span id="cb17-383"><a href="#cb17-383" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.defend_prompt <span class="op">=</span> PROMPTS[<span class="st">"defend_prompt"</span>]</span>
<span id="cb17-384"><a href="#cb17-384" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.judge_prompt <span class="op">=</span> PROMPTS[<span class="st">"judge_prompt"</span>]</span>
<span id="cb17-385"><a href="#cb17-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-386"><a href="#cb17-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-387"><a href="#cb17-387" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> get_completion(<span class="va">self</span>, prompt: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb17-388"><a href="#cb17-388" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get a completion from the OpenAI API."""</span></span>
<span id="cb17-389"><a href="#cb17-389" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.client:</span>
<span id="cb17-390"><a href="#cb17-390" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'"</span>)</span>
<span id="cb17-391"><a href="#cb17-391" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-392"><a href="#cb17-392" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.client.chat.completions.create(</span>
<span id="cb17-393"><a href="#cb17-393" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span><span class="va">self</span>.model,</span>
<span id="cb17-394"><a href="#cb17-394" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: prompt}],</span>
<span id="cb17-395"><a href="#cb17-395" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span><span class="dv">0</span></span>
<span id="cb17-396"><a href="#cb17-396" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-397"><a href="#cb17-397" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb17-398"><a href="#cb17-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-399"><a href="#cb17-399" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _extract_final_scores(<span class="va">self</span>, score_response: <span class="bu">str</span>) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb17-400"><a href="#cb17-400" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extracts final scores from model response based on evaluation mode"""</span></span>
<span id="cb17-401"><a href="#cb17-401" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"samre"</span>:</span>
<span id="cb17-402"><a href="#cb17-402" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Look for final tuple in format (score1, score2)</span></span>
<span id="cb17-403"><a href="#cb17-403" aria-hidden="true" tabindex="-1"></a>            tuple_pattern <span class="op">=</span> <span class="vs">r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'</span></span>
<span id="cb17-404"><a href="#cb17-404" aria-hidden="true" tabindex="-1"></a>            match <span class="op">=</span> re.search(tuple_pattern, score_response)</span>
<span id="cb17-405"><a href="#cb17-405" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match:</span>
<span id="cb17-406"><a href="#cb17-406" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> (<span class="bu">float</span>(match.group(<span class="dv">1</span>)), <span class="bu">float</span>(match.group(<span class="dv">2</span>)))</span>
<span id="cb17-407"><a href="#cb17-407" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find score tuple in SAMRE response"</span>)</span>
<span id="cb17-408"><a href="#cb17-408" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-409"><a href="#cb17-409" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_weak"</span>:</span>
<span id="cb17-410"><a href="#cb17-410" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Look for final tuple in format (score1, score2)</span></span>
<span id="cb17-411"><a href="#cb17-411" aria-hidden="true" tabindex="-1"></a>            tuple_pattern <span class="op">=</span> <span class="vs">r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'</span></span>
<span id="cb17-412"><a href="#cb17-412" aria-hidden="true" tabindex="-1"></a>            match <span class="op">=</span> re.search(tuple_pattern, score_response)</span>
<span id="cb17-413"><a href="#cb17-413" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match:</span>
<span id="cb17-414"><a href="#cb17-414" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> (<span class="bu">float</span>(match.group(<span class="dv">1</span>)), <span class="bu">float</span>(match.group(<span class="dv">2</span>)))</span>
<span id="cb17-415"><a href="#cb17-415" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find score tuple in weak baseline response"</span>)</span>
<span id="cb17-416"><a href="#cb17-416" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-417"><a href="#cb17-417" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_strong"</span>:</span>
<span id="cb17-418"><a href="#cb17-418" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use XML parsing for strong baseline</span></span>
<span id="cb17-419"><a href="#cb17-419" aria-hidden="true" tabindex="-1"></a>            score_a_pattern <span class="op">=</span> <span class="vs">r'&lt;Answer1Score&gt;\s*(\d+\.?\d*)\s*&lt;/Answer1Score&gt;'</span></span>
<span id="cb17-420"><a href="#cb17-420" aria-hidden="true" tabindex="-1"></a>            score_b_pattern <span class="op">=</span> <span class="vs">r'&lt;Answer2Score&gt;\s*(\d+\.?\d*)\s*&lt;/Answer2Score&gt;'</span></span>
<span id="cb17-421"><a href="#cb17-421" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-422"><a href="#cb17-422" aria-hidden="true" tabindex="-1"></a>            scores_a <span class="op">=</span> [<span class="bu">float</span>(match.group(<span class="dv">1</span>)) <span class="cf">for</span> match <span class="kw">in</span> re.finditer(score_a_pattern, score_response)]</span>
<span id="cb17-423"><a href="#cb17-423" aria-hidden="true" tabindex="-1"></a>            scores_b <span class="op">=</span> [<span class="bu">float</span>(match.group(<span class="dv">1</span>)) <span class="cf">for</span> match <span class="kw">in</span> re.finditer(score_b_pattern, score_response)]</span>
<span id="cb17-424"><a href="#cb17-424" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-425"><a href="#cb17-425" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> scores_a <span class="kw">or</span> <span class="kw">not</span> scores_b:</span>
<span id="cb17-426"><a href="#cb17-426" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find scores for both candidates"</span>)</span>
<span id="cb17-427"><a href="#cb17-427" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-428"><a href="#cb17-428" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(scores_a) <span class="op">!=</span> <span class="bu">len</span>(scores_b):</span>
<span id="cb17-429"><a href="#cb17-429" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Mismatched number of scores: A=</span><span class="sc">{</span><span class="bu">len</span>(scores_a)<span class="sc">}</span><span class="ss">, B=</span><span class="sc">{</span><span class="bu">len</span>(scores_b)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-430"><a href="#cb17-430" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-431"><a href="#cb17-431" aria-hidden="true" tabindex="-1"></a>            final_score_a <span class="op">=</span> <span class="bu">sum</span>(scores_a) <span class="op">/</span> <span class="bu">len</span>(scores_a)</span>
<span id="cb17-432"><a href="#cb17-432" aria-hidden="true" tabindex="-1"></a>            final_score_b <span class="op">=</span> <span class="bu">sum</span>(scores_b) <span class="op">/</span> <span class="bu">len</span>(scores_b)</span>
<span id="cb17-433"><a href="#cb17-433" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-434"><a href="#cb17-434" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (final_score_a, final_score_b)</span>
<span id="cb17-435"><a href="#cb17-435" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-436"><a href="#cb17-436" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-437"><a href="#cb17-437" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Unknown mode: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>mode<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-438"><a href="#cb17-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-439"><a href="#cb17-439" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> evaluate(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, num_rounds: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb17-440"><a href="#cb17-440" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Main evaluation entry point that routes to appropriate evaluation method based on mode"""</span></span>
<span id="cb17-441"><a href="#cb17-441" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.client:</span>
<span id="cb17-442"><a href="#cb17-442" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'"</span>)</span>
<span id="cb17-443"><a href="#cb17-443" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-444"><a href="#cb17-444" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mode.startswith(<span class="st">"baseline"</span>):</span>
<span id="cb17-445"><a href="#cb17-445" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">=== Starting </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>mode<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Evaluation ===</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb17-446"><a href="#cb17-446" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>._evaluate_baseline(question, answer_1, answer_2, num_rounds)</span>
<span id="cb17-447"><a href="#cb17-447" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-448"><a href="#cb17-448" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting SAMRE Evaluation ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb17-449"><a href="#cb17-449" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>._evaluate_samre(question, answer_1, answer_2)</span>
<span id="cb17-450"><a href="#cb17-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-451"><a href="#cb17-451" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _evaluate_baseline(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, num_rounds: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb17-452"><a href="#cb17-452" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implements baseline evaluation methods (both weak and strong)"""</span></span>
<span id="cb17-453"><a href="#cb17-453" aria-hidden="true" tabindex="-1"></a>        score_history <span class="op">=</span> []</span>
<span id="cb17-454"><a href="#cb17-454" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-455"><a href="#cb17-455" aria-hidden="true" tabindex="-1"></a>        num_rounds <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_weak"</span> <span class="cf">else</span> num_rounds</span>
<span id="cb17-456"><a href="#cb17-456" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_rounds):</span>
<span id="cb17-457"><a href="#cb17-457" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Select appropriate prompt based on mode</span></span>
<span id="cb17-458"><a href="#cb17-458" aria-hidden="true" tabindex="-1"></a>            prompt_key <span class="op">=</span> <span class="st">"score_prompt_"</span> <span class="op">+</span> <span class="va">self</span>.mode</span>
<span id="cb17-459"><a href="#cb17-459" aria-hidden="true" tabindex="-1"></a>            score_prompt <span class="op">=</span> PROMPTS[prompt_key].<span class="bu">format</span>(</span>
<span id="cb17-460"><a href="#cb17-460" aria-hidden="true" tabindex="-1"></a>                question<span class="op">=</span>question,</span>
<span id="cb17-461"><a href="#cb17-461" aria-hidden="true" tabindex="-1"></a>                answer_1<span class="op">=</span>answer_1,</span>
<span id="cb17-462"><a href="#cb17-462" aria-hidden="true" tabindex="-1"></a>                answer_2<span class="op">=</span>answer_2</span>
<span id="cb17-463"><a href="#cb17-463" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-464"><a href="#cb17-464" aria-hidden="true" tabindex="-1"></a>            score_response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(score_prompt)</span>
<span id="cb17-465"><a href="#cb17-465" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"Score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-466"><a href="#cb17-466" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-467"><a href="#cb17-467" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb17-468"><a href="#cb17-468" aria-hidden="true" tabindex="-1"></a>                round_scores <span class="op">=</span> <span class="va">self</span>._extract_final_scores(score_response)</span>
<span id="cb17-469"><a href="#cb17-469" aria-hidden="true" tabindex="-1"></a>                score_history.append(<span class="bu">list</span>(round_scores))</span>
<span id="cb17-470"><a href="#cb17-470" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-471"><a href="#cb17-471" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.error(<span class="ss">f"Score parsing error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-472"><a href="#cb17-472" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.error(<span class="ss">f"Raw score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-473"><a href="#cb17-473" aria-hidden="true" tabindex="-1"></a>                score_history.append([<span class="fl">10.0</span>, <span class="fl">10.0</span>])</span>
<span id="cb17-474"><a href="#cb17-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-475"><a href="#cb17-475" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate average scores across all rounds</span></span>
<span id="cb17-476"><a href="#cb17-476" aria-hidden="true" tabindex="-1"></a>        avg_scores <span class="op">=</span> [</span>
<span id="cb17-477"><a href="#cb17-477" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span>(scores[i] <span class="cf">for</span> scores <span class="kw">in</span> score_history) <span class="op">/</span> <span class="bu">len</span>(score_history)</span>
<span id="cb17-478"><a href="#cb17-478" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb17-479"><a href="#cb17-479" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb17-480"><a href="#cb17-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-481"><a href="#cb17-481" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Determine winner based on average scores</span></span>
<span id="cb17-482"><a href="#cb17-482" aria-hidden="true" tabindex="-1"></a>        winner <span class="op">=</span> (</span>
<span id="cb17-483"><a href="#cb17-483" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&gt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb17-484"><a href="#cb17-484" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&lt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb17-485"><a href="#cb17-485" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'tie'</span></span>
<span id="cb17-486"><a href="#cb17-486" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-487"><a href="#cb17-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-488"><a href="#cb17-488" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb17-489"><a href="#cb17-489" aria-hidden="true" tabindex="-1"></a>            <span class="st">"winner"</span>: winner,</span>
<span id="cb17-490"><a href="#cb17-490" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_scores"</span>: [<span class="bu">round</span>(score, <span class="dv">2</span>) <span class="cf">for</span> score <span class="kw">in</span> avg_scores] ,</span>
<span id="cb17-491"><a href="#cb17-491" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rounds"</span>: <span class="bu">len</span>(score_history),</span>
<span id="cb17-492"><a href="#cb17-492" aria-hidden="true" tabindex="-1"></a>            <span class="st">"score_history"</span>: score_history,</span>
<span id="cb17-493"><a href="#cb17-493" aria-hidden="true" tabindex="-1"></a>            <span class="st">"full_response"</span>: score_response  <span class="co"># Include the final response for analysis</span></span>
<span id="cb17-494"><a href="#cb17-494" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb17-495"><a href="#cb17-495" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-496"><a href="#cb17-496" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _evaluate_samre(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb17-497"><a href="#cb17-497" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implements SAMRE evaluation with multi-round debate process</span></span>
<span id="cb17-498"><a href="#cb17-498" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb17-499"><a href="#cb17-499" aria-hidden="true" tabindex="-1"></a><span class="co">        Flow:</span></span>
<span id="cb17-500"><a href="#cb17-500" aria-hidden="true" tabindex="-1"></a><span class="co">        1. Get defenses from both advocates</span></span>
<span id="cb17-501"><a href="#cb17-501" aria-hidden="true" tabindex="-1"></a><span class="co">        2. Judge provides feedback and scores</span></span>
<span id="cb17-502"><a href="#cb17-502" aria-hidden="true" tabindex="-1"></a><span class="co">        3. Repeat until max rounds or convergence</span></span>
<span id="cb17-503"><a href="#cb17-503" aria-hidden="true" tabindex="-1"></a><span class="co">        4. Return averaged results</span></span>
<span id="cb17-504"><a href="#cb17-504" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb17-505"><a href="#cb17-505" aria-hidden="true" tabindex="-1"></a>        local_memory <span class="op">=</span> Memory()</span>
<span id="cb17-506"><a href="#cb17-506" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-507"><a href="#cb17-507" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting SAMRE Evaluation ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb17-508"><a href="#cb17-508" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-509"><a href="#cb17-509" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> round_num <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.max_rounds):</span>
<span id="cb17-510"><a href="#cb17-510" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Round </span><span class="sc">{</span>round_num <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb17-511"><a href="#cb17-511" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-512"><a href="#cb17-512" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>._run_debate_round(</span>
<span id="cb17-513"><a href="#cb17-513" aria-hidden="true" tabindex="-1"></a>                question,</span>
<span id="cb17-514"><a href="#cb17-514" aria-hidden="true" tabindex="-1"></a>                answer_1, </span>
<span id="cb17-515"><a href="#cb17-515" aria-hidden="true" tabindex="-1"></a>                answer_2, </span>
<span id="cb17-516"><a href="#cb17-516" aria-hidden="true" tabindex="-1"></a>                round_num,</span>
<span id="cb17-517"><a href="#cb17-517" aria-hidden="true" tabindex="-1"></a>                local_memory</span>
<span id="cb17-518"><a href="#cb17-518" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-519"><a href="#cb17-519" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-520"><a href="#cb17-520" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._has_scores_converged(round_num, local_memory):</span>
<span id="cb17-521"><a href="#cb17-521" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">Scores have converged - ending debate early."</span>)</span>
<span id="cb17-522"><a href="#cb17-522" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb17-523"><a href="#cb17-523" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-524"><a href="#cb17-524" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._prepare_results(local_memory)</span>
<span id="cb17-525"><a href="#cb17-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-526"><a href="#cb17-526" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> defend_answer(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, </span>
<span id="cb17-527"><a href="#cb17-527" aria-hidden="true" tabindex="-1"></a>                        advocate_id: <span class="bu">int</span>, feedback: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>, </span>
<span id="cb17-528"><a href="#cb17-528" aria-hidden="true" tabindex="-1"></a>                        opponent_argument: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>,</span>
<span id="cb17-529"><a href="#cb17-529" aria-hidden="true" tabindex="-1"></a>                        team_arguments: List[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb17-530"><a href="#cb17-530" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get defense from an advocate.</span></span>
<span id="cb17-531"><a href="#cb17-531" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb17-532"><a href="#cb17-532" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb17-533"><a href="#cb17-533" aria-hidden="true" tabindex="-1"></a><span class="co">            question: The question being debated</span></span>
<span id="cb17-534"><a href="#cb17-534" aria-hidden="true" tabindex="-1"></a><span class="co">            answer_1: First answer in the debate</span></span>
<span id="cb17-535"><a href="#cb17-535" aria-hidden="true" tabindex="-1"></a><span class="co">            answer_2: Second answer in the debate</span></span>
<span id="cb17-536"><a href="#cb17-536" aria-hidden="true" tabindex="-1"></a><span class="co">            advocate_id: Which advocate (1 or 2) is defending</span></span>
<span id="cb17-537"><a href="#cb17-537" aria-hidden="true" tabindex="-1"></a><span class="co">            feedback: Previous feedback from judge</span></span>
<span id="cb17-538"><a href="#cb17-538" aria-hidden="true" tabindex="-1"></a><span class="co">            opponent_argument: Last argument from opponent</span></span>
<span id="cb17-539"><a href="#cb17-539" aria-hidden="true" tabindex="-1"></a><span class="co">            team_arguments: List of previous arguments from this advocate's team</span></span>
<span id="cb17-540"><a href="#cb17-540" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb17-541"><a href="#cb17-541" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> team_arguments <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb17-542"><a href="#cb17-542" aria-hidden="true" tabindex="-1"></a>            team_arguments <span class="op">=</span> []</span>
<span id="cb17-543"><a href="#cb17-543" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-544"><a href="#cb17-544" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Map answers based on advocate_id</span></span>
<span id="cb17-545"><a href="#cb17-545" aria-hidden="true" tabindex="-1"></a>        answer <span class="op">=</span> answer_1 <span class="cf">if</span> advocate_id <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> answer_2</span>
<span id="cb17-546"><a href="#cb17-546" aria-hidden="true" tabindex="-1"></a>        opponent_answer <span class="op">=</span> answer_2 <span class="cf">if</span> advocate_id <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> answer_1</span>
<span id="cb17-547"><a href="#cb17-547" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-548"><a href="#cb17-548" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> <span class="va">self</span>.defend_prompt.<span class="bu">format</span>(</span>
<span id="cb17-549"><a href="#cb17-549" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb17-550"><a href="#cb17-550" aria-hidden="true" tabindex="-1"></a>            advocate_id<span class="op">=</span>advocate_id,</span>
<span id="cb17-551"><a href="#cb17-551" aria-hidden="true" tabindex="-1"></a>            answer<span class="op">=</span>answer,  <span class="co"># The answer this advocate is defending</span></span>
<span id="cb17-552"><a href="#cb17-552" aria-hidden="true" tabindex="-1"></a>            opponent_answer<span class="op">=</span>opponent_answer,  <span class="co"># The opposing answer</span></span>
<span id="cb17-553"><a href="#cb17-553" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>feedback,</span>
<span id="cb17-554"><a href="#cb17-554" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>opponent_argument,</span>
<span id="cb17-555"><a href="#cb17-555" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(team_arguments)</span>
<span id="cb17-556"><a href="#cb17-556" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-557"><a href="#cb17-557" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(prompt)</span>
<span id="cb17-558"><a href="#cb17-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-559"><a href="#cb17-559" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> judge_debate(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>,</span>
<span id="cb17-560"><a href="#cb17-560" aria-hidden="true" tabindex="-1"></a>                          defense_1: <span class="bu">str</span>, defense_2: <span class="bu">str</span>, </span>
<span id="cb17-561"><a href="#cb17-561" aria-hidden="true" tabindex="-1"></a>                          current_round: <span class="bu">int</span>,</span>
<span id="cb17-562"><a href="#cb17-562" aria-hidden="true" tabindex="-1"></a>                          memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, Tuple[<span class="bu">float</span>, <span class="bu">float</span>]]:</span>
<span id="cb17-563"><a href="#cb17-563" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Judge the debate between two answers."""</span></span>
<span id="cb17-564"><a href="#cb17-564" aria-hidden="true" tabindex="-1"></a>        feedback_prompt <span class="op">=</span> <span class="va">self</span>.judge_prompt.<span class="bu">format</span>(</span>
<span id="cb17-565"><a href="#cb17-565" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb17-566"><a href="#cb17-566" aria-hidden="true" tabindex="-1"></a>            answer_1<span class="op">=</span>answer_1,</span>
<span id="cb17-567"><a href="#cb17-567" aria-hidden="true" tabindex="-1"></a>            answer_2<span class="op">=</span>answer_2,</span>
<span id="cb17-568"><a href="#cb17-568" aria-hidden="true" tabindex="-1"></a>            current_round<span class="op">=</span>current_round,</span>
<span id="cb17-569"><a href="#cb17-569" aria-hidden="true" tabindex="-1"></a>            total_rounds<span class="op">=</span><span class="va">self</span>.max_rounds,</span>
<span id="cb17-570"><a href="#cb17-570" aria-hidden="true" tabindex="-1"></a>            previous_scores<span class="op">=</span>memory.scores,</span>
<span id="cb17-571"><a href="#cb17-571" aria-hidden="true" tabindex="-1"></a>            defense_1<span class="op">=</span>defense_1,</span>
<span id="cb17-572"><a href="#cb17-572" aria-hidden="true" tabindex="-1"></a>            defense_2<span class="op">=</span>defense_2</span>
<span id="cb17-573"><a href="#cb17-573" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-574"><a href="#cb17-574" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(feedback_prompt)</span>
<span id="cb17-575"><a href="#cb17-575" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-576"><a href="#cb17-576" aria-hidden="true" tabindex="-1"></a>        score_prompt <span class="op">=</span> PROMPTS[<span class="st">"score_prompt_samre"</span>].<span class="bu">format</span>(</span>
<span id="cb17-577"><a href="#cb17-577" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb17-578"><a href="#cb17-578" aria-hidden="true" tabindex="-1"></a>            answer_1<span class="op">=</span>answer_1,</span>
<span id="cb17-579"><a href="#cb17-579" aria-hidden="true" tabindex="-1"></a>            answer_2<span class="op">=</span>answer_2,</span>
<span id="cb17-580"><a href="#cb17-580" aria-hidden="true" tabindex="-1"></a>            defense_1<span class="op">=</span>defense_1,</span>
<span id="cb17-581"><a href="#cb17-581" aria-hidden="true" tabindex="-1"></a>            defense_2<span class="op">=</span>defense_2,</span>
<span id="cb17-582"><a href="#cb17-582" aria-hidden="true" tabindex="-1"></a>            total_rounds<span class="op">=</span><span class="va">self</span>.max_rounds,</span>
<span id="cb17-583"><a href="#cb17-583" aria-hidden="true" tabindex="-1"></a>            previous_scores<span class="op">=</span>memory.scores,</span>
<span id="cb17-584"><a href="#cb17-584" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>feedback</span>
<span id="cb17-585"><a href="#cb17-585" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-586"><a href="#cb17-586" aria-hidden="true" tabindex="-1"></a>        score_response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(score_prompt)    </span>
<span id="cb17-587"><a href="#cb17-587" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-588"><a href="#cb17-588" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-589"><a href="#cb17-589" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb17-590"><a href="#cb17-590" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="va">self</span>._extract_final_scores(score_response)</span>
<span id="cb17-591"><a href="#cb17-591" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-592"><a href="#cb17-592" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Score parsing error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-593"><a href="#cb17-593" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Raw score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-594"><a href="#cb17-594" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> (<span class="fl">10.0</span>, <span class="fl">10.0</span>)</span>
<span id="cb17-595"><a href="#cb17-595" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-596"><a href="#cb17-596" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> feedback, scores</span>
<span id="cb17-597"><a href="#cb17-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-598"><a href="#cb17-598" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _run_debate_round(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, </span>
<span id="cb17-599"><a href="#cb17-599" aria-hidden="true" tabindex="-1"></a>                               round_num: <span class="bu">int</span>, memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb17-600"><a href="#cb17-600" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Executes single debate round in SAMRE evaluation"""</span></span>
<span id="cb17-601"><a href="#cb17-601" aria-hidden="true" tabindex="-1"></a>        defenses <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>._get_advocate_defenses(question, answer_1, answer_2, memory)</span>
<span id="cb17-602"><a href="#cb17-602" aria-hidden="true" tabindex="-1"></a>        memory.arguments.append(defenses)</span>
<span id="cb17-603"><a href="#cb17-603" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-604"><a href="#cb17-604" aria-hidden="true" tabindex="-1"></a>        feedback, scores <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.judge_debate(</span>
<span id="cb17-605"><a href="#cb17-605" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, defenses[<span class="dv">0</span>], defenses[<span class="dv">1</span>], round_num <span class="op">+</span> <span class="dv">1</span>, memory</span>
<span id="cb17-606"><a href="#cb17-606" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-607"><a href="#cb17-607" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-608"><a href="#cb17-608" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._store_round_results(feedback, scores, memory)</span>
<span id="cb17-609"><a href="#cb17-609" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._display_round_results(defenses, feedback, scores)</span>
<span id="cb17-610"><a href="#cb17-610" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-611"><a href="#cb17-611" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scores</span>
<span id="cb17-612"><a href="#cb17-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-613"><a href="#cb17-613" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _get_advocate_defenses(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>,</span>
<span id="cb17-614"><a href="#cb17-614" aria-hidden="true" tabindex="-1"></a>                                   memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, <span class="bu">str</span>]:</span>
<span id="cb17-615"><a href="#cb17-615" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get defenses from both advocates."""</span></span>
<span id="cb17-616"><a href="#cb17-616" aria-hidden="true" tabindex="-1"></a>        defense_1 <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.defend_answer(</span>
<span id="cb17-617"><a href="#cb17-617" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, <span class="dv">1</span>,</span>
<span id="cb17-618"><a href="#cb17-618" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>memory.feedback[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory.feedback <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb17-619"><a href="#cb17-619" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>memory.arguments[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>] <span class="cf">if</span> memory.arguments <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb17-620"><a href="#cb17-620" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span>[args[<span class="dv">0</span>] <span class="cf">for</span> args <span class="kw">in</span> memory.arguments]</span>
<span id="cb17-621"><a href="#cb17-621" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-622"><a href="#cb17-622" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-623"><a href="#cb17-623" aria-hidden="true" tabindex="-1"></a>        defense_2 <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.defend_answer(</span>
<span id="cb17-624"><a href="#cb17-624" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, <span class="dv">2</span>,</span>
<span id="cb17-625"><a href="#cb17-625" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>memory.feedback[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory.feedback <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb17-626"><a href="#cb17-626" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>memory.arguments[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="cf">if</span> memory.arguments <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb17-627"><a href="#cb17-627" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span>[args[<span class="dv">1</span>] <span class="cf">for</span> args <span class="kw">in</span> memory.arguments]</span>
<span id="cb17-628"><a href="#cb17-628" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-629"><a href="#cb17-629" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-630"><a href="#cb17-630" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (defense_1, defense_2)</span>
<span id="cb17-631"><a href="#cb17-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-632"><a href="#cb17-632" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _store_round_results(<span class="va">self</span>, feedback: <span class="bu">str</span>, scores: Tuple[<span class="bu">float</span>, <span class="bu">float</span>],</span>
<span id="cb17-633"><a href="#cb17-633" aria-hidden="true" tabindex="-1"></a>                           memory: Memory) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb17-634"><a href="#cb17-634" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Store feedback and scores from the round."""</span></span>
<span id="cb17-635"><a href="#cb17-635" aria-hidden="true" tabindex="-1"></a>        memory.feedback.append(feedback)</span>
<span id="cb17-636"><a href="#cb17-636" aria-hidden="true" tabindex="-1"></a>        memory.scores.append(scores)</span>
<span id="cb17-637"><a href="#cb17-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-638"><a href="#cb17-638" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _display_round_results(<span class="va">self</span>, defenses: Tuple[<span class="bu">str</span>, <span class="bu">str</span>], </span>
<span id="cb17-639"><a href="#cb17-639" aria-hidden="true" tabindex="-1"></a>                             feedback: <span class="bu">str</span>, scores: Tuple[<span class="bu">float</span>, <span class="bu">float</span>]) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb17-640"><a href="#cb17-640" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Display the results of the current round."""</span></span>
<span id="cb17-641"><a href="#cb17-641" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Advocate 1's defense:</span><span class="ch">\n</span><span class="sc">{</span>defenses[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-642"><a href="#cb17-642" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Advocate 2's defense:</span><span class="ch">\n</span><span class="sc">{</span>defenses[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-643"><a href="#cb17-643" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Judge's feedback:</span><span class="ch">\n</span><span class="sc">{</span>feedback<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-644"><a href="#cb17-644" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Scores for this round: Answer 1 = </span><span class="sc">{</span><span class="bu">round</span>(scores[<span class="dv">0</span>], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">, Answer 2 = </span><span class="sc">{</span><span class="bu">round</span>(scores[<span class="dv">1</span>], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-645"><a href="#cb17-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-646"><a href="#cb17-646" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _has_scores_converged(<span class="va">self</span>, round_num: <span class="bu">int</span>, memory: Memory) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb17-647"><a href="#cb17-647" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Checks if debate scores have converged by comparing last two rounds"""</span></span>
<span id="cb17-648"><a href="#cb17-648" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> round_num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb17-649"><a href="#cb17-649" aria-hidden="true" tabindex="-1"></a>            prev_diff <span class="op">=</span> memory.scores[<span class="op">-</span><span class="dv">2</span>][<span class="dv">0</span>] <span class="op">-</span> memory.scores[<span class="op">-</span><span class="dv">2</span>][<span class="dv">1</span>]</span>
<span id="cb17-650"><a href="#cb17-650" aria-hidden="true" tabindex="-1"></a>            curr_diff <span class="op">=</span> memory.scores[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="op">-</span> memory.scores[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>]</span>
<span id="cb17-651"><a href="#cb17-651" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (prev_diff <span class="op">*</span> curr_diff) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb17-652"><a href="#cb17-652" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb17-653"><a href="#cb17-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-654"><a href="#cb17-654" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _prepare_results(<span class="va">self</span>, memory: Memory) <span class="op">-&gt;</span> Dict:</span>
<span id="cb17-655"><a href="#cb17-655" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Prepare the final results dictionary."""</span></span>
<span id="cb17-656"><a href="#cb17-656" aria-hidden="true" tabindex="-1"></a>        avg_scores <span class="op">=</span> [</span>
<span id="cb17-657"><a href="#cb17-657" aria-hidden="true" tabindex="-1"></a>            <span class="bu">round</span>(<span class="bu">sum</span>(scores[i] <span class="cf">for</span> scores <span class="kw">in</span> memory.scores) <span class="op">/</span> <span class="bu">len</span>(memory.scores), <span class="dv">2</span>)</span>
<span id="cb17-658"><a href="#cb17-658" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb17-659"><a href="#cb17-659" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb17-660"><a href="#cb17-660" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-661"><a href="#cb17-661" aria-hidden="true" tabindex="-1"></a>        winner <span class="op">=</span> (</span>
<span id="cb17-662"><a href="#cb17-662" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&gt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb17-663"><a href="#cb17-663" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&lt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb17-664"><a href="#cb17-664" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'tie'</span></span>
<span id="cb17-665"><a href="#cb17-665" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-666"><a href="#cb17-666" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-667"><a href="#cb17-667" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb17-668"><a href="#cb17-668" aria-hidden="true" tabindex="-1"></a>            <span class="st">"winner"</span>: winner,</span>
<span id="cb17-669"><a href="#cb17-669" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_scores"</span>: avg_scores,</span>
<span id="cb17-670"><a href="#cb17-670" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rounds"</span>: <span class="bu">len</span>(memory.scores),</span>
<span id="cb17-671"><a href="#cb17-671" aria-hidden="true" tabindex="-1"></a>            <span class="st">"score_history"</span>: [[<span class="bu">round</span>(s[<span class="dv">0</span>], <span class="dv">2</span>), <span class="bu">round</span>(s[<span class="dv">1</span>], <span class="dv">2</span>)] <span class="cf">for</span> s <span class="kw">in</span> memory.scores],</span>
<span id="cb17-672"><a href="#cb17-672" aria-hidden="true" tabindex="-1"></a>            <span class="st">"argument_history"</span>: memory.arguments,</span>
<span id="cb17-673"><a href="#cb17-673" aria-hidden="true" tabindex="-1"></a>            <span class="st">"feedback_history"</span>: memory.feedback</span>
<span id="cb17-674"><a href="#cb17-674" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb17-675"><a href="#cb17-675" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-676"><a href="#cb17-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-677"><a href="#cb17-677" aria-hidden="true" tabindex="-1"></a><span class="fu"># Load the MT-Bench dataset</span></span>
<span id="cb17-678"><a href="#cb17-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-679"><a href="#cb17-679" aria-hidden="true" tabindex="-1"></a>For evaluation, I'll use MT-Bench which is the dataset used in the paper. MT-Bench is a dataset that contains human annotator judgments of preference between two alternative LLM responses.</span>
<span id="cb17-680"><a href="#cb17-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-681"><a href="#cb17-681" aria-hidden="true" tabindex="-1"></a>I'll read the dataset from Llamahub <span class="co">[</span><span class="ot">MtBenchHumanJudgementDataset</span><span class="co">](https://llamahub.ai/l/llama_datasets/MT%20Bench%20Human%20Judgement%20Dataset?from=)</span>, which has simplified the dataset by aggregating human judgments for repeated observations of the same model competitions.</span>
<span id="cb17-682"><a href="#cb17-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-683"><a href="#cb17-683" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In the original version, there can be more than one human evaluator for a given example (query, two model responses). In this adapted version however, we aggregate these 'repeated' entries entries and convert the 'winner' column of the original schema to instead represent the proportion of times 'model_a' wins across all of the human evaluators. To adapt this to a llama-dataset, and to better consider ties (albeit with small samples) we set an uncertainty threshold for this proportion in that if it is between </span><span class="co">[</span><span class="ot">0.4, 0.6</span><span class="co">]</span><span class="at"> then we consider there to be no winner between the two models.</span></span>
<span id="cb17-684"><a href="#cb17-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-685"><a href="#cb17-685" aria-hidden="true" tabindex="-1"></a>Although it's not entirely clear from this datacard description, the human evaluator judgments were encoded as "1" (model_a wins), "0" (model_b wins), or "0.5" (tie). Essentially, they were aggregated to represent the majority winner across repeated observations.</span>
<span id="cb17-686"><a href="#cb17-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-689"><a href="#cb17-689" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-690"><a href="#cb17-690" aria-hidden="true" tabindex="-1"></a><span class="co"># Commented out since the dataset is already downloaded</span></span>
<span id="cb17-691"><a href="#cb17-691" aria-hidden="true" tabindex="-1"></a><span class="co">#!llamaindex-cli download-llamadataset MtBenchHumanJudgementDataset --download-dir ./data</span></span>
<span id="cb17-692"><a href="#cb17-692" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-693"><a href="#cb17-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-694"><a href="#cb17-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-697"><a href="#cb17-697" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-698"><a href="#cb17-698" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-699"><a href="#cb17-699" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb17-700"><a href="#cb17-700" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Code to load the dataset"</span></span>
<span id="cb17-701"><a href="#cb17-701" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb17-702"><a href="#cb17-702" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-703"><a href="#cb17-703" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.llama_dataset <span class="im">import</span> LabelledPairwiseEvaluatorDataset</span>
<span id="cb17-704"><a href="#cb17-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-705"><a href="#cb17-705" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> LabelledPairwiseEvaluatorDataset.from_json(</span>
<span id="cb17-706"><a href="#cb17-706" aria-hidden="true" tabindex="-1"></a>    <span class="st">"./data/pairwise_evaluator_dataset.json"</span></span>
<span id="cb17-707"><a href="#cb17-707" aria-hidden="true" tabindex="-1"></a>).to_pandas()</span>
<span id="cb17-708"><a href="#cb17-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-709"><a href="#cb17-709" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the dataset</span></span>
<span id="cb17-710"><a href="#cb17-710" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Dataset shape: </span><span class="sc">{</span>df<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-711"><a href="#cb17-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-712"><a href="#cb17-712" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the reference_score value counts, just to confirm that this column is encoding the winner as I expect</span></span>
<span id="cb17-713"><a href="#cb17-713" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\n</span><span class="ss">Reference score (winner) value counts: </span><span class="sc">{</span>df[<span class="st">"reference_score"</span>]<span class="sc">.</span>value_counts()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-714"><a href="#cb17-714" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-715"><a href="#cb17-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-716"><a href="#cb17-716" aria-hidden="true" tabindex="-1"></a>I'll rename some of the columns, and also encode a "human_winner" column to indicate whether model_a was preferred, model_b, or if there was a tie. (Note: This is just my own preference for how to represent the data).</span>
<span id="cb17-717"><a href="#cb17-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-720"><a href="#cb17-720" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-721"><a href="#cb17-721" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-722"><a href="#cb17-722" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb17-723"><a href="#cb17-723" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Code to rename variables and encode a winner column"</span></span>
<span id="cb17-724"><a href="#cb17-724" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[[<span class="st">'query'</span>, <span class="st">'answer'</span>, <span class="st">'second_answer'</span>, <span class="st">'answer_by'</span>, <span class="st">'second_answer_by'</span>, <span class="st">'reference_score'</span>]]</span>
<span id="cb17-725"><a href="#cb17-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-726"><a href="#cb17-726" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename as follows: query =&gt; question, answer =&gt; model_a_answer, second_answer =&gt; model_b_answer, answer_by =&gt; model_a, second_answer_by =&gt; model_b, reference_score =&gt; human_winner</span></span>
<span id="cb17-727"><a href="#cb17-727" aria-hidden="true" tabindex="-1"></a>df.rename(columns<span class="op">=</span>{<span class="st">'query'</span>: <span class="st">'question'</span>, <span class="st">'answer'</span>: <span class="st">'model_a_answer'</span>, <span class="st">'second_answer'</span>: <span class="st">'model_b_answer'</span>, <span class="st">'answer_by'</span>: <span class="st">'model_a'</span>, <span class="st">'second_answer_by'</span>: <span class="st">'model_b'</span>, <span class="st">'reference_score'</span>: <span class="st">'human_winner'</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-728"><a href="#cb17-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-729"><a href="#cb17-729" aria-hidden="true" tabindex="-1"></a><span class="co"># Reencode human winner as "model_a" if 1, "model_b" if 0, and "tie" if 0.5</span></span>
<span id="cb17-730"><a href="#cb17-730" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'human_winner'</span>] <span class="op">=</span> df[<span class="st">'human_winner'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">'model_a'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'tie'</span>)</span>
<span id="cb17-731"><a href="#cb17-731" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-732"><a href="#cb17-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-733"><a href="#cb17-733" aria-hidden="true" tabindex="-1"></a>The original dataset contains ~1200 rows. I'll take a random sample of 300 rows for my testing to save on time and API costs.</span>
<span id="cb17-734"><a href="#cb17-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-737"><a href="#cb17-737" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-738"><a href="#cb17-738" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-739"><a href="#cb17-739" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb17-740"><a href="#cb17-740" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Code to take a random sample of 300 rows"</span></span>
<span id="cb17-741"><a href="#cb17-741" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a random sample of 300 rows</span></span>
<span id="cb17-742"><a href="#cb17-742" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.sample(n<span class="op">=</span><span class="dv">300</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-743"><a href="#cb17-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-744"><a href="#cb17-744" aria-hidden="true" tabindex="-1"></a>df.head()</span>
<span id="cb17-745"><a href="#cb17-745" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-746"><a href="#cb17-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-747"><a href="#cb17-747" aria-hidden="true" tabindex="-1"></a><span class="fu"># Use methods to evaluate MT-Bench dataset</span></span>
<span id="cb17-748"><a href="#cb17-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-749"><a href="#cb17-749" aria-hidden="true" tabindex="-1"></a>Using this sample of 300 rows from the MT-Bench dataset, I will run the three LLM models (Baseline-Weak, Baseline-Strong, and SAMRE) on each set of question and answers.</span>
<span id="cb17-750"><a href="#cb17-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-751"><a href="#cb17-751" aria-hidden="true" tabindex="-1"></a>The code below is the main evaluation loop, designed to run multiple evaluations asynchronously (to save time). It will evaluate each item in the dataset, and save the results to disk as a checkpoint. If the evaluation is interrupted, the code can be resumed from the last checkpoint.</span>
<span id="cb17-752"><a href="#cb17-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-753"><a href="#cb17-753" aria-hidden="true" tabindex="-1"></a>I'll use <span class="in">`gpt-4o-mini`</span> for the evaluations. In the paper they had tested models like <span class="in">`gpt-4o`</span> and <span class="in">`gpt-3.5-turbo`</span>, and I would not expect <span class="in">`gpt-4o-mini`</span> to be an exception.</span>
<span id="cb17-754"><a href="#cb17-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-757"><a href="#cb17-757" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-758"><a href="#cb17-758" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-759"><a href="#cb17-759" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb17-760"><a href="#cb17-760" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that runs the evaluations"</span></span>
<span id="cb17-761"><a href="#cb17-761" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> asyncio</span>
<span id="cb17-762"><a href="#cb17-762" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> asyncio <span class="im">import</span> Semaphore</span>
<span id="cb17-763"><a href="#cb17-763" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb17-764"><a href="#cb17-764" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb17-765"><a href="#cb17-765" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> hashlib</span>
<span id="cb17-766"><a href="#cb17-766" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb17-767"><a href="#cb17-767" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.WARNING)</span>
<span id="cb17-768"><a href="#cb17-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-769"><a href="#cb17-769" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> evaluate_conversation_pair(row, evaluators, semaphore, idx, total):</span>
<span id="cb17-770"><a href="#cb17-770" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate a single conversation pair with all evaluators"""</span></span>
<span id="cb17-771"><a href="#cb17-771" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="cf">with</span> semaphore:</span>
<span id="cb17-772"><a href="#cb17-772" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add delay between API calls</span></span>
<span id="cb17-773"><a href="#cb17-773" aria-hidden="true" tabindex="-1"></a>        <span class="co">#await asyncio.sleep(1)  # Add small delay between conversations</span></span>
<span id="cb17-774"><a href="#cb17-774" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-775"><a href="#cb17-775" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate pair_id from conversation hash</span></span>
<span id="cb17-776"><a href="#cb17-776" aria-hidden="true" tabindex="-1"></a>        pair_id <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>row[<span class="st">'model_a'</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>row[<span class="st">'model_b'</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>hashlib<span class="sc">.</span>sha256(<span class="bu">str</span>(row[<span class="st">'question'</span>]).encode())<span class="sc">.</span>hexdigest()[:<span class="dv">12</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb17-777"><a href="#cb17-777" aria-hidden="true" tabindex="-1"></a>        checkpoint_file <span class="op">=</span> <span class="ss">f'checkpoints/</span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">.json'</span></span>
<span id="cb17-778"><a href="#cb17-778" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-779"><a href="#cb17-779" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return existing checkpoint if available</span></span>
<span id="cb17-780"><a href="#cb17-780" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> os.path.exists(checkpoint_file):</span>
<span id="cb17-781"><a href="#cb17-781" aria-hidden="true" tabindex="-1"></a>            logging.info(<span class="ss">f"Found existing checkpoint file for </span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-782"><a href="#cb17-782" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> json.load(<span class="bu">open</span>(checkpoint_file))</span>
<span id="cb17-783"><a href="#cb17-783" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-784"><a href="#cb17-784" aria-hidden="true" tabindex="-1"></a>        logging.info(<span class="ss">f"No checkpoint file found for </span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-785"><a href="#cb17-785" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> {</span>
<span id="cb17-786"><a href="#cb17-786" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span>: row[<span class="st">'model_a'</span>],</span>
<span id="cb17-787"><a href="#cb17-787" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_b'</span>: row[<span class="st">'model_b'</span>],</span>
<span id="cb17-788"><a href="#cb17-788" aria-hidden="true" tabindex="-1"></a>            <span class="st">'human_winner'</span>: row[<span class="st">'human_winner'</span>],</span>
<span id="cb17-789"><a href="#cb17-789" aria-hidden="true" tabindex="-1"></a>            <span class="st">'pair_id'</span>: pair_id</span>
<span id="cb17-790"><a href="#cb17-790" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb17-791"><a href="#cb17-791" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-792"><a href="#cb17-792" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb17-793"><a href="#cb17-793" aria-hidden="true" tabindex="-1"></a>            <span class="co"># First run SAMRE evaluation with retries</span></span>
<span id="cb17-794"><a href="#cb17-794" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):  <span class="co"># Try up to 3 times</span></span>
<span id="cb17-795"><a href="#cb17-795" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb17-796"><a href="#cb17-796" aria-hidden="true" tabindex="-1"></a>                    samre_evaluator <span class="op">=</span> evaluators[<span class="st">'samre'</span>]</span>
<span id="cb17-797"><a href="#cb17-797" aria-hidden="true" tabindex="-1"></a>                    samre_result <span class="op">=</span> <span class="cf">await</span> samre_evaluator.evaluate(</span>
<span id="cb17-798"><a href="#cb17-798" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>], </span>
<span id="cb17-799"><a href="#cb17-799" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>], </span>
<span id="cb17-800"><a href="#cb17-800" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>]</span>
<span id="cb17-801"><a href="#cb17-801" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb17-802"><a href="#cb17-802" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'samre_winner'</span>] <span class="op">=</span> samre_result[<span class="st">'winner'</span>]</span>
<span id="cb17-803"><a href="#cb17-803" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'samre_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: samre_result[k] <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb17-804"><a href="#cb17-804" aria-hidden="true" tabindex="-1"></a>                    result.update({</span>
<span id="cb17-805"><a href="#cb17-805" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'samre_argument_history'</span>: samre_result[<span class="st">'argument_history'</span>],</span>
<span id="cb17-806"><a href="#cb17-806" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'samre_feedback_history'</span>: samre_result[<span class="st">'feedback_history'</span>]</span>
<span id="cb17-807"><a href="#cb17-807" aria-hidden="true" tabindex="-1"></a>                    })</span>
<span id="cb17-808"><a href="#cb17-808" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span>  <span class="co"># If successful, break retry loop</span></span>
<span id="cb17-809"><a href="#cb17-809" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-810"><a href="#cb17-810" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb17-811"><a href="#cb17-811" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span>  <span class="co"># Exponential backoff</span></span>
<span id="cb17-812"><a href="#cb17-812" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on SAMRE, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb17-813"><a href="#cb17-813" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb17-814"><a href="#cb17-814" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:  <span class="co"># Last attempt failed</span></span>
<span id="cb17-815"><a href="#cb17-815" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb17-816"><a href="#cb17-816" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb17-817"><a href="#cb17-817" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span>  <span class="co"># Re-raise non-rate-limit errors</span></span>
<span id="cb17-818"><a href="#cb17-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-819"><a href="#cb17-819" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> asyncio.sleep(<span class="fl">0.5</span>)  <span class="co"># Add small delay between evaluator calls</span></span>
<span id="cb17-820"><a href="#cb17-820" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-821"><a href="#cb17-821" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run baseline strong with same number of rounds as SAMRE</span></span>
<span id="cb17-822"><a href="#cb17-822" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb17-823"><a href="#cb17-823" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb17-824"><a href="#cb17-824" aria-hidden="true" tabindex="-1"></a>                    baseline_strong_evaluator <span class="op">=</span> evaluators[<span class="st">'baseline_strong'</span>]</span>
<span id="cb17-825"><a href="#cb17-825" aria-hidden="true" tabindex="-1"></a>                    baseline_strong_result <span class="op">=</span> <span class="cf">await</span> baseline_strong_evaluator.evaluate(</span>
<span id="cb17-826"><a href="#cb17-826" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>],</span>
<span id="cb17-827"><a href="#cb17-827" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>],</span>
<span id="cb17-828"><a href="#cb17-828" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>],</span>
<span id="cb17-829"><a href="#cb17-829" aria-hidden="true" tabindex="-1"></a>                        num_rounds<span class="op">=</span>result[<span class="st">'samre_rounds'</span>]</span>
<span id="cb17-830"><a href="#cb17-830" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb17-831"><a href="#cb17-831" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_strong_winner'</span>] <span class="op">=</span> baseline_strong_result[<span class="st">'winner'</span>]</span>
<span id="cb17-832"><a href="#cb17-832" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'baseline_strong_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: baseline_strong_result[k] </span>
<span id="cb17-833"><a href="#cb17-833" aria-hidden="true" tabindex="-1"></a>                                 <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb17-834"><a href="#cb17-834" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_strong_full_response'</span>] <span class="op">=</span> baseline_strong_result[<span class="st">'full_response'</span>]</span>
<span id="cb17-835"><a href="#cb17-835" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb17-836"><a href="#cb17-836" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-837"><a href="#cb17-837" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb17-838"><a href="#cb17-838" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span></span>
<span id="cb17-839"><a href="#cb17-839" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on baseline strong, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb17-840"><a href="#cb17-840" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb17-841"><a href="#cb17-841" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb17-842"><a href="#cb17-842" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb17-843"><a href="#cb17-843" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb17-844"><a href="#cb17-844" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span></span>
<span id="cb17-845"><a href="#cb17-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-846"><a href="#cb17-846" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> asyncio.sleep(<span class="fl">0.5</span>)  <span class="co"># Add small delay between evaluator calls</span></span>
<span id="cb17-847"><a href="#cb17-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-848"><a href="#cb17-848" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run baseline weak with 1 round</span></span>
<span id="cb17-849"><a href="#cb17-849" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb17-850"><a href="#cb17-850" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb17-851"><a href="#cb17-851" aria-hidden="true" tabindex="-1"></a>                    baseline_weak_evaluator <span class="op">=</span> evaluators[<span class="st">'baseline_weak'</span>]</span>
<span id="cb17-852"><a href="#cb17-852" aria-hidden="true" tabindex="-1"></a>                    baseline_weak_result <span class="op">=</span> <span class="cf">await</span> baseline_weak_evaluator.evaluate(</span>
<span id="cb17-853"><a href="#cb17-853" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>],</span>
<span id="cb17-854"><a href="#cb17-854" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>],</span>
<span id="cb17-855"><a href="#cb17-855" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>],</span>
<span id="cb17-856"><a href="#cb17-856" aria-hidden="true" tabindex="-1"></a>                        num_rounds<span class="op">=</span><span class="dv">1</span></span>
<span id="cb17-857"><a href="#cb17-857" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb17-858"><a href="#cb17-858" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_weak_winner'</span>] <span class="op">=</span> baseline_weak_result[<span class="st">'winner'</span>]</span>
<span id="cb17-859"><a href="#cb17-859" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'baseline_weak_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: baseline_weak_result[k] </span>
<span id="cb17-860"><a href="#cb17-860" aria-hidden="true" tabindex="-1"></a>                                 <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb17-861"><a href="#cb17-861" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_weak_full_response'</span>] <span class="op">=</span> baseline_weak_result[<span class="st">'full_response'</span>]</span>
<span id="cb17-862"><a href="#cb17-862" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb17-863"><a href="#cb17-863" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-864"><a href="#cb17-864" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb17-865"><a href="#cb17-865" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span></span>
<span id="cb17-866"><a href="#cb17-866" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on baseline weak, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb17-867"><a href="#cb17-867" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb17-868"><a href="#cb17-868" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb17-869"><a href="#cb17-869" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb17-870"><a href="#cb17-870" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb17-871"><a href="#cb17-871" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span></span>
<span id="cb17-872"><a href="#cb17-872" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb17-873"><a href="#cb17-873" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-874"><a href="#cb17-874" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Error evaluating row </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-875"><a href="#cb17-875" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'samre_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-876"><a href="#cb17-876" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'baseline_strong_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-877"><a href="#cb17-877" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'baseline_weak_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-878"><a href="#cb17-878" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'error'</span>] <span class="op">=</span> <span class="bu">str</span>(e)</span>
<span id="cb17-879"><a href="#cb17-879" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-880"><a href="#cb17-880" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save checkpoint after each evaluation</span></span>
<span id="cb17-881"><a href="#cb17-881" aria-hidden="true" tabindex="-1"></a>        os.makedirs(<span class="st">'checkpoints'</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-882"><a href="#cb17-882" aria-hidden="true" tabindex="-1"></a>        json.dump(result, <span class="bu">open</span>(checkpoint_file, <span class="st">'w'</span>))</span>
<span id="cb17-883"><a href="#cb17-883" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-884"><a href="#cb17-884" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (idx <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-885"><a href="#cb17-885" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Processed </span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>total<span class="sc">}</span><span class="ss"> conversations"</span>)</span>
<span id="cb17-886"><a href="#cb17-886" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-887"><a href="#cb17-887" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result</span>
<span id="cb17-888"><a href="#cb17-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-889"><a href="#cb17-889" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> evaluate_conversations_async(df, evaluators, semaphore_limit<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb17-890"><a href="#cb17-890" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate conversations asynchronously"""</span></span>
<span id="cb17-891"><a href="#cb17-891" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reduce semaphore limit</span></span>
<span id="cb17-892"><a href="#cb17-892" aria-hidden="true" tabindex="-1"></a>    semaphore_limit <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Process one at a time to avoid rate limits</span></span>
<span id="cb17-893"><a href="#cb17-893" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-894"><a href="#cb17-894" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process in smaller batches</span></span>
<span id="cb17-895"><a href="#cb17-895" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb17-896"><a href="#cb17-896" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb17-897"><a href="#cb17-897" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-898"><a href="#cb17-898" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(df), batch_size):</span>
<span id="cb17-899"><a href="#cb17-899" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> df.iloc[i:i<span class="op">+</span>batch_size]</span>
<span id="cb17-900"><a href="#cb17-900" aria-hidden="true" tabindex="-1"></a>        tasks <span class="op">=</span> [</span>
<span id="cb17-901"><a href="#cb17-901" aria-hidden="true" tabindex="-1"></a>            evaluate_conversation_pair(row[<span class="dv">1</span>], evaluators, Semaphore(semaphore_limit), idx, <span class="bu">len</span>(df))</span>
<span id="cb17-902"><a href="#cb17-902" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, row <span class="kw">in</span> <span class="bu">enumerate</span>(batch.iterrows(), start<span class="op">=</span>i)</span>
<span id="cb17-903"><a href="#cb17-903" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb17-904"><a href="#cb17-904" aria-hidden="true" tabindex="-1"></a>        batch_results <span class="op">=</span> <span class="cf">await</span> asyncio.gather(<span class="op">*</span>tasks)</span>
<span id="cb17-905"><a href="#cb17-905" aria-hidden="true" tabindex="-1"></a>        results.extend(batch_results)</span>
<span id="cb17-906"><a href="#cb17-906" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-907"><a href="#cb17-907" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add delay between batches</span></span>
<span id="cb17-908"><a href="#cb17-908" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">+</span> batch_size <span class="op">&lt;</span> <span class="bu">len</span>(df):</span>
<span id="cb17-909"><a href="#cb17-909" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Completed batch </span><span class="sc">{</span>i<span class="op">//</span>batch_size <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, waiting before next batch..."</span>)</span>
<span id="cb17-910"><a href="#cb17-910" aria-hidden="true" tabindex="-1"></a>            <span class="co">#await asyncio.sleep(5)  # 5 second delay between batches</span></span>
<span id="cb17-911"><a href="#cb17-911" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-912"><a href="#cb17-912" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(results)</span>
<span id="cb17-913"><a href="#cb17-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-914"><a href="#cb17-914" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> main():</span>
<span id="cb17-915"><a href="#cb17-915" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="cf">with</span> ModelEvaluator.create(mode<span class="op">=</span><span class="st">"samre"</span>) <span class="im">as</span> samre_evaluator, <span class="op">\</span></span>
<span id="cb17-916"><a href="#cb17-916" aria-hidden="true" tabindex="-1"></a>               ModelEvaluator.create(mode<span class="op">=</span><span class="st">"baseline_strong"</span>) <span class="im">as</span> baseline_strong_evaluator, <span class="op">\</span></span>
<span id="cb17-917"><a href="#cb17-917" aria-hidden="true" tabindex="-1"></a>               ModelEvaluator.create(mode<span class="op">=</span><span class="st">"baseline_weak"</span>) <span class="im">as</span> baseline_weak_evaluator:</span>
<span id="cb17-918"><a href="#cb17-918" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="cf">await</span> evaluate_conversations_async(</span>
<span id="cb17-919"><a href="#cb17-919" aria-hidden="true" tabindex="-1"></a>            df,</span>
<span id="cb17-920"><a href="#cb17-920" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb17-921"><a href="#cb17-921" aria-hidden="true" tabindex="-1"></a>                <span class="st">'samre'</span>: samre_evaluator, </span>
<span id="cb17-922"><a href="#cb17-922" aria-hidden="true" tabindex="-1"></a>                <span class="st">'baseline_strong'</span>: baseline_strong_evaluator,</span>
<span id="cb17-923"><a href="#cb17-923" aria-hidden="true" tabindex="-1"></a>                <span class="st">'baseline_weak'</span>: baseline_weak_evaluator</span>
<span id="cb17-924"><a href="#cb17-924" aria-hidden="true" tabindex="-1"></a>            },</span>
<span id="cb17-925"><a href="#cb17-925" aria-hidden="true" tabindex="-1"></a>            semaphore_limit<span class="op">=</span><span class="dv">1</span></span>
<span id="cb17-926"><a href="#cb17-926" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-927"><a href="#cb17-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-928"><a href="#cb17-928" aria-hidden="true" tabindex="-1"></a><span class="co"># Run evaluation with checkpoint recovery</span></span>
<span id="cb17-929"><a href="#cb17-929" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb17-930"><a href="#cb17-930" aria-hidden="true" tabindex="-1"></a>    eval_df <span class="op">=</span> <span class="cf">await</span> main()</span>
<span id="cb17-931"><a href="#cb17-931" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-932"><a href="#cb17-932" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Error during evaluation: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ch">\n</span><span class="ss">Recovering from checkpoints..."</span>)</span>
<span id="cb17-933"><a href="#cb17-933" aria-hidden="true" tabindex="-1"></a>    eval_df <span class="op">=</span> pd.DataFrame([json.load(<span class="bu">open</span>(<span class="ss">f'checkpoints/</span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">'</span>)) </span>
<span id="cb17-934"><a href="#cb17-934" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">for</span> f <span class="kw">in</span> os.listdir(<span class="st">'checkpoints'</span>) </span>
<span id="cb17-935"><a href="#cb17-935" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">if</span> f.endswith(<span class="st">'.json'</span>)])</span>
<span id="cb17-936"><a href="#cb17-936" aria-hidden="true" tabindex="-1"></a><span class="cf">finally</span>:</span>
<span id="cb17-937"><a href="#cb17-937" aria-hidden="true" tabindex="-1"></a>    eval_df.to_csv(<span class="st">'eval_df.csv'</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-938"><a href="#cb17-938" aria-hidden="true" tabindex="-1"></a>    eval_df.head()</span>
<span id="cb17-939"><a href="#cb17-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-940"><a href="#cb17-940" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop rows with any null values on the model winner columns</span></span>
<span id="cb17-941"><a href="#cb17-941" aria-hidden="true" tabindex="-1"></a>eval_df <span class="op">=</span> eval_df.dropna(subset<span class="op">=</span>[<span class="st">'baseline_strong_winner'</span>, <span class="st">'baseline_weak_winner'</span>, <span class="st">'samre_winner'</span>])</span>
<span id="cb17-942"><a href="#cb17-942" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-943"><a href="#cb17-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-944"><a href="#cb17-944" aria-hidden="true" tabindex="-1"></a><span class="fu"># Performance evaluation</span></span>
<span id="cb17-945"><a href="#cb17-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-946"><a href="#cb17-946" aria-hidden="true" tabindex="-1"></a>Now that the evaluation is complete, I will evaluate the performance of each of the three methods by first looking at how well each method agreed with the human judgments. </span>
<span id="cb17-947"><a href="#cb17-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-948"><a href="#cb17-948" aria-hidden="true" tabindex="-1"></a>I'll use Krippendorff's alpha to measure agreement, since it is a robust measure of agreement that can handle non-binary ratings (among other things).</span>
<span id="cb17-949"><a href="#cb17-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-952"><a href="#cb17-952" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-953"><a href="#cb17-953" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-954"><a href="#cb17-954" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb17-955"><a href="#cb17-955" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that calculates agreement"</span></span>
<span id="cb17-956"><a href="#cb17-956" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> krippendorff <span class="im">import</span> alpha</span>
<span id="cb17-957"><a href="#cb17-957" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-958"><a href="#cb17-958" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb17-959"><a href="#cb17-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-960"><a href="#cb17-960" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_agreement(df, rater1_col, rater2_col):</span>
<span id="cb17-961"><a href="#cb17-961" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-962"><a href="#cb17-962" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate Krippendorff's alpha between two raters.</span></span>
<span id="cb17-963"><a href="#cb17-963" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-964"><a href="#cb17-964" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb17-965"><a href="#cb17-965" aria-hidden="true" tabindex="-1"></a><span class="co">        df: DataFrame containing the ratings</span></span>
<span id="cb17-966"><a href="#cb17-966" aria-hidden="true" tabindex="-1"></a><span class="co">        rater1_col: Name of first rater's column</span></span>
<span id="cb17-967"><a href="#cb17-967" aria-hidden="true" tabindex="-1"></a><span class="co">        rater2_col: Name of second rater's column</span></span>
<span id="cb17-968"><a href="#cb17-968" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-969"><a href="#cb17-969" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb17-970"><a href="#cb17-970" aria-hidden="true" tabindex="-1"></a><span class="co">        float: Krippendorff's alpha score</span></span>
<span id="cb17-971"><a href="#cb17-971" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-972"><a href="#cb17-972" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create label encoder</span></span>
<span id="cb17-973"><a href="#cb17-973" aria-hidden="true" tabindex="-1"></a>    le <span class="op">=</span> LabelEncoder()</span>
<span id="cb17-974"><a href="#cb17-974" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-975"><a href="#cb17-975" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine all unique values from both columns</span></span>
<span id="cb17-976"><a href="#cb17-976" aria-hidden="true" tabindex="-1"></a>    all_values <span class="op">=</span> pd.concat([df[rater1_col], df[rater2_col]]).unique()</span>
<span id="cb17-977"><a href="#cb17-977" aria-hidden="true" tabindex="-1"></a>    le.fit(all_values)</span>
<span id="cb17-978"><a href="#cb17-978" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-979"><a href="#cb17-979" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Transform the ratings to numeric values</span></span>
<span id="cb17-980"><a href="#cb17-980" aria-hidden="true" tabindex="-1"></a>    ratings1 <span class="op">=</span> le.transform(df[rater1_col].fillna(<span class="st">'missing'</span>))</span>
<span id="cb17-981"><a href="#cb17-981" aria-hidden="true" tabindex="-1"></a>    ratings2 <span class="op">=</span> le.transform(df[rater2_col].fillna(<span class="st">'missing'</span>))</span>
<span id="cb17-982"><a href="#cb17-982" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-983"><a href="#cb17-983" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape data for krippendorff alpha calculation</span></span>
<span id="cb17-984"><a href="#cb17-984" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each row represents one item, each column represents one rater</span></span>
<span id="cb17-985"><a href="#cb17-985" aria-hidden="true" tabindex="-1"></a>    reliability_data <span class="op">=</span> np.vstack([ratings1, ratings2])</span>
<span id="cb17-986"><a href="#cb17-986" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-987"><a href="#cb17-987" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha(reliability_data<span class="op">=</span>reliability_data, level_of_measurement<span class="op">=</span><span class="st">'nominal'</span>)</span>
<span id="cb17-988"><a href="#cb17-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-989"><a href="#cb17-989" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate agreement scores for all methods</span></span>
<span id="cb17-990"><a href="#cb17-990" aria-hidden="true" tabindex="-1"></a>human_baseline_strong_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_strong_winner'</span>)</span>
<span id="cb17-991"><a href="#cb17-991" aria-hidden="true" tabindex="-1"></a>human_baseline_weak_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_weak_winner'</span>)</span>
<span id="cb17-992"><a href="#cb17-992" aria-hidden="true" tabindex="-1"></a>human_samre_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'samre_winner'</span>)</span>
<span id="cb17-993"><a href="#cb17-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-994"><a href="#cb17-994" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame with the agreement scores</span></span>
<span id="cb17-995"><a href="#cb17-995" aria-hidden="true" tabindex="-1"></a>agreement_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb17-996"><a href="#cb17-996" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Evaluator Pair'</span>: [<span class="st">'Baseline-Strong Agreement with Humans'</span>, <span class="st">'Baseline-Weak Agreement with Humans'</span>, <span class="st">'SAMRE Agreement with Humans'</span>],</span>
<span id="cb17-997"><a href="#cb17-997" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Krippendorff Alpha'</span>: [human_baseline_strong_agreement, human_baseline_weak_agreement, human_samre_agreement]</span>
<span id="cb17-998"><a href="#cb17-998" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb17-999"><a href="#cb17-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1000"><a href="#cb17-1000" aria-hidden="true" tabindex="-1"></a><span class="co"># Round the scores to 3 decimal places</span></span>
<span id="cb17-1001"><a href="#cb17-1001" aria-hidden="true" tabindex="-1"></a>agreement_df[<span class="st">'Krippendorff Alpha'</span>] <span class="op">=</span> agreement_df[<span class="st">'Krippendorff Alpha'</span>].<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb17-1002"><a href="#cb17-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1003"><a href="#cb17-1003" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percent difference between Baseline-Strong and Baseline-Weak, and SAMRE and Baseline-Strong</span></span>
<span id="cb17-1004"><a href="#cb17-1004" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_diff <span class="op">=</span> (human_baseline_strong_agreement <span class="op">-</span> human_baseline_weak_agreement) <span class="op">/</span> human_baseline_strong_agreement</span>
<span id="cb17-1005"><a href="#cb17-1005" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_diff <span class="op">=</span> (human_baseline_strong_agreement <span class="op">-</span> human_samre_agreement) <span class="op">/</span> human_baseline_strong_agreement</span>
<span id="cb17-1006"><a href="#cb17-1006" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_diff <span class="op">=</span> (human_samre_agreement <span class="op">-</span> human_baseline_weak_agreement) <span class="op">/</span> human_samre_agreement</span>
<span id="cb17-1007"><a href="#cb17-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1008"><a href="#cb17-1008" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb17-1009"><a href="#cb17-1009" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(agreement_df)</span>
<span id="cb17-1010"><a href="#cb17-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1011"><a href="#cb17-1011" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the percent difference</span></span>
<span id="cb17-1012"><a href="#cb17-1012" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Krippendorff Alpha Improvements:"</span>)</span>
<span id="cb17-1013"><a href="#cb17-1013" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb17-1014"><a href="#cb17-1014" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb17-1015"><a href="#cb17-1015" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb17-1016"><a href="#cb17-1016" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-1017"><a href="#cb17-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1018"><a href="#cb17-1018" aria-hidden="true" tabindex="-1"></a>Although none of the methods yielded particularly strong agreement with the human judges in an absolute sense, their relative performance is in line with my predictions:</span>
<span id="cb17-1019"><a href="#cb17-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1020"><a href="#cb17-1020" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>As reported in the paper, SAMRE yielded significantly better agreement than Baseline-Weak (0.369 vs. 0.321, an increase of ~13%).</span>
<span id="cb17-1021"><a href="#cb17-1021" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Strong yielded significantly better agreement than Baseline-Weak (0.411 vs. 0.321, an increase of ~22%).</span>
<span id="cb17-1022"><a href="#cb17-1022" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Importantly, Baseline-Strong also yielded significantly better agreement than SAMRE (0.411 vs. 0.321, an increase of ~10%)!</span>
<span id="cb17-1023"><a href="#cb17-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1024"><a href="#cb17-1024" aria-hidden="true" tabindex="-1"></a>Next, we can also measure performance in terms of binary classification accuracy using Matthews Correlation Coefficient (MCC) as a balanced accuracy metric, while re-encoding the "winner" columns to indicate whether model_a was selected as better (1) or not better (0) in each case.</span>
<span id="cb17-1025"><a href="#cb17-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1028"><a href="#cb17-1028" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-1029"><a href="#cb17-1029" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-1030"><a href="#cb17-1030" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb17-1031"><a href="#cb17-1031" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that calculates Matthews Correlation Coefficient (MCC)"</span></span>
<span id="cb17-1032"><a href="#cb17-1032" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode winner as binary</span></span>
<span id="cb17-1033"><a href="#cb17-1033" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_winner_as_binary(winner):</span>
<span id="cb17-1034"><a href="#cb17-1034" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> winner <span class="op">==</span> <span class="st">'model_a'</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb17-1035"><a href="#cb17-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1036"><a href="#cb17-1036" aria-hidden="true" tabindex="-1"></a><span class="co"># Create binary columns for each evaluator</span></span>
<span id="cb17-1037"><a href="#cb17-1037" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'human_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'human_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb17-1038"><a href="#cb17-1038" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'baseline_strong_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'baseline_strong_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb17-1039"><a href="#cb17-1039" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'baseline_weak_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'baseline_weak_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb17-1040"><a href="#cb17-1040" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'samre_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'samre_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb17-1041"><a href="#cb17-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1042"><a href="#cb17-1042" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> matthews_corrcoef</span>
<span id="cb17-1043"><a href="#cb17-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1044"><a href="#cb17-1044" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate MCC for each method</span></span>
<span id="cb17-1045"><a href="#cb17-1045" aria-hidden="true" tabindex="-1"></a>metrics_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb17-1046"><a href="#cb17-1046" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Method'</span>: [<span class="st">'Baseline-Strong'</span>, <span class="st">'Baseline-Weak'</span>, <span class="st">'SAMRE'</span>],</span>
<span id="cb17-1047"><a href="#cb17-1047" aria-hidden="true" tabindex="-1"></a>    <span class="st">'MCC'</span>: [</span>
<span id="cb17-1048"><a href="#cb17-1048" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb17-1049"><a href="#cb17-1049" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb17-1050"><a href="#cb17-1050" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'baseline_strong_model_a_better'</span>]</span>
<span id="cb17-1051"><a href="#cb17-1051" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb17-1052"><a href="#cb17-1052" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb17-1053"><a href="#cb17-1053" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb17-1054"><a href="#cb17-1054" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'baseline_weak_model_a_better'</span>]</span>
<span id="cb17-1055"><a href="#cb17-1055" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb17-1056"><a href="#cb17-1056" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb17-1057"><a href="#cb17-1057" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb17-1058"><a href="#cb17-1058" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'samre_model_a_better'</span>]</span>
<span id="cb17-1059"><a href="#cb17-1059" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-1060"><a href="#cb17-1060" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb17-1061"><a href="#cb17-1061" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb17-1062"><a href="#cb17-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1063"><a href="#cb17-1063" aria-hidden="true" tabindex="-1"></a><span class="co"># Round the scores to 3 decimal places</span></span>
<span id="cb17-1064"><a href="#cb17-1064" aria-hidden="true" tabindex="-1"></a>metrics_df[<span class="st">'MCC'</span>] <span class="op">=</span> metrics_df[<span class="st">'MCC'</span>].<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb17-1065"><a href="#cb17-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1066"><a href="#cb17-1066" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percent differences</span></span>
<span id="cb17-1067"><a href="#cb17-1067" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_percent_diff(new, old):</span>
<span id="cb17-1068"><a href="#cb17-1068" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (new <span class="op">-</span> old) <span class="op">/</span> old <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb17-1069"><a href="#cb17-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1070"><a href="#cb17-1070" aria-hidden="true" tabindex="-1"></a><span class="co"># MCC differences</span></span>
<span id="cb17-1071"><a href="#cb17-1071" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb17-1072"><a href="#cb17-1072" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb17-1073"><a href="#cb17-1073" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb17-1074"><a href="#cb17-1074" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-1075"><a href="#cb17-1075" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb17-1076"><a href="#cb17-1076" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb17-1077"><a href="#cb17-1077" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb17-1078"><a href="#cb17-1078" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-1079"><a href="#cb17-1079" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb17-1080"><a href="#cb17-1080" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb17-1081"><a href="#cb17-1081" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb17-1082"><a href="#cb17-1082" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-1083"><a href="#cb17-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1084"><a href="#cb17-1084" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb17-1085"><a href="#cb17-1085" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(metrics_df)</span>
<span id="cb17-1086"><a href="#cb17-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1087"><a href="#cb17-1087" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">MCC Improvements:"</span>)</span>
<span id="cb17-1088"><a href="#cb17-1088" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb17-1089"><a href="#cb17-1089" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb17-1090"><a href="#cb17-1090" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb17-1091"><a href="#cb17-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1092"><a href="#cb17-1092" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-1093"><a href="#cb17-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1094"><a href="#cb17-1094" aria-hidden="true" tabindex="-1"></a>Looking at MCC values, we observe a similar pattern of findings to the Krippendorff alphas:</span>
<span id="cb17-1095"><a href="#cb17-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1096"><a href="#cb17-1096" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>SAMRE did not perform better than Baseline-Weak, in fact it performed slightly worse (0.401 vs. 0.417, a decrease of 4%). This is a bit different than what we saw with Krippendorff alpha.</span>
<span id="cb17-1097"><a href="#cb17-1097" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Strong performed better than Baseline-Weak (0.482 vs. 0.401, an increase of 16%).</span>
<span id="cb17-1098"><a href="#cb17-1098" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Baseline-Strong performed better than SAMRE (0.464 vs. 0.401, an increase of 20%).</span>
<span id="cb17-1099"><a href="#cb17-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1100"><a href="#cb17-1100" aria-hidden="true" tabindex="-1"></a>_Side-note: Why does MCC disagree with the Krippendorff alpha on the SAMRE vs. Baseline-Weak comparison? I would guess this is due to how ties were resolved when encoding the winner as binary._</span>
<span id="cb17-1101"><a href="#cb17-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1102"><a href="#cb17-1102" aria-hidden="true" tabindex="-1"></a>Finally, we can look at accuracy in terms of percentage agreement. Percentage agreement is not a "balanced" accuracy metric and therefore needs to be used with caution (for example, if the classes are imbalanced, then percentage agreement accuracy can be misleading). But it is the metric used in the paper.</span>
<span id="cb17-1103"><a href="#cb17-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1106"><a href="#cb17-1106" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-1107"><a href="#cb17-1107" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-1108"><a href="#cb17-1108" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb17-1109"><a href="#cb17-1109" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that calculates percentage agreement"</span></span>
<span id="cb17-1110"><a href="#cb17-1110" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate percentage agreement for each method</span></span>
<span id="cb17-1111"><a href="#cb17-1111" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_percent_agreement(df, rater1_col, rater2_col):</span>
<span id="cb17-1112"><a href="#cb17-1112" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate percentage agreement between two raters"""</span></span>
<span id="cb17-1113"><a href="#cb17-1113" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (df[rater1_col] <span class="op">==</span> df[rater2_col]).mean()</span>
<span id="cb17-1114"><a href="#cb17-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1115"><a href="#cb17-1115" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate agreement percentages</span></span>
<span id="cb17-1116"><a href="#cb17-1116" aria-hidden="true" tabindex="-1"></a>agreement_percentages <span class="op">=</span> pd.DataFrame({</span>
<span id="cb17-1117"><a href="#cb17-1117" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Method'</span>: [<span class="st">'Baseline-Strong'</span>, <span class="st">'Baseline-Weak'</span>, <span class="st">'SAMRE'</span>],</span>
<span id="cb17-1118"><a href="#cb17-1118" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Agreement'</span>: [</span>
<span id="cb17-1119"><a href="#cb17-1119" aria-hidden="true" tabindex="-1"></a>        calculate_percent_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_strong_winner'</span>),</span>
<span id="cb17-1120"><a href="#cb17-1120" aria-hidden="true" tabindex="-1"></a>        calculate_percent_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_weak_winner'</span>),</span>
<span id="cb17-1121"><a href="#cb17-1121" aria-hidden="true" tabindex="-1"></a>        calculate_percent_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'samre_winner'</span>)</span>
<span id="cb17-1122"><a href="#cb17-1122" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb17-1123"><a href="#cb17-1123" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb17-1124"><a href="#cb17-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1125"><a href="#cb17-1125" aria-hidden="true" tabindex="-1"></a><span class="co"># Round to 3 decimal places and convert to percentage</span></span>
<span id="cb17-1126"><a href="#cb17-1126" aria-hidden="true" tabindex="-1"></a>agreement_percentages[<span class="st">'Agreement'</span>] <span class="op">=</span> (agreement_percentages[<span class="st">'Agreement'</span>] <span class="op">*</span> <span class="dv">100</span>).<span class="bu">round</span>(<span class="dv">1</span>)</span>
<span id="cb17-1127"><a href="#cb17-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1128"><a href="#cb17-1128" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percentage point differences</span></span>
<span id="cb17-1129"><a href="#cb17-1129" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_diff <span class="op">=</span> (</span>
<span id="cb17-1130"><a href="#cb17-1130" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>] <span class="op">-</span></span>
<span id="cb17-1131"><a href="#cb17-1131" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb17-1132"><a href="#cb17-1132" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-1133"><a href="#cb17-1133" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_diff <span class="op">=</span> (</span>
<span id="cb17-1134"><a href="#cb17-1134" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>] <span class="op">-</span></span>
<span id="cb17-1135"><a href="#cb17-1135" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb17-1136"><a href="#cb17-1136" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-1137"><a href="#cb17-1137" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_diff <span class="op">=</span> (</span>
<span id="cb17-1138"><a href="#cb17-1138" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>] <span class="op">-</span></span>
<span id="cb17-1139"><a href="#cb17-1139" aria-hidden="true" tabindex="-1"></a>    agreement_percentages.loc[agreement_percentages[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'Agreement'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb17-1140"><a href="#cb17-1140" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-1141"><a href="#cb17-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1142"><a href="#cb17-1142" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb17-1143"><a href="#cb17-1143" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Percentage Agreement with Human Judgments:"</span>)</span>
<span id="cb17-1144"><a href="#cb17-1144" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(agreement_percentages)</span>
<span id="cb17-1145"><a href="#cb17-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1146"><a href="#cb17-1146" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Percentage Point Differences:"</span>)</span>
<span id="cb17-1147"><a href="#cb17-1147" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_diff<span class="sc">:+.1f}</span><span class="ss">"</span>)</span>
<span id="cb17-1148"><a href="#cb17-1148" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_diff<span class="sc">:+.1f}</span><span class="ss">"</span>)</span>
<span id="cb17-1149"><a href="#cb17-1149" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_diff<span class="sc">:+.1f}</span><span class="ss">"</span>)</span>
<span id="cb17-1150"><a href="#cb17-1150" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-1151"><a href="#cb17-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1152"><a href="#cb17-1152" aria-hidden="true" tabindex="-1"></a>Overall across these three metrics, the story is the same: SAMRE did not perform better than a baseline that is designed with best practices.</span>
<span id="cb17-1153"><a href="#cb17-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1154"><a href="#cb17-1154" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion</span></span>
<span id="cb17-1155"><a href="#cb17-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1156"><a href="#cb17-1156" aria-hidden="true" tabindex="-1"></a>In this post, I have shown that SAMRE does not perform better than a well-engineered baseline method. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2014–2025 Tyler Burleigh</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <i class="fa-brands fa-r-project" aria-label="r-project"></i>, <a href="https://quarto.org/">Quarto</a>, and the <a href="https://github.com/andrewheiss/ath-quarto">ath-quarto</a> theme</span></p>
</div>
  </div>
</footer>




</body></html>