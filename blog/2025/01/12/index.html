<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tyler Burleigh">
<meta name="description" content="In this post, I re-evaluate a method that was recently published in arXiv, critiquing the baseline model used in the paper and then implementing a new baseline model that implements standard best practices and similar multi-round aggregation. I find that the SAMRE method does not perform better than the new baseline model. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as the being skeptical of claims in research papers that compare new methods to a baseline.">

<title>A test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices | Tyler Burleigh – Tyler Burleigh</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../..//files/favico.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dd08061cb7210c315e315379d94beb87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-4c6f914d40da27735db8d274bf8358e4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PRHQZ8HPLB"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-PRHQZ8HPLB', { 'anonymize_ip': true});
</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background: #170C3A;
      }
</style>


</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Tyler Burleigh</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../cv/index.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../research/index.html"> 
<span class="menu-text">Research</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://fosstodon.org/users/tylerburleigh" rel="me"> <i class="bi bi-mastodon" role="img" aria-label="mastodon">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/tylerburleigh.bsky.social" rel="me"> <i class="bi bi-square" role="img" aria-label="bluesky">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tylerburleigh" rel="me"> <i class="bi bi-github" role="img" aria-label="github">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tylerburleigh" rel="me"> <i class="bi bi-linkedin" role="img" aria-label="linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tylerburleigh@gmail.com" rel="me"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default blog-post page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">A test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                  <div>
        <div class="description">
          In this post, I re-evaluate a method that was recently published in arXiv, critiquing the baseline model used in the paper and then implementing a new baseline model that implements standard best practices and similar multi-round aggregation. I find that the SAMRE method does not perform better than the new baseline model. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as the being skeptical of claims in research papers that compare new methods to a baseline.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">prompt-engineering</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">LLM-as-judge</div>
                <div class="quarto-category">LLM-evals</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://www.tylerburleigh.com/">Tyler Burleigh</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Sunday, January 12, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#baseline-model-prompt-inadequacies" id="toc-baseline-model-prompt-inadequacies" class="nav-link active" data-scroll-target="#baseline-model-prompt-inadequacies">Baseline model prompt inadequacies</a></li>
  <li><a href="#hypothesis-and-predictions" id="toc-hypothesis-and-predictions" class="nav-link" data-scroll-target="#hypothesis-and-predictions">Hypothesis and predictions</a></li>
  <li><a href="#my-implementation-of-samre-and-baseline" id="toc-my-implementation-of-samre-and-baseline" class="nav-link" data-scroll-target="#my-implementation-of-samre-and-baseline">My implementation of SAMRE and Baseline</a></li>
  <li><a href="#load-the-mt-bench-dataset" id="toc-load-the-mt-bench-dataset" class="nav-link" data-scroll-target="#load-the-mt-bench-dataset">Load the MT-bench dataset</a></li>
  <li><a href="#use-methods-to-evaluate-mt-bench-dataset" id="toc-use-methods-to-evaluate-mt-bench-dataset" class="nav-link" data-scroll-target="#use-methods-to-evaluate-mt-bench-dataset">Use methods to evaluate MT-bench dataset</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>I’ve been doing a lot of work with LLM-based evaluations lately, and I’ve been thinking about how to improve the quality of these evaluations.</p>
<p>I like to read research papers from arXiv for inspiration, and I recently came across a paper called <a href="https://arxiv.org/abs/2410.04663">Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates</a>, which introduces a new method inspired by judicial process called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process.</p>
<p>I was initially impressed by the results, as they reported a gain of 6-8% over the baseline method(!!). However, I am often skeptical of comparisons to “baseline” models in these research papers, as I find that they often fail to implement standard best practices and are therefore not represenative of true gains over baseline.</p>
<p>Given this skepticism of mine, I decided that it might be interesting to put it to the test: What if I implemented the SAMRE method, and compared it to a baseline model that does implement standard best practices for prompt engineering? Would I find that the SAMRE method is indeed an improvement over the baseline? Or would I find that SAMRE is inferior to a properly implemented baseline?</p>
<p>Using a sample of 180 conversations from MT-bench for testing and evaluation, I evaluated three methods:</p>
<ol type="1">
<li>SAMRE, as implemented in the paper</li>
<li>Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)</li>
<li>Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.</li>
</ol>
<p>After running the evaluations and calculating Krippendorff alpha agreement with human judges, I found that although SAMRE did yield better agreement than Baseline-Weak (18% improvement), it was inferior to Baseline-Strong (which was 36% better than SAMRE!). A similar result was found when examining binary classification accuracy using Matthews Correlation Coefficient (MCC).</p>
<p>These results serve to highlight the importance of implementing standard best practices in baseline models, as well as being skeptical of claims in research papers that compare new methods to a “baseline model”. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.</p>
<section id="baseline-model-prompt-inadequacies" class="level1">
<h1>Baseline model prompt inadequacies</h1>
<p>First, let’s consider some of the inadequacies in the Baseline model’s prompt reported in the paper. The prompt they used was as follows:</p>
<pre class="prompt"><code>You are a fair, impartial judge scoring a debate on the following question:
question.
Answer 1: answer_1
Answer 2: answer_2
Score each answer on a scale of 1-20 for each of the following criteria:
1. Relevance to the question
2. Accuracy of information and use of credible sources
3. Depth of analysis and completeness of argument
4. Clarity of expression and logical flow
5. Strength of reasoning and factual support
6. Effectiveness in addressing opponent’s points
Provide scores as [answer_1_score, answer_2_score] for each criterion in a list format, then sum for final scores. Please keep an eye on the slightest difference that should make a difference in the scoring. Don’t overthink!
Relevance:
Accuracy:
Depth:
Clarity:
Logic and Factuality:
Addressing opponent’s points:
Final Scores (sum of above) as a tuple (example: (18, 9)):
Explain your scoring, focusing on why one answer is better than the other based on the criteria above. Keep your explanation concise but informative.
Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).
Example: (18, 9)
Your scores and explanation:</code></pre>
<p>Here are the issues I see with this prompt:</p>
<ol type="1">
<li><p>The prompt does not use delimiters for most of the inputs. I would enclose the inputs inside XML tags like <question></question>, <answer1></answer1>, and <answer2></answer2>, but in a pinch delimiters like triple backticks can be used.</p></li>
<li><p>The prompt instructs the model to first generate scores in list format, and then to sum them. But as we know, language models models often make arithmetic mistakes. It would be better to ask the model to generate scores for each criterion, and then to programmatically extract and summarize them in python (or another programming language) from which the routine is run.</p></li>
<li><p>Although the prompt asks the model to “explain your scoring”, it is not clear if the model should be reasoning about each criterion before it scores them, or if it should provide reasoning at the end when giving its final score. I would ask the model to provide reasoning for each criterion that it is asked to score, and ask it to reason before scoring.</p></li>
<li><p>It’s unclear why a scale of 1-20 is used. This is not a standard scale for scoring. I would use a scale of 1-10 which is likely more familiar to the model and can be expected to be used more consistently.</p></li>
<li><p>Although the prompt does suggest that the model provide its scores in tuple format, it would be better to provide more explicit format instructions.</p></li>
<li><p>The prompt includes an “Effectiveness in addressing opponent’s points” criterion, but this is almost certainly irrelevant given that the answers to the question were not generated with the goal of addressing an opponent.</p></li>
<li><p>Finally, although this goes beyond the prompt itself, the authors of the paper are comparing a multi-round method to a single-round method. This is obviously an unfair comparison. Instead, it would be better to compare the SAMRE method to a baseline that uses the same number of rounds and then similarly averages its scores.</p></li>
</ol>
<p>With all of that in mind, here’s how I would rewrite the prompt:</p>
<pre class="prompt"><code>You are a fair, impartial judge scoring a debate on Question.

&lt;Question&gt;
{question}
&lt;/Question&gt;

Two Answers have been given to the Question.

&lt;Answer1&gt;
{answer_1}
&lt;/Answer1&gt;

&lt;Answer2&gt;
{answer_2}
&lt;/Answer2&gt;

The Answers are being judged on the following Criteria:

&lt;Criteria&gt;
&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;
&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;
&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;
&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;
&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;
&lt;/Criteria&gt;

For each Criterion, briefly analyze the performance of 
the two Answers, then give a score between 1 and 10.

Respond as follows:
&lt;Criterion1&gt;
&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;
&lt;Analysis&gt;
Answer 1: [Analysis of Answer 1 performance on the Criterion]
Answer 2: [Analysis of Answer 2 performance on the Criterion]
&lt;/Analysis&gt;
&lt;Scores&gt;
&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;
&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;
&lt;/Scores&gt;
&lt;/Criterion1&gt;
&lt;Criterion2&gt;
&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;
&lt;Analysis&gt;
Answer 1: [Analysis of Answer 1 performance on the Criterion]
Answer 2: [Analysis of Answer 2 performance on the Criterion]
&lt;/Analysis&gt;
&lt;Scores&gt;
&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;
&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;
&lt;/Scores&gt;
&lt;/Criterion2&gt;
...</code></pre>
<p>Notice that the prompt now uses XML tags to structure the instructions, that it asks the model to provide reasoning for each criterion before scoring, and that it gives the model a clear format for its response that reinforces analysis before scoring for each criterion.</p>
<p>I’ve also changed the scale from 1-20 to 1-10, removed the unnecessary “Effectiveness in addressing opponent’s points” criterion, and removed the instruction to summarize the scores, as I would handle this within the code.</p>
</section>
<section id="hypothesis-and-predictions" class="level1">
<h1>Hypothesis and predictions</h1>
<p>I hypothesize that SAMRE will not perform better than a baseline model that implements standard best practices for prompt engineering.</p>
<p>My predictions are as follows:</p>
<ol type="1">
<li>SAMRE will perform better than Baseline-Weak.</li>
<li>Baseline-Strong will perform better than Baseline-Weak.</li>
<li>Baseline-Strong will perform equal to or better than SAMRE.</li>
</ol>
</section>
<section id="my-implementation-of-samre-and-baseline" class="level1">
<h1>My implementation of SAMRE and Baseline</h1>
<p>Okay, so with those criticisms out of the way, let’s design evaluators to implement three methods:</p>
<ol type="1">
<li>SAMRE</li>
<li>Baseline-Weak: The baseline model used in the paper that does not implement standard best practices for prompt engineering.</li>
<li>Baseline-Strong: A baseline model that does implement standard best practices for prompt engineering.</li>
</ol>
<p>Below is my python implementation of the evaluators. To the best of my ability, I have implemented the SAMRE and Baseline methods as described in the paper (I call the paper’s Baseline method “Baseline-Weak”). And I have implemented a Baseline-Strong evaluator that implements what I would consider to be standard best practices for prompt engineering.</p>
<p>This will allow me to test my hypothesis and predictions.</p>
<div id="49682c25" class="cell" data-code-fold-show="false" data-execution_count="1">
<details class="code-fold">
<summary>Click to view the implementation code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass, field</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple, Dict</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> AsyncOpenAI</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> asynccontextmanager</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the prompts</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>PROMPTS <span class="op">=</span> {</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for advocates to defend their assigned answer, used in SAMRE</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="st">"defend_prompt"</span>: <span class="st">"""</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="st">You’re an eloquent advocate (Advocate </span><span class="sc">{advocate_id}</span><span class="st">) in a group defending this answer:</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer}</span><span class="st"> to the question: </span><span class="sc">{question}</span><span class="st">. Your opponent group defends: </span><span class="sc">{opponent_answer}</span><span class="st">.</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="st">Collaborate with your fellow advocates to convince the audience your argument is</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="st">better. Use the latest feedback, your opponent’s last argument, and your team’s previous</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="st">arguments to improve your case.</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="st">Latest feedback: </span><span class="sc">{feedback}</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="st">Opponent’s last argument: </span><span class="sc">{opponent_argument}</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="st">Your team’s previous arguments: team_arguments</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="st">Respond in under 80 words.</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="st">Your defense:</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for judge to provide feedback on debate progress, used in SAMRE</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="st">"judge_prompt"</span>: <span class="st">"""</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="st">You’re a fair, impartial judge in a debate on: "</span><span class="sc">{question}</span><span class="st">". Answer 1: "</span><span class="sc">{answer_1}</span><span class="st">".</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: "</span><span class="sc">{answer_2}</span><span class="st">". Your goal is to provide feedback that will help advocate groups</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="st">improve and differentiate their arguments more clearly.</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="st">Current round: </span><span class="sc">{current_round}</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="st">Total rounds: </span><span class="sc">{total_rounds}</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="st">Previous scores: </span><span class="sc">{previous_scores}</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 1st answer: </span><span class="sc">{defense_1}</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 2nd answer: </span><span class="sc">{defense_2}</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="st">Provide specific, constructive feedback to help each advocate group strengthen their</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="st">unique position. Encourage them to address weaknesses and highlight distinctions. Aim</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="st">for your feedback to lead to more divergent scores in future rounds.</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="st">Give your feedback in under 50 words:</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for SAMRE method scoring</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_samre"</span>: <span class="st">"""</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="st">You’re a critical, impartial judge in a high-stakes debate on: "</span><span class="sc">{question}</span><span class="st">". Answer</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="st">1: "</span><span class="sc">{answer_1}</span><span class="st">". Answer 2: "</span><span class="sc">{answer_2}</span><span class="st">". Your goal is to provide detailed, constructive</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="st">feedback that will push advocates to significantly improve their arguments.</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="st">Total rounds: </span><span class="sc">{total_rounds}</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="st">Previous scores: </span><span class="sc">{previous_scores}</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 1st answer: </span><span class="sc">{defense_1}</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 2nd answer: </span><span class="sc">{defense_2}</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="st">Analyze each argument meticulously. Be thorough and unbiased in your assessment of:</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="st">1. Relevance to the question</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="st">2. Accuracy of information and use of credible sources</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="st">3. Depth of analysis and completeness of argument</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="st">4. Clarity of expression and logical flow</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="st">5. Strength of reasoning and factual support</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="st">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="st">For each criterion, provide a score on a scale of 1-20 and detailed justification.</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="st">Scores should be given as [answer_1_score, answer_2_score] for each criterion.</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="st">Your comprehensive feedback for each advocate (50 words each):</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="st">Feedback for Advocate 1:</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="st">Feedback for Advocate 2:</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a><span class="st">Sum up the scores and return the final score tuple (score1, score2). Example: (95, 87)</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a><span class="st">Your detailed scores and final tally:</span></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for Baseline-Weak method scoring, which represents the baseline model used in the paper</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_baseline_weak"</span>: <span class="st">"""</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="st">You are a fair, impartial judge scoring a debate on the following question:</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="st">question.</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: </span><span class="sc">{answer_1}</span></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: </span><span class="sc">{answer_2}</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="st">Score each answer on a scale of 1-20 for each of the following criteria:</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="st">1. Relevance to the question</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="st">2. Accuracy of information and use of credible sources</span></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a><span class="st">3. Depth of analysis and completeness of argument</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a><span class="st">4. Clarity of expression and logical flow</span></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a><span class="st">5. Strength of reasoning and factual support</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a><span class="st">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a><span class="st">Provide scores as [Answer1_score, Answer2_score] for each criterion in a list format,</span></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a><span class="st">then sum for final scores. Please keep an eye on the slightest difference that should</span></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a><span class="st">make a difference in the scoring. Don’t overthink!</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a><span class="st">Relevance:</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a><span class="st">Accuracy:</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a><span class="st">Depth:</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a><span class="st">Clarity:</span></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a><span class="st">Logic and Factuality:</span></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="st">Addressing opponent’s points:</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a><span class="st">Final Scores (sum of above) as a tuple (example: (18, 9)):</span></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a><span class="st">Explain your scoring, focusing on why one answer is better than the other based on the</span></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a><span class="st">criteria above. Keep your explanation concise but informative.</span></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a><span class="st">Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).</span></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a><span class="st">Example: (18, 9)</span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a><span class="st">Your scores and explanation:</span></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for Baseline-Strong method scoring, which implements what I consider to be standard best practices for prompt engineering</span></span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_baseline_strong"</span>: <span class="st">"""</span></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a><span class="st">You are a fair, impartial judge scoring a debate on Question.</span></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Question&gt;</span></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a><span class="sc">{question}</span></span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Question&gt;</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a><span class="st">Two Answers have been given to the Question.</span></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1&gt;</span></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_1}</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Answer1&gt;</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2&gt;</span></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_2}</span></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Answer2&gt;</span></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="st">The Answers are being judged on the following Criteria:</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criteria&gt;</span></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;</span></span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;</span></span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;</span></span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;</span></span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criteria&gt;</span></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a><span class="st">For each Criterion, briefly analyze the performance of </span></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a><span class="st">the two Answers, then give a score between 1 and 10.</span></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a><span class="st">Respond as follows:</span></span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion1&gt;</span></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;</span></span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Analysis&gt;</span></span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Analysis&gt;</span></span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Scores&gt;</span></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Scores&gt;</span></span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criterion1&gt;</span></span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion2&gt;</span></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;</span></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Analysis&gt;</span></span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Analysis&gt;</span></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Scores&gt;</span></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Scores&gt;</span></span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criterion2&gt;</span></span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a><span class="st">...</span></span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Memory:</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stores debate history including arguments, scores, and feedback for each round, used in SAMRE"""</span></span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>    arguments: List[Tuple[<span class="bu">str</span>, <span class="bu">str</span>]] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>    scores: List[Tuple[<span class="bu">float</span>, <span class="bu">float</span>]] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>    feedback: List[<span class="bu">str</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelEvaluator:</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>    <span class="at">@classmethod</span></span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>    <span class="at">@asynccontextmanager</span></span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> create(cls, mode<span class="op">=</span><span class="st">"samre"</span>, model<span class="op">=</span><span class="st">"gpt-4o-mini"</span>, logging_level<span class="op">=</span>logging.WARNING):</span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Factory method to create evaluator instance with proper async context management"""</span></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>        instance <span class="op">=</span> cls(mode<span class="op">=</span>mode, model<span class="op">=</span>model, logging_level<span class="op">=</span>logging_level)</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>        instance.client <span class="op">=</span> AsyncOpenAI()</span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> instance</span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>        <span class="cf">finally</span>:</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> instance.client.close()</span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _setup_logger(<span class="va">self</span>, logging_level):</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Setup logger with word wrapping."""</span></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>        logger <span class="op">=</span> logging.getLogger(<span class="va">__name__</span>)</span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>        logger.setLevel(logging_level)</span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> logger.handlers:</span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a>            handler <span class="op">=</span> logging.StreamHandler()</span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>            <span class="kw">class</span> WrapFormatter(logging.Formatter):</span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>                <span class="kw">def</span> <span class="bu">format</span>(<span class="va">self</span>, record):</span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>                    <span class="im">import</span> textwrap</span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>                    message <span class="op">=</span> <span class="bu">super</span>().<span class="bu">format</span>(record)</span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(textwrap.fill(line, width<span class="op">=</span><span class="dv">80</span>) </span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>                                <span class="cf">for</span> line <span class="kw">in</span> message.split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>))</span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a>            formatter <span class="op">=</span> WrapFormatter(<span class="st">'</span><span class="sc">%(message)s</span><span class="st">'</span>)</span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a>            handler.setFormatter(formatter)</span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>            logger.addHandler(handler)</span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logger</span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mode<span class="op">=</span><span class="st">"samre"</span>, model<span class="op">=</span><span class="st">"gpt-4o-mini"</span>, logging_level<span class="op">=</span>logging.WARNING):</span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mode <span class="op">=</span> mode</span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Modify to handle both baseline modes</span></span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_rounds <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> mode.startswith(<span class="st">"baseline"</span>) <span class="cf">else</span> <span class="dv">4</span></span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger <span class="op">=</span> <span class="va">self</span>._setup_logger(logging_level)</span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize all prompts</span></span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.defend_prompt <span class="op">=</span> PROMPTS[<span class="st">"defend_prompt"</span>]</span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.judge_prompt <span class="op">=</span> PROMPTS[<span class="st">"judge_prompt"</span>]</span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> get_completion(<span class="va">self</span>, prompt: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get a completion from the OpenAI API."""</span></span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.client:</span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'"</span>)</span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.client.chat.completions.create(</span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span><span class="va">self</span>.model,</span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: prompt}],</span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span><span class="dv">0</span></span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _extract_final_scores(<span class="va">self</span>, score_response: <span class="bu">str</span>) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extracts final scores from model response based on evaluation mode"""</span></span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"samre"</span>:</span>
<span id="cb3-214"><a href="#cb3-214" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Look for final tuple in format (score1, score2)</span></span>
<span id="cb3-215"><a href="#cb3-215" aria-hidden="true" tabindex="-1"></a>            tuple_pattern <span class="op">=</span> <span class="vs">r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'</span></span>
<span id="cb3-216"><a href="#cb3-216" aria-hidden="true" tabindex="-1"></a>            match <span class="op">=</span> re.search(tuple_pattern, score_response)</span>
<span id="cb3-217"><a href="#cb3-217" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match:</span>
<span id="cb3-218"><a href="#cb3-218" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> (<span class="bu">float</span>(match.group(<span class="dv">1</span>)), <span class="bu">float</span>(match.group(<span class="dv">2</span>)))</span>
<span id="cb3-219"><a href="#cb3-219" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find score tuple in SAMRE response"</span>)</span>
<span id="cb3-220"><a href="#cb3-220" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-221"><a href="#cb3-221" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_weak"</span>:</span>
<span id="cb3-222"><a href="#cb3-222" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Look for final tuple in format (score1, score2)</span></span>
<span id="cb3-223"><a href="#cb3-223" aria-hidden="true" tabindex="-1"></a>            tuple_pattern <span class="op">=</span> <span class="vs">r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'</span></span>
<span id="cb3-224"><a href="#cb3-224" aria-hidden="true" tabindex="-1"></a>            match <span class="op">=</span> re.search(tuple_pattern, score_response)</span>
<span id="cb3-225"><a href="#cb3-225" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match:</span>
<span id="cb3-226"><a href="#cb3-226" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> (<span class="bu">float</span>(match.group(<span class="dv">1</span>)), <span class="bu">float</span>(match.group(<span class="dv">2</span>)))</span>
<span id="cb3-227"><a href="#cb3-227" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find score tuple in weak baseline response"</span>)</span>
<span id="cb3-228"><a href="#cb3-228" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-229"><a href="#cb3-229" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_strong"</span>:</span>
<span id="cb3-230"><a href="#cb3-230" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use XML parsing for strong baseline</span></span>
<span id="cb3-231"><a href="#cb3-231" aria-hidden="true" tabindex="-1"></a>            score_a_pattern <span class="op">=</span> <span class="vs">r'&lt;Answer1Score&gt;\s*(\d+\.?\d*)\s*&lt;/Answer1Score&gt;'</span></span>
<span id="cb3-232"><a href="#cb3-232" aria-hidden="true" tabindex="-1"></a>            score_b_pattern <span class="op">=</span> <span class="vs">r'&lt;Answer2Score&gt;\s*(\d+\.?\d*)\s*&lt;/Answer2Score&gt;'</span></span>
<span id="cb3-233"><a href="#cb3-233" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-234"><a href="#cb3-234" aria-hidden="true" tabindex="-1"></a>            scores_a <span class="op">=</span> [<span class="bu">float</span>(match.group(<span class="dv">1</span>)) <span class="cf">for</span> match <span class="kw">in</span> re.finditer(score_a_pattern, score_response)]</span>
<span id="cb3-235"><a href="#cb3-235" aria-hidden="true" tabindex="-1"></a>            scores_b <span class="op">=</span> [<span class="bu">float</span>(match.group(<span class="dv">1</span>)) <span class="cf">for</span> match <span class="kw">in</span> re.finditer(score_b_pattern, score_response)]</span>
<span id="cb3-236"><a href="#cb3-236" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-237"><a href="#cb3-237" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> scores_a <span class="kw">or</span> <span class="kw">not</span> scores_b:</span>
<span id="cb3-238"><a href="#cb3-238" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find scores for both candidates"</span>)</span>
<span id="cb3-239"><a href="#cb3-239" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-240"><a href="#cb3-240" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(scores_a) <span class="op">!=</span> <span class="bu">len</span>(scores_b):</span>
<span id="cb3-241"><a href="#cb3-241" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Mismatched number of scores: A=</span><span class="sc">{</span><span class="bu">len</span>(scores_a)<span class="sc">}</span><span class="ss">, B=</span><span class="sc">{</span><span class="bu">len</span>(scores_b)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-242"><a href="#cb3-242" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-243"><a href="#cb3-243" aria-hidden="true" tabindex="-1"></a>            final_score_a <span class="op">=</span> <span class="bu">sum</span>(scores_a) <span class="op">/</span> <span class="bu">len</span>(scores_a)</span>
<span id="cb3-244"><a href="#cb3-244" aria-hidden="true" tabindex="-1"></a>            final_score_b <span class="op">=</span> <span class="bu">sum</span>(scores_b) <span class="op">/</span> <span class="bu">len</span>(scores_b)</span>
<span id="cb3-245"><a href="#cb3-245" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-246"><a href="#cb3-246" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (final_score_a, final_score_b)</span>
<span id="cb3-247"><a href="#cb3-247" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-248"><a href="#cb3-248" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-249"><a href="#cb3-249" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Unknown mode: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>mode<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-250"><a href="#cb3-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-251"><a href="#cb3-251" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> evaluate(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, num_rounds: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb3-252"><a href="#cb3-252" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Main evaluation entry point that routes to appropriate evaluation method based on mode"""</span></span>
<span id="cb3-253"><a href="#cb3-253" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.client:</span>
<span id="cb3-254"><a href="#cb3-254" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'"</span>)</span>
<span id="cb3-255"><a href="#cb3-255" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-256"><a href="#cb3-256" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mode.startswith(<span class="st">"baseline"</span>):</span>
<span id="cb3-257"><a href="#cb3-257" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">=== Starting </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>mode<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Evaluation ===</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb3-258"><a href="#cb3-258" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>._evaluate_baseline(question, answer_1, answer_2, num_rounds)</span>
<span id="cb3-259"><a href="#cb3-259" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-260"><a href="#cb3-260" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting SAMRE Evaluation ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-261"><a href="#cb3-261" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>._evaluate_samre(question, answer_1, answer_2)</span>
<span id="cb3-262"><a href="#cb3-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-263"><a href="#cb3-263" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _evaluate_baseline(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, num_rounds: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb3-264"><a href="#cb3-264" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implements baseline evaluation methods (both weak and strong)"""</span></span>
<span id="cb3-265"><a href="#cb3-265" aria-hidden="true" tabindex="-1"></a>        score_history <span class="op">=</span> []</span>
<span id="cb3-266"><a href="#cb3-266" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-267"><a href="#cb3-267" aria-hidden="true" tabindex="-1"></a>        num_rounds <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_weak"</span> <span class="cf">else</span> num_rounds</span>
<span id="cb3-268"><a href="#cb3-268" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_rounds):</span>
<span id="cb3-269"><a href="#cb3-269" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Select appropriate prompt based on mode</span></span>
<span id="cb3-270"><a href="#cb3-270" aria-hidden="true" tabindex="-1"></a>            prompt_key <span class="op">=</span> <span class="st">"score_prompt_"</span> <span class="op">+</span> <span class="va">self</span>.mode</span>
<span id="cb3-271"><a href="#cb3-271" aria-hidden="true" tabindex="-1"></a>            score_prompt <span class="op">=</span> PROMPTS[prompt_key].<span class="bu">format</span>(</span>
<span id="cb3-272"><a href="#cb3-272" aria-hidden="true" tabindex="-1"></a>                question<span class="op">=</span>question,</span>
<span id="cb3-273"><a href="#cb3-273" aria-hidden="true" tabindex="-1"></a>                answer_1<span class="op">=</span>answer_1,</span>
<span id="cb3-274"><a href="#cb3-274" aria-hidden="true" tabindex="-1"></a>                answer_2<span class="op">=</span>answer_2</span>
<span id="cb3-275"><a href="#cb3-275" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-276"><a href="#cb3-276" aria-hidden="true" tabindex="-1"></a>            score_response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(score_prompt)</span>
<span id="cb3-277"><a href="#cb3-277" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"Score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-278"><a href="#cb3-278" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-279"><a href="#cb3-279" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb3-280"><a href="#cb3-280" aria-hidden="true" tabindex="-1"></a>                round_scores <span class="op">=</span> <span class="va">self</span>._extract_final_scores(score_response)</span>
<span id="cb3-281"><a href="#cb3-281" aria-hidden="true" tabindex="-1"></a>                score_history.append(<span class="bu">list</span>(round_scores))</span>
<span id="cb3-282"><a href="#cb3-282" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb3-283"><a href="#cb3-283" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.error(<span class="ss">f"Score parsing error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-284"><a href="#cb3-284" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.error(<span class="ss">f"Raw score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-285"><a href="#cb3-285" aria-hidden="true" tabindex="-1"></a>                score_history.append([<span class="fl">10.0</span>, <span class="fl">10.0</span>])</span>
<span id="cb3-286"><a href="#cb3-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-287"><a href="#cb3-287" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate average scores across all rounds</span></span>
<span id="cb3-288"><a href="#cb3-288" aria-hidden="true" tabindex="-1"></a>        avg_scores <span class="op">=</span> [</span>
<span id="cb3-289"><a href="#cb3-289" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span>(scores[i] <span class="cf">for</span> scores <span class="kw">in</span> score_history) <span class="op">/</span> <span class="bu">len</span>(score_history)</span>
<span id="cb3-290"><a href="#cb3-290" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb3-291"><a href="#cb3-291" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-292"><a href="#cb3-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-293"><a href="#cb3-293" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Determine winner based on average scores</span></span>
<span id="cb3-294"><a href="#cb3-294" aria-hidden="true" tabindex="-1"></a>        winner <span class="op">=</span> (</span>
<span id="cb3-295"><a href="#cb3-295" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&gt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb3-296"><a href="#cb3-296" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&lt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb3-297"><a href="#cb3-297" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'tie'</span></span>
<span id="cb3-298"><a href="#cb3-298" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-299"><a href="#cb3-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-300"><a href="#cb3-300" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb3-301"><a href="#cb3-301" aria-hidden="true" tabindex="-1"></a>            <span class="st">"winner"</span>: winner,</span>
<span id="cb3-302"><a href="#cb3-302" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_scores"</span>: [<span class="bu">round</span>(score, <span class="dv">2</span>) <span class="cf">for</span> score <span class="kw">in</span> avg_scores] ,</span>
<span id="cb3-303"><a href="#cb3-303" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rounds"</span>: <span class="bu">len</span>(score_history),</span>
<span id="cb3-304"><a href="#cb3-304" aria-hidden="true" tabindex="-1"></a>            <span class="st">"score_history"</span>: score_history,</span>
<span id="cb3-305"><a href="#cb3-305" aria-hidden="true" tabindex="-1"></a>            <span class="st">"full_response"</span>: score_response  <span class="co"># Include the final response for analysis</span></span>
<span id="cb3-306"><a href="#cb3-306" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-307"><a href="#cb3-307" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-308"><a href="#cb3-308" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _evaluate_samre(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb3-309"><a href="#cb3-309" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implements SAMRE evaluation with multi-round debate process</span></span>
<span id="cb3-310"><a href="#cb3-310" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-311"><a href="#cb3-311" aria-hidden="true" tabindex="-1"></a><span class="co">        Flow:</span></span>
<span id="cb3-312"><a href="#cb3-312" aria-hidden="true" tabindex="-1"></a><span class="co">        1. Get defenses from both advocates</span></span>
<span id="cb3-313"><a href="#cb3-313" aria-hidden="true" tabindex="-1"></a><span class="co">        2. Judge provides feedback and scores</span></span>
<span id="cb3-314"><a href="#cb3-314" aria-hidden="true" tabindex="-1"></a><span class="co">        3. Repeat until max rounds or convergence</span></span>
<span id="cb3-315"><a href="#cb3-315" aria-hidden="true" tabindex="-1"></a><span class="co">        4. Return averaged results</span></span>
<span id="cb3-316"><a href="#cb3-316" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-317"><a href="#cb3-317" aria-hidden="true" tabindex="-1"></a>        local_memory <span class="op">=</span> Memory()</span>
<span id="cb3-318"><a href="#cb3-318" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-319"><a href="#cb3-319" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting SAMRE Evaluation ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-320"><a href="#cb3-320" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-321"><a href="#cb3-321" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> round_num <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.max_rounds):</span>
<span id="cb3-322"><a href="#cb3-322" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Round </span><span class="sc">{</span>round_num <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb3-323"><a href="#cb3-323" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-324"><a href="#cb3-324" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>._run_debate_round(</span>
<span id="cb3-325"><a href="#cb3-325" aria-hidden="true" tabindex="-1"></a>                question,</span>
<span id="cb3-326"><a href="#cb3-326" aria-hidden="true" tabindex="-1"></a>                answer_1, </span>
<span id="cb3-327"><a href="#cb3-327" aria-hidden="true" tabindex="-1"></a>                answer_2, </span>
<span id="cb3-328"><a href="#cb3-328" aria-hidden="true" tabindex="-1"></a>                round_num,</span>
<span id="cb3-329"><a href="#cb3-329" aria-hidden="true" tabindex="-1"></a>                local_memory</span>
<span id="cb3-330"><a href="#cb3-330" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-331"><a href="#cb3-331" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-332"><a href="#cb3-332" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._has_scores_converged(round_num, local_memory):</span>
<span id="cb3-333"><a href="#cb3-333" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">Scores have converged - ending debate early."</span>)</span>
<span id="cb3-334"><a href="#cb3-334" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb3-335"><a href="#cb3-335" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-336"><a href="#cb3-336" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._prepare_results(local_memory)</span>
<span id="cb3-337"><a href="#cb3-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-338"><a href="#cb3-338" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> defend_answer(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, </span>
<span id="cb3-339"><a href="#cb3-339" aria-hidden="true" tabindex="-1"></a>                        advocate_id: <span class="bu">int</span>, feedback: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>, </span>
<span id="cb3-340"><a href="#cb3-340" aria-hidden="true" tabindex="-1"></a>                        opponent_argument: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>,</span>
<span id="cb3-341"><a href="#cb3-341" aria-hidden="true" tabindex="-1"></a>                        team_arguments: List[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb3-342"><a href="#cb3-342" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get defense from an advocate.</span></span>
<span id="cb3-343"><a href="#cb3-343" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-344"><a href="#cb3-344" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb3-345"><a href="#cb3-345" aria-hidden="true" tabindex="-1"></a><span class="co">            question: The question being debated</span></span>
<span id="cb3-346"><a href="#cb3-346" aria-hidden="true" tabindex="-1"></a><span class="co">            answer_1: First answer in the debate</span></span>
<span id="cb3-347"><a href="#cb3-347" aria-hidden="true" tabindex="-1"></a><span class="co">            answer_2: Second answer in the debate</span></span>
<span id="cb3-348"><a href="#cb3-348" aria-hidden="true" tabindex="-1"></a><span class="co">            advocate_id: Which advocate (1 or 2) is defending</span></span>
<span id="cb3-349"><a href="#cb3-349" aria-hidden="true" tabindex="-1"></a><span class="co">            feedback: Previous feedback from judge</span></span>
<span id="cb3-350"><a href="#cb3-350" aria-hidden="true" tabindex="-1"></a><span class="co">            opponent_argument: Last argument from opponent</span></span>
<span id="cb3-351"><a href="#cb3-351" aria-hidden="true" tabindex="-1"></a><span class="co">            team_arguments: List of previous arguments from this advocate's team</span></span>
<span id="cb3-352"><a href="#cb3-352" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-353"><a href="#cb3-353" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> team_arguments <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-354"><a href="#cb3-354" aria-hidden="true" tabindex="-1"></a>            team_arguments <span class="op">=</span> []</span>
<span id="cb3-355"><a href="#cb3-355" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-356"><a href="#cb3-356" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Map answers based on advocate_id</span></span>
<span id="cb3-357"><a href="#cb3-357" aria-hidden="true" tabindex="-1"></a>        answer <span class="op">=</span> answer_1 <span class="cf">if</span> advocate_id <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> answer_2</span>
<span id="cb3-358"><a href="#cb3-358" aria-hidden="true" tabindex="-1"></a>        opponent_answer <span class="op">=</span> answer_2 <span class="cf">if</span> advocate_id <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> answer_1</span>
<span id="cb3-359"><a href="#cb3-359" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-360"><a href="#cb3-360" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> <span class="va">self</span>.defend_prompt.<span class="bu">format</span>(</span>
<span id="cb3-361"><a href="#cb3-361" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb3-362"><a href="#cb3-362" aria-hidden="true" tabindex="-1"></a>            advocate_id<span class="op">=</span>advocate_id,</span>
<span id="cb3-363"><a href="#cb3-363" aria-hidden="true" tabindex="-1"></a>            answer<span class="op">=</span>answer,  <span class="co"># The answer this advocate is defending</span></span>
<span id="cb3-364"><a href="#cb3-364" aria-hidden="true" tabindex="-1"></a>            opponent_answer<span class="op">=</span>opponent_answer,  <span class="co"># The opposing answer</span></span>
<span id="cb3-365"><a href="#cb3-365" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>feedback,</span>
<span id="cb3-366"><a href="#cb3-366" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>opponent_argument,</span>
<span id="cb3-367"><a href="#cb3-367" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(team_arguments)</span>
<span id="cb3-368"><a href="#cb3-368" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-369"><a href="#cb3-369" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(prompt)</span>
<span id="cb3-370"><a href="#cb3-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-371"><a href="#cb3-371" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> judge_debate(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>,</span>
<span id="cb3-372"><a href="#cb3-372" aria-hidden="true" tabindex="-1"></a>                          defense_1: <span class="bu">str</span>, defense_2: <span class="bu">str</span>, </span>
<span id="cb3-373"><a href="#cb3-373" aria-hidden="true" tabindex="-1"></a>                          current_round: <span class="bu">int</span>,</span>
<span id="cb3-374"><a href="#cb3-374" aria-hidden="true" tabindex="-1"></a>                          memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, Tuple[<span class="bu">float</span>, <span class="bu">float</span>]]:</span>
<span id="cb3-375"><a href="#cb3-375" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Judge the debate between two answers."""</span></span>
<span id="cb3-376"><a href="#cb3-376" aria-hidden="true" tabindex="-1"></a>        feedback_prompt <span class="op">=</span> <span class="va">self</span>.judge_prompt.<span class="bu">format</span>(</span>
<span id="cb3-377"><a href="#cb3-377" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb3-378"><a href="#cb3-378" aria-hidden="true" tabindex="-1"></a>            answer_1<span class="op">=</span>answer_1,</span>
<span id="cb3-379"><a href="#cb3-379" aria-hidden="true" tabindex="-1"></a>            answer_2<span class="op">=</span>answer_2,</span>
<span id="cb3-380"><a href="#cb3-380" aria-hidden="true" tabindex="-1"></a>            current_round<span class="op">=</span>current_round,</span>
<span id="cb3-381"><a href="#cb3-381" aria-hidden="true" tabindex="-1"></a>            total_rounds<span class="op">=</span><span class="va">self</span>.max_rounds,</span>
<span id="cb3-382"><a href="#cb3-382" aria-hidden="true" tabindex="-1"></a>            previous_scores<span class="op">=</span>memory.scores,</span>
<span id="cb3-383"><a href="#cb3-383" aria-hidden="true" tabindex="-1"></a>            defense_1<span class="op">=</span>defense_1,</span>
<span id="cb3-384"><a href="#cb3-384" aria-hidden="true" tabindex="-1"></a>            defense_2<span class="op">=</span>defense_2</span>
<span id="cb3-385"><a href="#cb3-385" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-386"><a href="#cb3-386" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(feedback_prompt)</span>
<span id="cb3-387"><a href="#cb3-387" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-388"><a href="#cb3-388" aria-hidden="true" tabindex="-1"></a>        score_prompt <span class="op">=</span> PROMPTS[<span class="st">"score_prompt_samre"</span>].<span class="bu">format</span>(</span>
<span id="cb3-389"><a href="#cb3-389" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb3-390"><a href="#cb3-390" aria-hidden="true" tabindex="-1"></a>            answer_1<span class="op">=</span>answer_1,</span>
<span id="cb3-391"><a href="#cb3-391" aria-hidden="true" tabindex="-1"></a>            answer_2<span class="op">=</span>answer_2,</span>
<span id="cb3-392"><a href="#cb3-392" aria-hidden="true" tabindex="-1"></a>            defense_1<span class="op">=</span>defense_1,</span>
<span id="cb3-393"><a href="#cb3-393" aria-hidden="true" tabindex="-1"></a>            defense_2<span class="op">=</span>defense_2,</span>
<span id="cb3-394"><a href="#cb3-394" aria-hidden="true" tabindex="-1"></a>            total_rounds<span class="op">=</span><span class="va">self</span>.max_rounds,</span>
<span id="cb3-395"><a href="#cb3-395" aria-hidden="true" tabindex="-1"></a>            previous_scores<span class="op">=</span>memory.scores,</span>
<span id="cb3-396"><a href="#cb3-396" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>feedback</span>
<span id="cb3-397"><a href="#cb3-397" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-398"><a href="#cb3-398" aria-hidden="true" tabindex="-1"></a>        score_response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(score_prompt)    </span>
<span id="cb3-399"><a href="#cb3-399" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-400"><a href="#cb3-400" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-401"><a href="#cb3-401" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb3-402"><a href="#cb3-402" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="va">self</span>._extract_final_scores(score_response)</span>
<span id="cb3-403"><a href="#cb3-403" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb3-404"><a href="#cb3-404" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Score parsing error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-405"><a href="#cb3-405" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Raw score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-406"><a href="#cb3-406" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> (<span class="fl">10.0</span>, <span class="fl">10.0</span>)</span>
<span id="cb3-407"><a href="#cb3-407" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-408"><a href="#cb3-408" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> feedback, scores</span>
<span id="cb3-409"><a href="#cb3-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-410"><a href="#cb3-410" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _run_debate_round(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, </span>
<span id="cb3-411"><a href="#cb3-411" aria-hidden="true" tabindex="-1"></a>                               round_num: <span class="bu">int</span>, memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb3-412"><a href="#cb3-412" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Executes single debate round in SAMRE evaluation"""</span></span>
<span id="cb3-413"><a href="#cb3-413" aria-hidden="true" tabindex="-1"></a>        defenses <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>._get_advocate_defenses(question, answer_1, answer_2, memory)</span>
<span id="cb3-414"><a href="#cb3-414" aria-hidden="true" tabindex="-1"></a>        memory.arguments.append(defenses)</span>
<span id="cb3-415"><a href="#cb3-415" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-416"><a href="#cb3-416" aria-hidden="true" tabindex="-1"></a>        feedback, scores <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.judge_debate(</span>
<span id="cb3-417"><a href="#cb3-417" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, defenses[<span class="dv">0</span>], defenses[<span class="dv">1</span>], round_num <span class="op">+</span> <span class="dv">1</span>, memory</span>
<span id="cb3-418"><a href="#cb3-418" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-419"><a href="#cb3-419" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-420"><a href="#cb3-420" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._store_round_results(feedback, scores, memory)</span>
<span id="cb3-421"><a href="#cb3-421" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._display_round_results(defenses, feedback, scores)</span>
<span id="cb3-422"><a href="#cb3-422" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-423"><a href="#cb3-423" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scores</span>
<span id="cb3-424"><a href="#cb3-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-425"><a href="#cb3-425" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _get_advocate_defenses(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>,</span>
<span id="cb3-426"><a href="#cb3-426" aria-hidden="true" tabindex="-1"></a>                                   memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, <span class="bu">str</span>]:</span>
<span id="cb3-427"><a href="#cb3-427" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get defenses from both advocates."""</span></span>
<span id="cb3-428"><a href="#cb3-428" aria-hidden="true" tabindex="-1"></a>        defense_1 <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.defend_answer(</span>
<span id="cb3-429"><a href="#cb3-429" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, <span class="dv">1</span>,</span>
<span id="cb3-430"><a href="#cb3-430" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>memory.feedback[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory.feedback <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb3-431"><a href="#cb3-431" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>memory.arguments[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>] <span class="cf">if</span> memory.arguments <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb3-432"><a href="#cb3-432" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span>[args[<span class="dv">0</span>] <span class="cf">for</span> args <span class="kw">in</span> memory.arguments]</span>
<span id="cb3-433"><a href="#cb3-433" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-434"><a href="#cb3-434" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-435"><a href="#cb3-435" aria-hidden="true" tabindex="-1"></a>        defense_2 <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.defend_answer(</span>
<span id="cb3-436"><a href="#cb3-436" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, <span class="dv">2</span>,</span>
<span id="cb3-437"><a href="#cb3-437" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>memory.feedback[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory.feedback <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb3-438"><a href="#cb3-438" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>memory.arguments[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="cf">if</span> memory.arguments <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb3-439"><a href="#cb3-439" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span>[args[<span class="dv">1</span>] <span class="cf">for</span> args <span class="kw">in</span> memory.arguments]</span>
<span id="cb3-440"><a href="#cb3-440" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-441"><a href="#cb3-441" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-442"><a href="#cb3-442" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (defense_1, defense_2)</span>
<span id="cb3-443"><a href="#cb3-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-444"><a href="#cb3-444" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _store_round_results(<span class="va">self</span>, feedback: <span class="bu">str</span>, scores: Tuple[<span class="bu">float</span>, <span class="bu">float</span>],</span>
<span id="cb3-445"><a href="#cb3-445" aria-hidden="true" tabindex="-1"></a>                           memory: Memory) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-446"><a href="#cb3-446" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Store feedback and scores from the round."""</span></span>
<span id="cb3-447"><a href="#cb3-447" aria-hidden="true" tabindex="-1"></a>        memory.feedback.append(feedback)</span>
<span id="cb3-448"><a href="#cb3-448" aria-hidden="true" tabindex="-1"></a>        memory.scores.append(scores)</span>
<span id="cb3-449"><a href="#cb3-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-450"><a href="#cb3-450" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _display_round_results(<span class="va">self</span>, defenses: Tuple[<span class="bu">str</span>, <span class="bu">str</span>], </span>
<span id="cb3-451"><a href="#cb3-451" aria-hidden="true" tabindex="-1"></a>                             feedback: <span class="bu">str</span>, scores: Tuple[<span class="bu">float</span>, <span class="bu">float</span>]) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-452"><a href="#cb3-452" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Display the results of the current round."""</span></span>
<span id="cb3-453"><a href="#cb3-453" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Advocate 1's defense:</span><span class="ch">\n</span><span class="sc">{</span>defenses[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-454"><a href="#cb3-454" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Advocate 2's defense:</span><span class="ch">\n</span><span class="sc">{</span>defenses[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-455"><a href="#cb3-455" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Judge's feedback:</span><span class="ch">\n</span><span class="sc">{</span>feedback<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-456"><a href="#cb3-456" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Scores for this round: Answer 1 = </span><span class="sc">{</span><span class="bu">round</span>(scores[<span class="dv">0</span>], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">, Answer 2 = </span><span class="sc">{</span><span class="bu">round</span>(scores[<span class="dv">1</span>], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-457"><a href="#cb3-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-458"><a href="#cb3-458" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _has_scores_converged(<span class="va">self</span>, round_num: <span class="bu">int</span>, memory: Memory) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb3-459"><a href="#cb3-459" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Checks if debate scores have converged by comparing last two rounds"""</span></span>
<span id="cb3-460"><a href="#cb3-460" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> round_num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb3-461"><a href="#cb3-461" aria-hidden="true" tabindex="-1"></a>            prev_diff <span class="op">=</span> memory.scores[<span class="op">-</span><span class="dv">2</span>][<span class="dv">0</span>] <span class="op">-</span> memory.scores[<span class="op">-</span><span class="dv">2</span>][<span class="dv">1</span>]</span>
<span id="cb3-462"><a href="#cb3-462" aria-hidden="true" tabindex="-1"></a>            curr_diff <span class="op">=</span> memory.scores[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="op">-</span> memory.scores[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>]</span>
<span id="cb3-463"><a href="#cb3-463" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (prev_diff <span class="op">*</span> curr_diff) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb3-464"><a href="#cb3-464" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb3-465"><a href="#cb3-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-466"><a href="#cb3-466" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _prepare_results(<span class="va">self</span>, memory: Memory) <span class="op">-&gt;</span> Dict:</span>
<span id="cb3-467"><a href="#cb3-467" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Prepare the final results dictionary."""</span></span>
<span id="cb3-468"><a href="#cb3-468" aria-hidden="true" tabindex="-1"></a>        avg_scores <span class="op">=</span> [</span>
<span id="cb3-469"><a href="#cb3-469" aria-hidden="true" tabindex="-1"></a>            <span class="bu">round</span>(<span class="bu">sum</span>(scores[i] <span class="cf">for</span> scores <span class="kw">in</span> memory.scores) <span class="op">/</span> <span class="bu">len</span>(memory.scores), <span class="dv">2</span>)</span>
<span id="cb3-470"><a href="#cb3-470" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb3-471"><a href="#cb3-471" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-472"><a href="#cb3-472" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-473"><a href="#cb3-473" aria-hidden="true" tabindex="-1"></a>        winner <span class="op">=</span> (</span>
<span id="cb3-474"><a href="#cb3-474" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&gt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb3-475"><a href="#cb3-475" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&lt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb3-476"><a href="#cb3-476" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'tie'</span></span>
<span id="cb3-477"><a href="#cb3-477" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-478"><a href="#cb3-478" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-479"><a href="#cb3-479" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb3-480"><a href="#cb3-480" aria-hidden="true" tabindex="-1"></a>            <span class="st">"winner"</span>: winner,</span>
<span id="cb3-481"><a href="#cb3-481" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_scores"</span>: avg_scores,</span>
<span id="cb3-482"><a href="#cb3-482" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rounds"</span>: <span class="bu">len</span>(memory.scores),</span>
<span id="cb3-483"><a href="#cb3-483" aria-hidden="true" tabindex="-1"></a>            <span class="st">"score_history"</span>: [[<span class="bu">round</span>(s[<span class="dv">0</span>], <span class="dv">2</span>), <span class="bu">round</span>(s[<span class="dv">1</span>], <span class="dv">2</span>)] <span class="cf">for</span> s <span class="kw">in</span> memory.scores],</span>
<span id="cb3-484"><a href="#cb3-484" aria-hidden="true" tabindex="-1"></a>            <span class="st">"argument_history"</span>: memory.arguments,</span>
<span id="cb3-485"><a href="#cb3-485" aria-hidden="true" tabindex="-1"></a>            <span class="st">"feedback_history"</span>: memory.feedback</span>
<span id="cb3-486"><a href="#cb3-486" aria-hidden="true" tabindex="-1"></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="load-the-mt-bench-dataset" class="level1">
<h1>Load the MT-bench dataset</h1>
<p>Next I will read in the MT-bench dataset from disk and prepare it for evaluation. I will use <a href="https://llamahub.ai/l/llama_datasets/MT%20Bench%20Human%20Judgement%20Dataset?from=">MtBenchHumanJudgementDataset</a> from Llamahub.</p>
<div id="2db6577c" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Commented out since the dataset is already downloaded</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#!llamaindex-cli download-llamadataset MtBenchHumanJudgementDataset --download-dir ./data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, I will load the dataset into a pandas dataframe and take a random sample of 300 rows.</p>
<div id="79487af7" class="cell" data-code-fold-show="false" data-execution_count="3">
<details class="code-fold">
<summary>Click to view the code that loads the dataset</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.llama_dataset <span class="im">import</span> LabelledPairwiseEvaluatorDataset</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> LabelledPairwiseEvaluatorDataset.from_json(</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"./data/pairwise_evaluator_dataset.json"</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>).to_pandas()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[[<span class="st">'query'</span>, <span class="st">'answer'</span>, <span class="st">'second_answer'</span>, <span class="st">'answer_by'</span>, <span class="st">'second_answer_by'</span>, <span class="st">'reference_score'</span>]]</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename as follows: query =&gt; question, answer =&gt; model_a_answer, second_answer =&gt; model_b_answer, answer_by =&gt; model_a, second_answer_by =&gt; model_b, reference_score =&gt; human_winner</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>df.rename(columns<span class="op">=</span>{<span class="st">'query'</span>: <span class="st">'question'</span>, <span class="st">'answer'</span>: <span class="st">'model_a_answer'</span>, <span class="st">'second_answer'</span>: <span class="st">'model_b_answer'</span>, <span class="st">'answer_by'</span>: <span class="st">'model_a'</span>, <span class="st">'second_answer_by'</span>: <span class="st">'model_b'</span>, <span class="st">'reference_score'</span>: <span class="st">'human_winner'</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Reencode human winner as "model_a" if 1, "model_b" if 0, and "tie" if 0.5</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'human_winner'</span>] <span class="op">=</span> df[<span class="st">'human_winner'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">'model_a'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'tie'</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a random sample of 300 rows</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.sample(n<span class="op">=</span><span class="dv">300</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>df.head()</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Take first 180 rows</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.iloc[:<span class="dv">180</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="use-methods-to-evaluate-mt-bench-dataset" class="level1">
<h1>Use methods to evaluate MT-bench dataset</h1>
<p>Using the MT-bench dataset, I will run the three LLM models (Baseline-Weak, Baseline-Strong, and SAMRE) on each set of question and answers.</p>
<p>The code below is the main evaluation loop, designed to run multiple evaluations asynchronously (to save time). It will evaluate each item in the dataset, and save the results to disk as a checkpoint. If the evaluation is interrupted, the code can be resumed from the last checkpoint.</p>
<div id="ee415dac" class="cell" data-code-fold-show="false" data-execution_count="4">
<details class="code-fold">
<summary>Click to view the code that runs the evaluations</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> asyncio</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> asyncio <span class="im">import</span> Semaphore</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> hashlib</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.WARNING)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> evaluate_conversation_pair(row, evaluators, semaphore, idx, total):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate a single conversation pair with all evaluators"""</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="cf">with</span> semaphore:</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add delay between API calls</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">#await asyncio.sleep(1)  # Add small delay between conversations</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate pair_id from conversation hash</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        pair_id <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>row[<span class="st">'model_a'</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>row[<span class="st">'model_b'</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>hashlib<span class="sc">.</span>sha256(<span class="bu">str</span>(row[<span class="st">'question'</span>]).encode())<span class="sc">.</span>hexdigest()[:<span class="dv">12</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        checkpoint_file <span class="op">=</span> <span class="ss">f'checkpoints/</span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">.json'</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return existing checkpoint if available</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> os.path.exists(checkpoint_file):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>            logging.info(<span class="ss">f"Found existing checkpoint file for </span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> json.load(<span class="bu">open</span>(checkpoint_file))</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        logging.info(<span class="ss">f"No checkpoint file found for </span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> {</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span>: row[<span class="st">'model_a'</span>],</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_b'</span>: row[<span class="st">'model_b'</span>],</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">'human_winner'</span>: row[<span class="st">'human_winner'</span>],</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">'pair_id'</span>: pair_id</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># First run SAMRE evaluation with retries</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):  <span class="co"># Try up to 3 times</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>                    samre_evaluator <span class="op">=</span> evaluators[<span class="st">'samre'</span>]</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>                    samre_result <span class="op">=</span> <span class="cf">await</span> samre_evaluator.evaluate(</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>], </span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>], </span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>]</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'samre_winner'</span>] <span class="op">=</span> samre_result[<span class="st">'winner'</span>]</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'samre_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: samre_result[k] <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>                    result.update({</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'samre_argument_history'</span>: samre_result[<span class="st">'argument_history'</span>],</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'samre_feedback_history'</span>: samre_result[<span class="st">'feedback_history'</span>]</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>                    })</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span>  <span class="co"># If successful, break retry loop</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span>  <span class="co"># Exponential backoff</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on SAMRE, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:  <span class="co"># Last attempt failed</span></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span>  <span class="co"># Re-raise non-rate-limit errors</span></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> asyncio.sleep(<span class="fl">0.5</span>)  <span class="co"># Add small delay between evaluator calls</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run baseline strong with same number of rounds as SAMRE</span></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>                    baseline_strong_evaluator <span class="op">=</span> evaluators[<span class="st">'baseline_strong'</span>]</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>                    baseline_strong_result <span class="op">=</span> <span class="cf">await</span> baseline_strong_evaluator.evaluate(</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>],</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>],</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>],</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>                        num_rounds<span class="op">=</span>result[<span class="st">'samre_rounds'</span>]</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_strong_winner'</span>] <span class="op">=</span> baseline_strong_result[<span class="st">'winner'</span>]</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'baseline_strong_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: baseline_strong_result[k] </span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>                                 <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_strong_full_response'</span>] <span class="op">=</span> baseline_strong_result[<span class="st">'full_response'</span>]</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span></span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on baseline strong, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> asyncio.sleep(<span class="fl">0.5</span>)  <span class="co"># Add small delay between evaluator calls</span></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run baseline weak with 1 round</span></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>                    baseline_weak_evaluator <span class="op">=</span> evaluators[<span class="st">'baseline_weak'</span>]</span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>                    baseline_weak_result <span class="op">=</span> <span class="cf">await</span> baseline_weak_evaluator.evaluate(</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>],</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>],</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>],</span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>                        num_rounds<span class="op">=</span><span class="dv">1</span></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_weak_winner'</span>] <span class="op">=</span> baseline_weak_result[<span class="st">'winner'</span>]</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'baseline_weak_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: baseline_weak_result[k] </span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>                                 <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_weak_full_response'</span>] <span class="op">=</span> baseline_weak_result[<span class="st">'full_response'</span>]</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on baseline weak, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span></span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Error evaluating row </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'samre_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'baseline_strong_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'baseline_weak_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'error'</span>] <span class="op">=</span> <span class="bu">str</span>(e)</span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save checkpoint after each evaluation</span></span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a>        os.makedirs(<span class="st">'checkpoints'</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a>        json.dump(result, <span class="bu">open</span>(checkpoint_file, <span class="st">'w'</span>))</span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (idx <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Processed </span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>total<span class="sc">}</span><span class="ss"> conversations"</span>)</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result</span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> evaluate_conversations_async(df, evaluators, semaphore_limit<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate conversations asynchronously"""</span></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reduce semaphore limit</span></span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a>    semaphore_limit <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Process one at a time to avoid rate limits</span></span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process in smaller batches</span></span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(df), batch_size):</span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> df.iloc[i:i<span class="op">+</span>batch_size]</span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a>        tasks <span class="op">=</span> [</span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>            evaluate_conversation_pair(row[<span class="dv">1</span>], evaluators, Semaphore(semaphore_limit), idx, <span class="bu">len</span>(df))</span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, row <span class="kw">in</span> <span class="bu">enumerate</span>(batch.iterrows(), start<span class="op">=</span>i)</span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a>        batch_results <span class="op">=</span> <span class="cf">await</span> asyncio.gather(<span class="op">*</span>tasks)</span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a>        results.extend(batch_results)</span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add delay between batches</span></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">+</span> batch_size <span class="op">&lt;</span> <span class="bu">len</span>(df):</span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Completed batch </span><span class="sc">{</span>i<span class="op">//</span>batch_size <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, waiting before next batch..."</span>)</span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a>            <span class="co">#await asyncio.sleep(5)  # 5 second delay between batches</span></span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(results)</span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> main():</span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="cf">with</span> ModelEvaluator.create(mode<span class="op">=</span><span class="st">"samre"</span>) <span class="im">as</span> samre_evaluator, <span class="op">\</span></span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a>               ModelEvaluator.create(mode<span class="op">=</span><span class="st">"baseline_strong"</span>) <span class="im">as</span> baseline_strong_evaluator, <span class="op">\</span></span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a>               ModelEvaluator.create(mode<span class="op">=</span><span class="st">"baseline_weak"</span>) <span class="im">as</span> baseline_weak_evaluator:</span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="cf">await</span> evaluate_conversations_async(</span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a>            df,</span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a>                <span class="st">'samre'</span>: samre_evaluator, </span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a>                <span class="st">'baseline_strong'</span>: baseline_strong_evaluator,</span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a>                <span class="st">'baseline_weak'</span>: baseline_weak_evaluator</span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a>            },</span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a>            semaphore_limit<span class="op">=</span><span class="dv">1</span></span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Run evaluation with checkpoint recovery</span></span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a>    eval_df <span class="op">=</span> <span class="cf">await</span> main()</span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Error during evaluation: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ch">\n</span><span class="ss">Recovering from checkpoints..."</span>)</span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a>    eval_df <span class="op">=</span> pd.DataFrame([json.load(<span class="bu">open</span>(<span class="ss">f'checkpoints/</span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">'</span>)) </span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">for</span> f <span class="kw">in</span> os.listdir(<span class="st">'checkpoints'</span>) </span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">if</span> f.endswith(<span class="st">'.json'</span>)])</span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a><span class="cf">finally</span>:</span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a>    eval_df.to_csv(<span class="st">'eval_df.csv'</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a>    eval_df.head()</span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop rows with any null values on the model winner columns</span></span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a>eval_df <span class="op">=</span> eval_df.dropna(subset<span class="op">=</span>[<span class="st">'baseline_strong_winner'</span>, <span class="st">'baseline_weak_winner'</span>, <span class="st">'samre_winner'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Completed batch 1, waiting before next batch...
Completed batch 2, waiting before next batch...
Completed batch 3, waiting before next batch...
Completed batch 4, waiting before next batch...
Completed batch 5, waiting before next batch...
Completed batch 6, waiting before next batch...
Completed batch 7, waiting before next batch...
Completed batch 8, waiting before next batch...
Completed batch 9, waiting before next batch...
Completed batch 10, waiting before next batch...
Completed batch 11, waiting before next batch...
Completed batch 12, waiting before next batch...
Completed batch 13, waiting before next batch...
Completed batch 14, waiting before next batch...
Completed batch 15, waiting before next batch...
Completed batch 16, waiting before next batch...
Completed batch 17, waiting before next batch...</code></pre>
</div>
</div>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>Now that the evaluation is complete, I will evaluate the performance of each of the three methods by looking at how well each method agreed with the human judgments. I’ll use Krippendorff’s alpha to measure agreement, since it is a robust measure of agreement that can handle non-binary ratings (among other things).</p>
<div id="1987f5ed" class="cell" data-code-fold-show="false" data-execution_count="5">
<details class="code-fold">
<summary>Click to view the code that calculates agreement</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> krippendorff <span class="im">import</span> alpha</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_agreement(df, rater1_col, rater2_col):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate Krippendorff's alpha between two raters.</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">        df: DataFrame containing the ratings</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">        rater1_col: Name of first rater's column</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">        rater2_col: Name of second rater's column</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co">        float: Krippendorff's alpha score</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create label encoder</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    le <span class="op">=</span> LabelEncoder()</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine all unique values from both columns</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    all_values <span class="op">=</span> pd.concat([df[rater1_col], df[rater2_col]]).unique()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    le.fit(all_values)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Transform the ratings to numeric values</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    ratings1 <span class="op">=</span> le.transform(df[rater1_col].fillna(<span class="st">'missing'</span>))</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    ratings2 <span class="op">=</span> le.transform(df[rater2_col].fillna(<span class="st">'missing'</span>))</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape data for krippendorff alpha calculation</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each row represents one item, each column represents one rater</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    reliability_data <span class="op">=</span> np.vstack([ratings1, ratings2])</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha(reliability_data<span class="op">=</span>reliability_data, level_of_measurement<span class="op">=</span><span class="st">'nominal'</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate agreement scores for all methods</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>human_baseline_strong_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_strong_winner'</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>human_baseline_weak_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_weak_winner'</span>)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>human_samre_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'samre_winner'</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame with the agreement scores</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>agreement_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Evaluator Pair'</span>: [<span class="st">'Human vs. Baseline-Strong'</span>, <span class="st">'Human vs. Baseline-Weak'</span>, <span class="st">'Human vs. SAMRE'</span>],</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Krippendorff Alpha'</span>: [human_baseline_strong_agreement, human_baseline_weak_agreement, human_samre_agreement]</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Round the scores to 3 decimal places</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>agreement_df[<span class="st">'Krippendorff Alpha'</span>] <span class="op">=</span> agreement_df[<span class="st">'Krippendorff Alpha'</span>].<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percent difference between Baseline-Strong and Baseline-Weak, and SAMRE and Baseline-Strong</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_diff <span class="op">=</span> (human_baseline_strong_agreement <span class="op">-</span> human_baseline_weak_agreement) <span class="op">/</span> human_baseline_strong_agreement</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_diff <span class="op">=</span> (human_baseline_strong_agreement <span class="op">-</span> human_samre_agreement) <span class="op">/</span> human_baseline_strong_agreement</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_diff <span class="op">=</span> (human_samre_agreement <span class="op">-</span> human_baseline_weak_agreement) <span class="op">/</span> human_samre_agreement</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(agreement_df)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the percent difference</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Krippendorff Alpha Improvements:"</span>)</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>              Evaluator Pair  Krippendorff Alpha
0  Human vs. Baseline-Strong               0.391
1    Human vs. Baseline-Weak               0.252
2            Human vs. SAMRE               0.307

Krippendorff Alpha Improvements:
SAMRE vs. Baseline-Weak: 18%
Baseline-Strong vs. Baseline-Weak: 36%
Baseline-Strong vs. SAMRE: 21%</code></pre>
</div>
</div>
<p>Although none of the methods yielded particularly strong agreement with the human judges, we can observe a few things: 1. As reported in the paper, SAMRE yielded significantly better agreement than Baseline-Weak (0.307 vs.&nbsp;0.252, an increase of ~18%). 2. Baseline-Strong yielded significantly better agreement than Baseline-Weak (0.391 vs.&nbsp;0.252, an increase of ~36%). 3. Importantly, Baseline-Strong also yielded significantly better agreement than SAMRE (0.391 vs.&nbsp;0.252, an increase of ~21%)!</p>
<p>We can also measure performance in terms of binary classification accuracy, using Matthews Correlation Coefficient (MCC) as the metric, while re-encoding the “winner” columns to indicate whether model_a was selected as better (1) or not better (0) in each case.</p>
<div id="aabdaa1a" class="cell" data-code-fold-show="false" data-execution_count="6">
<details class="code-fold">
<summary>Click to view the code that calculates Matthews Correlation Coefficient (MCC)</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode winner as binary</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_winner_as_binary(winner):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> winner <span class="op">==</span> <span class="st">'model_a'</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create binary columns for each evaluator</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'human_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'human_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'baseline_strong_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'baseline_strong_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'baseline_weak_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'baseline_weak_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'samre_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'samre_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> matthews_corrcoef</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate MCC for each method</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>metrics_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Method'</span>: [<span class="st">'Baseline-Strong'</span>, <span class="st">'Baseline-Weak'</span>, <span class="st">'SAMRE'</span>],</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'MCC'</span>: [</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'baseline_strong_model_a_better'</span>]</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'baseline_weak_model_a_better'</span>]</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'samre_model_a_better'</span>]</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Round the scores to 3 decimal places</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>metrics_df[<span class="st">'MCC'</span>] <span class="op">=</span> metrics_df[<span class="st">'MCC'</span>].<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percent differences</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_percent_diff(new, old):</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (new <span class="op">-</span> old) <span class="op">/</span> old <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="co"># MCC differences</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(metrics_df)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">MCC Improvements:"</span>)</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Method    MCC
0  Baseline-Strong  0.464
1    Baseline-Weak  0.331
2            SAMRE  0.331

MCC Improvements:
SAMRE vs. Baseline-Weak: 0%
Baseline-Strong vs. Baseline-Weak: 40%
Baseline-Strong vs. SAMRE: 40%</code></pre>
</div>
</div>
<p>Looking at MCC values, we observe the following:</p>
<ol type="1">
<li>SAMRE did not perform better than Baseline-Weak (MCCs = 0.331 in both cases).</li>
<li>Baseline-Strong performed better than Baseline-Weak (0.464 vs.&nbsp;0.331, an increase of ~40%).</li>
<li>Baseline-Strong did not perform better than SAMRE (0.464 vs.&nbsp;0.331, an increase of ~40%).</li>
</ol>
<p>Why does this metric disagree with the Krippendorff alpha results on the SAMRE vs.&nbsp;Baseline-Weak comparison? I would guess this is due to how ties were resolved when encoding the winner as binary. Also note that the same MCC values for SAMRE and Baseline-Weak is not an error. We can see that the confusion matrices were different.</p>
<div id="bbbf7ea6" class="cell" data-code-fold-show="false" data-execution_count="7">
<details class="code-fold">
<summary>Click to view the code that generates confusion matrices</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Baseline-Weak Confusion Matrix:"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    eval_df[<span class="st">'baseline_weak_model_a_better'</span>]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">SAMRE Confusion Matrix:"</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    eval_df[<span class="st">'samre_model_a_better'</span>]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Baseline-Weak Confusion Matrix:
[[72 42]
 [16 41]]

SAMRE Confusion Matrix:
[[70 44]
 [15 42]]</code></pre>
</div>
</div>
<p>Thus, across both of these measures of performance, we see that SAMRE did not perform better than a baseline that is designed with best practices.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, I have shown that SAMRE does not perform better than a well-engineered baseline method. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tylerburleigh\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="tylerburleigh/tylerburleigh.github.io" data-repo-id="R_kgDOKMo8ww" data-category="Blog comments" data-category-id="DIC_kwDOIg6EJc4CSz92" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "A test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2025-01-12</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "In this post, I re-evaluate a method that was recently published in arXiv, critiquing the baseline model used in the paper and then implementing a new baseline model that implements standard best practices and similar multi-round aggregation. I find that the SAMRE method does not perform better than the new baseline model. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as the being skeptical of claims in research papers that compare new methods to a baseline."</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - prompt-engineering</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - python</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - LLM-as-judge</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - LLM-evals</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="an">freeze:</span><span class="co"> true</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>I've been doing a lot of work with LLM-based evaluations lately, and I've been thinking about how to improve the quality of these evaluations.</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>I like to read research papers from arXiv for inspiration, and I recently came across a paper called <span class="co">[</span><span class="ot">Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates</span><span class="co">](https://arxiv.org/abs/2410.04663)</span>, which introduces a new method inspired by judicial process called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process.</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>I was initially impressed by the results, as they reported a gain of 6-8% over the baseline method(!!). However, I am often skeptical of comparisons to "baseline" models in these research papers, as I find that they often fail to implement standard best practices and are therefore not represenative of true gains over baseline.</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>Given this skepticism of mine, I decided that it might be interesting to put it to the test: What if I implemented the SAMRE method, and compared it to a baseline model that does implement standard best practices for prompt engineering? Would I find that the SAMRE method is indeed an improvement over the baseline? Or would I find that SAMRE is inferior to a properly implemented baseline?</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>Using a sample of 180 conversations from MT-bench for testing and evaluation, I evaluated three methods:</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>SAMRE, as implemented in the paper</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>After running the evaluations and calculating Krippendorff alpha agreement with human judges, I found that although SAMRE did yield better agreement than Baseline-Weak (18% improvement), it was inferior to Baseline-Strong (which was 36% better than SAMRE!). A similar result was found when examining binary classification accuracy using Matthews Correlation Coefficient (MCC).</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>These results serve to highlight the importance of implementing standard best practices in baseline models, as well as being skeptical of claims in research papers that compare new methods to a "baseline model". Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="fu"># Baseline model prompt inadequacies</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>First, let's consider some of the inadequacies in the Baseline model's prompt reported in the paper. The prompt they used was as follows:</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="in">```prompt</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="in">You are a fair, impartial judge scoring a debate on the following question:</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="in">question.</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 1: answer_1</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 2: answer_2</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="in">Score each answer on a scale of 1-20 for each of the following criteria:</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="in">1. Relevance to the question</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="in">2. Accuracy of information and use of credible sources</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="in">3. Depth of analysis and completeness of argument</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="in">4. Clarity of expression and logical flow</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="in">5. Strength of reasoning and factual support</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="in">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="in">Provide scores as [answer_1_score, answer_2_score] for each criterion in a list format, then sum for final scores. Please keep an eye on the slightest difference that should make a difference in the scoring. Don’t overthink!</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="in">Relevance:</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a><span class="in">Accuracy:</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a><span class="in">Depth:</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="in">Clarity:</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a><span class="in">Logic and Factuality:</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="in">Addressing opponent’s points:</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a><span class="in">Final Scores (sum of above) as a tuple (example: (18, 9)):</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a><span class="in">Explain your scoring, focusing on why one answer is better than the other based on the criteria above. Keep your explanation concise but informative.</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a><span class="in">Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a><span class="in">Example: (18, 9)</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="in">Your scores and explanation:</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>Here are the issues I see with this prompt:</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The prompt does not use delimiters for most of the inputs. I would enclose the inputs inside XML tags like &lt;Question&gt;&lt;/Question&gt;, &lt;Answer1&gt;&lt;/Answer1&gt;, and &lt;Answer2&gt;&lt;/Answer2&gt;, but in a pinch delimiters like triple backticks can be used.</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The prompt instructs the model to first generate scores in list format, and then to sum them. But as we know, language models models often make arithmetic mistakes. It would be better to ask the model to generate scores for each criterion, and then to programmatically extract and summarize them in python (or another programming language) from which the routine is run.</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Although the prompt asks the model to "explain your scoring", it is not clear if the model should be reasoning about each criterion before it scores them, or if it should provide reasoning at the end when giving its final score. I would ask the model to provide reasoning for each criterion that it is asked to score, and ask it to reason before scoring.</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>It's unclear why a scale of 1-20 is used. This is not a standard scale for scoring. I would use a scale of 1-10 which is likely more familiar to the model and can be expected to be used more consistently.</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Although the prompt does suggest that the model provide its scores in tuple format, it would be better to provide more explicit format instructions.</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>The prompt includes an "Effectiveness in addressing opponent's points" criterion, but this is almost certainly irrelevant given that the answers to the question were not generated with the goal of addressing an opponent.</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Finally, although this goes beyond the prompt itself, the authors of the paper are comparing a multi-round method to a single-round method. This is obviously an unfair comparison. Instead, it would be better to compare the SAMRE method to a baseline that uses the same number of rounds and then similarly averages its scores.</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>With all of that in mind, here's how I would rewrite the prompt:</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a><span class="in">```prompt</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a><span class="in">You are a fair, impartial judge scoring a debate on Question.</span></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Question&gt;</span></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a><span class="in">{question}</span></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Question&gt;</span></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a><span class="in">Two Answers have been given to the Question.</span></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer1&gt;</span></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="in">{answer_1}</span></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Answer1&gt;</span></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer2&gt;</span></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a><span class="in">{answer_2}</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Answer2&gt;</span></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a><span class="in">The Answers are being judged on the following Criteria:</span></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criteria&gt;</span></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;</span></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;</span></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;</span></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;</span></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;</span></span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Criteria&gt;</span></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a><span class="in">For each Criterion, briefly analyze the performance of </span></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a><span class="in">the two Answers, then give a score between 1 and 10.</span></span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a><span class="in">Respond as follows:</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion1&gt;</span></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;</span></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Analysis&gt;</span></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Analysis&gt;</span></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Scores&gt;</span></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Scores&gt;</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Criterion1&gt;</span></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Criterion2&gt;</span></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;</span></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Analysis&gt;</span></span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a><span class="in">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Analysis&gt;</span></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Scores&gt;</span></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Scores&gt;</span></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/Criterion2&gt;</span></span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a><span class="in">...</span></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a>Notice that the prompt now uses XML tags to structure the instructions, that it asks the model to provide reasoning for each criterion before scoring, and that it gives the model a clear format for its response that reinforces analysis before scoring for each criterion.</span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a>I've also changed the scale from 1-20 to 1-10, removed the unnecessary "Effectiveness in addressing opponent's points" criterion, and removed the instruction to summarize the scores, as I would handle this within the code.</span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a><span class="fu"># Hypothesis and predictions</span></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a>I hypothesize that SAMRE will not perform better than a baseline model that implements standard best practices for prompt engineering.</span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a>My predictions are as follows:</span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>SAMRE will perform better than Baseline-Weak.</span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Strong will perform better than Baseline-Weak.</span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Baseline-Strong will perform equal to or better than SAMRE.</span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a><span class="fu"># My implementation of SAMRE and Baseline</span></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a>Okay, so with those criticisms out of the way, let's design evaluators to implement three methods:</span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>SAMRE</span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Weak: The baseline model used in the paper that does not implement standard best practices for prompt engineering.</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Baseline-Strong: A baseline model that does implement standard best practices for prompt engineering.</span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>Below is my python implementation of the evaluators. To the best of my ability, I have implemented the SAMRE and Baseline methods as described in the paper (I call the paper's Baseline method "Baseline-Weak"). And I have implemented a Baseline-Strong evaluator that implements what I would consider to be standard best practices for prompt engineering.</span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a>This will allow me to test my hypothesis and predictions.</span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the implementation code"</span></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass, field</span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple, Dict</span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> AsyncOpenAI</span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> asynccontextmanager</span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the prompts</span></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a>PROMPTS <span class="op">=</span> {</span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for advocates to defend their assigned answer, used in SAMRE</span></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a><span class="st">"defend_prompt"</span>: <span class="st">"""</span></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a><span class="st">You’re an eloquent advocate (Advocate </span><span class="sc">{advocate_id}</span><span class="st">) in a group defending this answer:</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer}</span><span class="st"> to the question: </span><span class="sc">{question}</span><span class="st">. Your opponent group defends: </span><span class="sc">{opponent_answer}</span><span class="st">.</span></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a><span class="st">Collaborate with your fellow advocates to convince the audience your argument is</span></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a><span class="st">better. Use the latest feedback, your opponent’s last argument, and your team’s previous</span></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a><span class="st">arguments to improve your case.</span></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a><span class="st">Latest feedback: </span><span class="sc">{feedback}</span></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a><span class="st">Opponent’s last argument: </span><span class="sc">{opponent_argument}</span></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a><span class="st">Your team’s previous arguments: team_arguments</span></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a><span class="st">Respond in under 80 words.</span></span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a><span class="st">Your defense:</span></span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for judge to provide feedback on debate progress, used in SAMRE</span></span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a><span class="st">"judge_prompt"</span>: <span class="st">"""</span></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a><span class="st">You’re a fair, impartial judge in a debate on: "</span><span class="sc">{question}</span><span class="st">". Answer 1: "</span><span class="sc">{answer_1}</span><span class="st">".</span></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: "</span><span class="sc">{answer_2}</span><span class="st">". Your goal is to provide feedback that will help advocate groups</span></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a><span class="st">improve and differentiate their arguments more clearly.</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a><span class="st">Current round: </span><span class="sc">{current_round}</span></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a><span class="st">Total rounds: </span><span class="sc">{total_rounds}</span></span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a><span class="st">Previous scores: </span><span class="sc">{previous_scores}</span></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 1st answer: </span><span class="sc">{defense_1}</span></span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 2nd answer: </span><span class="sc">{defense_2}</span></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a><span class="st">Provide specific, constructive feedback to help each advocate group strengthen their</span></span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a><span class="st">unique position. Encourage them to address weaknesses and highlight distinctions. Aim</span></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a><span class="st">for your feedback to lead to more divergent scores in future rounds.</span></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a><span class="st">Give your feedback in under 50 words:</span></span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for SAMRE method scoring</span></span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_samre"</span>: <span class="st">"""</span></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a><span class="st">You’re a critical, impartial judge in a high-stakes debate on: "</span><span class="sc">{question}</span><span class="st">". Answer</span></span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a><span class="st">1: "</span><span class="sc">{answer_1}</span><span class="st">". Answer 2: "</span><span class="sc">{answer_2}</span><span class="st">". Your goal is to provide detailed, constructive</span></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a><span class="st">feedback that will push advocates to significantly improve their arguments.</span></span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a><span class="st">Total rounds: </span><span class="sc">{total_rounds}</span></span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a><span class="st">Previous scores: </span><span class="sc">{previous_scores}</span></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 1st answer: </span><span class="sc">{defense_1}</span></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a><span class="st">Defense for 2nd answer: </span><span class="sc">{defense_2}</span></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a><span class="st">Analyze each argument meticulously. Be thorough and unbiased in your assessment of:</span></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a><span class="st">1. Relevance to the question</span></span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a><span class="st">2. Accuracy of information and use of credible sources</span></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a><span class="st">3. Depth of analysis and completeness of argument</span></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a><span class="st">4. Clarity of expression and logical flow</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a><span class="st">5. Strength of reasoning and factual support</span></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a><span class="st">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a><span class="st">For each criterion, provide a score on a scale of 1-20 and detailed justification.</span></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a><span class="st">Scores should be given as [answer_1_score, answer_2_score] for each criterion.</span></span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a><span class="st">Your comprehensive feedback for each advocate (50 words each):</span></span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a><span class="st">Feedback for Advocate 1:</span></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a><span class="st">Feedback for Advocate 2:</span></span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a><span class="st">Sum up the scores and return the final score tuple (score1, score2). Example: (95, 87)</span></span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a><span class="st">Your detailed scores and final tally:</span></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for Baseline-Weak method scoring, which represents the baseline model used in the paper</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_baseline_weak"</span>: <span class="st">"""</span></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a><span class="st">You are a fair, impartial judge scoring a debate on the following question:</span></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a><span class="st">question.</span></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: </span><span class="sc">{answer_1}</span></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: </span><span class="sc">{answer_2}</span></span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a><span class="st">Score each answer on a scale of 1-20 for each of the following criteria:</span></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a><span class="st">1. Relevance to the question</span></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a><span class="st">2. Accuracy of information and use of credible sources</span></span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a><span class="st">3. Depth of analysis and completeness of argument</span></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a><span class="st">4. Clarity of expression and logical flow</span></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a><span class="st">5. Strength of reasoning and factual support</span></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a><span class="st">6. Effectiveness in addressing opponent’s points</span></span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a><span class="st">Provide scores as [Answer1_score, Answer2_score] for each criterion in a list format,</span></span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a><span class="st">then sum for final scores. Please keep an eye on the slightest difference that should</span></span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a><span class="st">make a difference in the scoring. Don’t overthink!</span></span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a><span class="st">Relevance:</span></span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a><span class="st">Accuracy:</span></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a><span class="st">Depth:</span></span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a><span class="st">Clarity:</span></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a><span class="st">Logic and Factuality:</span></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a><span class="st">Addressing opponent’s points:</span></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a><span class="st">Final Scores (sum of above) as a tuple (example: (18, 9)):</span></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a><span class="st">Explain your scoring, focusing on why one answer is better than the other based on the</span></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a><span class="st">criteria above. Keep your explanation concise but informative.</span></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a><span class="st">Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).</span></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a><span class="st">Example: (18, 9)</span></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a><span class="st">Your scores and explanation:</span></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>,</span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a><span class="co"># Prompt for Baseline-Strong method scoring, which implements what I consider to be standard best practices for prompt engineering</span></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a><span class="st">"score_prompt_baseline_strong"</span>: <span class="st">"""</span></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a><span class="st">You are a fair, impartial judge scoring a debate on Question.</span></span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Question&gt;</span></span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a><span class="sc">{question}</span></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Question&gt;</span></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a><span class="st">Two Answers have been given to the Question.</span></span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1&gt;</span></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_1}</span></span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Answer1&gt;</span></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2&gt;</span></span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_2}</span></span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Answer2&gt;</span></span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a><span class="st">The Answers are being judged on the following Criteria:</span></span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criteria&gt;</span></span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;</span></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;</span></span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;</span></span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;</span></span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;</span></span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criteria&gt;</span></span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a><span class="st">For each Criterion, briefly analyze the performance of </span></span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a><span class="st">the two Answers, then give a score between 1 and 10.</span></span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a><span class="st">Respond as follows:</span></span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion1&gt;</span></span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;</span></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Analysis&gt;</span></span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Analysis&gt;</span></span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Scores&gt;</span></span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Scores&gt;</span></span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criterion1&gt;</span></span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Criterion2&gt;</span></span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;</span></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Analysis&gt;</span></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 1: [Analysis of Answer 1 performance on the Criterion]</span></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a><span class="st">Answer 2: [Analysis of Answer 2 performance on the Criterion]</span></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Analysis&gt;</span></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Scores&gt;</span></span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;</span></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Scores&gt;</span></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/Criterion2&gt;</span></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a><span class="st">...</span></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Memory:</span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stores debate history including arguments, scores, and feedback for each round, used in SAMRE"""</span></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a>    arguments: List[Tuple[<span class="bu">str</span>, <span class="bu">str</span>]] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a>    scores: List[Tuple[<span class="bu">float</span>, <span class="bu">float</span>]] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a>    feedback: List[<span class="bu">str</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="bu">list</span>)</span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelEvaluator:</span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a>    <span class="at">@classmethod</span></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a>    <span class="at">@asynccontextmanager</span></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> create(cls, mode<span class="op">=</span><span class="st">"samre"</span>, model<span class="op">=</span><span class="st">"gpt-4o-mini"</span>, logging_level<span class="op">=</span>logging.WARNING):</span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Factory method to create evaluator instance with proper async context management"""</span></span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a>        instance <span class="op">=</span> cls(mode<span class="op">=</span>mode, model<span class="op">=</span>model, logging_level<span class="op">=</span>logging_level)</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a>        instance.client <span class="op">=</span> AsyncOpenAI()</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> instance</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a>        <span class="cf">finally</span>:</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> instance.client.close()</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _setup_logger(<span class="va">self</span>, logging_level):</span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Setup logger with word wrapping."""</span></span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a>        logger <span class="op">=</span> logging.getLogger(<span class="va">__name__</span>)</span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a>        logger.setLevel(logging_level)</span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> logger.handlers:</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a>            handler <span class="op">=</span> logging.StreamHandler()</span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a>            <span class="kw">class</span> WrapFormatter(logging.Formatter):</span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a>                <span class="kw">def</span> <span class="bu">format</span>(<span class="va">self</span>, record):</span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a>                    <span class="im">import</span> textwrap</span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a>                    message <span class="op">=</span> <span class="bu">super</span>().<span class="bu">format</span>(record)</span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(textwrap.fill(line, width<span class="op">=</span><span class="dv">80</span>) </span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a>                                <span class="cf">for</span> line <span class="kw">in</span> message.split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>))</span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a>            formatter <span class="op">=</span> WrapFormatter(<span class="st">'</span><span class="sc">%(message)s</span><span class="st">'</span>)</span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a>            handler.setFormatter(formatter)</span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a>            logger.addHandler(handler)</span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logger</span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, mode<span class="op">=</span><span class="st">"samre"</span>, model<span class="op">=</span><span class="st">"gpt-4o-mini"</span>, logging_level<span class="op">=</span>logging.WARNING):</span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mode <span class="op">=</span> mode</span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Modify to handle both baseline modes</span></span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_rounds <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> mode.startswith(<span class="st">"baseline"</span>) <span class="cf">else</span> <span class="dv">4</span></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger <span class="op">=</span> <span class="va">self</span>._setup_logger(logging_level)</span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize all prompts</span></span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.defend_prompt <span class="op">=</span> PROMPTS[<span class="st">"defend_prompt"</span>]</span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.judge_prompt <span class="op">=</span> PROMPTS[<span class="st">"judge_prompt"</span>]</span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> get_completion(<span class="va">self</span>, prompt: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get a completion from the OpenAI API."""</span></span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.client:</span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'"</span>)</span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.client.chat.completions.create(</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span><span class="va">self</span>.model,</span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: prompt}],</span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span><span class="dv">0</span></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _extract_final_scores(<span class="va">self</span>, score_response: <span class="bu">str</span>) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extracts final scores from model response based on evaluation mode"""</span></span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"samre"</span>:</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Look for final tuple in format (score1, score2)</span></span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a>            tuple_pattern <span class="op">=</span> <span class="vs">r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'</span></span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a>            match <span class="op">=</span> re.search(tuple_pattern, score_response)</span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match:</span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> (<span class="bu">float</span>(match.group(<span class="dv">1</span>)), <span class="bu">float</span>(match.group(<span class="dv">2</span>)))</span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find score tuple in SAMRE response"</span>)</span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_weak"</span>:</span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Look for final tuple in format (score1, score2)</span></span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a>            tuple_pattern <span class="op">=</span> <span class="vs">r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'</span></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a>            match <span class="op">=</span> re.search(tuple_pattern, score_response)</span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match:</span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> (<span class="bu">float</span>(match.group(<span class="dv">1</span>)), <span class="bu">float</span>(match.group(<span class="dv">2</span>)))</span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find score tuple in weak baseline response"</span>)</span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_strong"</span>:</span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use XML parsing for strong baseline</span></span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a>            score_a_pattern <span class="op">=</span> <span class="vs">r'&lt;Answer1Score&gt;\s*(\d+\.?\d*)\s*&lt;/Answer1Score&gt;'</span></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a>            score_b_pattern <span class="op">=</span> <span class="vs">r'&lt;Answer2Score&gt;\s*(\d+\.?\d*)\s*&lt;/Answer2Score&gt;'</span></span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a>            scores_a <span class="op">=</span> [<span class="bu">float</span>(match.group(<span class="dv">1</span>)) <span class="cf">for</span> match <span class="kw">in</span> re.finditer(score_a_pattern, score_response)]</span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a>            scores_b <span class="op">=</span> [<span class="bu">float</span>(match.group(<span class="dv">1</span>)) <span class="cf">for</span> match <span class="kw">in</span> re.finditer(score_b_pattern, score_response)]</span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> scores_a <span class="kw">or</span> <span class="kw">not</span> scores_b:</span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Could not find scores for both candidates"</span>)</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(scores_a) <span class="op">!=</span> <span class="bu">len</span>(scores_b):</span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Mismatched number of scores: A=</span><span class="sc">{</span><span class="bu">len</span>(scores_a)<span class="sc">}</span><span class="ss">, B=</span><span class="sc">{</span><span class="bu">len</span>(scores_b)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a>            final_score_a <span class="op">=</span> <span class="bu">sum</span>(scores_a) <span class="op">/</span> <span class="bu">len</span>(scores_a)</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a>            final_score_b <span class="op">=</span> <span class="bu">sum</span>(scores_b) <span class="op">/</span> <span class="bu">len</span>(scores_b)</span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (final_score_a, final_score_b)</span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Unknown mode: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>mode<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> evaluate(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, num_rounds: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Main evaluation entry point that routes to appropriate evaluation method based on mode"""</span></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.client:</span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'"</span>)</span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mode.startswith(<span class="st">"baseline"</span>):</span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">=== Starting </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>mode<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Evaluation ===</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>._evaluate_baseline(question, answer_1, answer_2, num_rounds)</span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting SAMRE Evaluation ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>._evaluate_samre(question, answer_1, answer_2)</span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _evaluate_baseline(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, num_rounds: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implements baseline evaluation methods (both weak and strong)"""</span></span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a>        score_history <span class="op">=</span> []</span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a>        num_rounds <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> <span class="va">self</span>.mode <span class="op">==</span> <span class="st">"baseline_weak"</span> <span class="cf">else</span> num_rounds</span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_rounds):</span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Select appropriate prompt based on mode</span></span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a>            prompt_key <span class="op">=</span> <span class="st">"score_prompt_"</span> <span class="op">+</span> <span class="va">self</span>.mode</span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a>            score_prompt <span class="op">=</span> PROMPTS[prompt_key].<span class="bu">format</span>(</span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a>                question<span class="op">=</span>question,</span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a>                answer_1<span class="op">=</span>answer_1,</span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a>                answer_2<span class="op">=</span>answer_2</span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a>            score_response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(score_prompt)</span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"Score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a>                round_scores <span class="op">=</span> <span class="va">self</span>._extract_final_scores(score_response)</span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a>                score_history.append(<span class="bu">list</span>(round_scores))</span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.error(<span class="ss">f"Score parsing error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.error(<span class="ss">f"Raw score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a>                score_history.append([<span class="fl">10.0</span>, <span class="fl">10.0</span>])</span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate average scores across all rounds</span></span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a>        avg_scores <span class="op">=</span> [</span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span>(scores[i] <span class="cf">for</span> scores <span class="kw">in</span> score_history) <span class="op">/</span> <span class="bu">len</span>(score_history)</span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Determine winner based on average scores</span></span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a>        winner <span class="op">=</span> (</span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&gt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&lt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'tie'</span></span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a>            <span class="st">"winner"</span>: winner,</span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_scores"</span>: [<span class="bu">round</span>(score, <span class="dv">2</span>) <span class="cf">for</span> score <span class="kw">in</span> avg_scores] ,</span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rounds"</span>: <span class="bu">len</span>(score_history),</span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a>            <span class="st">"score_history"</span>: score_history,</span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a>            <span class="st">"full_response"</span>: score_response  <span class="co"># Include the final response for analysis</span></span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _evaluate_samre(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Implements SAMRE evaluation with multi-round debate process</span></span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a><span class="co">        Flow:</span></span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a><span class="co">        1. Get defenses from both advocates</span></span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a><span class="co">        2. Judge provides feedback and scores</span></span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a><span class="co">        3. Repeat until max rounds or convergence</span></span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a><span class="co">        4. Return averaged results</span></span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a>        local_memory <span class="op">=</span> Memory()</span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting SAMRE Evaluation ===</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> round_num <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.max_rounds):</span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Round </span><span class="sc">{</span>round_num <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>._run_debate_round(</span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a>                question,</span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a>                answer_1, </span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a>                answer_2, </span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a>                round_num,</span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a>                local_memory</span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._has_scores_converged(round_num, local_memory):</span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.logger.info(<span class="st">"</span><span class="ch">\n</span><span class="st">Scores have converged - ending debate early."</span>)</span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._prepare_results(local_memory)</span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> defend_answer(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, </span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a>                        advocate_id: <span class="bu">int</span>, feedback: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>, </span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a>                        opponent_argument: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>,</span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a>                        team_arguments: List[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get defense from an advocate.</span></span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a><span class="co">            question: The question being debated</span></span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a><span class="co">            answer_1: First answer in the debate</span></span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a><span class="co">            answer_2: Second answer in the debate</span></span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a><span class="co">            advocate_id: Which advocate (1 or 2) is defending</span></span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a><span class="co">            feedback: Previous feedback from judge</span></span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a><span class="co">            opponent_argument: Last argument from opponent</span></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a><span class="co">            team_arguments: List of previous arguments from this advocate's team</span></span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> team_arguments <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a>            team_arguments <span class="op">=</span> []</span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Map answers based on advocate_id</span></span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a>        answer <span class="op">=</span> answer_1 <span class="cf">if</span> advocate_id <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> answer_2</span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a>        opponent_answer <span class="op">=</span> answer_2 <span class="cf">if</span> advocate_id <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> answer_1</span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> <span class="va">self</span>.defend_prompt.<span class="bu">format</span>(</span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a>            advocate_id<span class="op">=</span>advocate_id,</span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a>            answer<span class="op">=</span>answer,  <span class="co"># The answer this advocate is defending</span></span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a>            opponent_answer<span class="op">=</span>opponent_answer,  <span class="co"># The opposing answer</span></span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>feedback,</span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>opponent_argument,</span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(team_arguments)</span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(prompt)</span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> judge_debate(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>,</span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a>                          defense_1: <span class="bu">str</span>, defense_2: <span class="bu">str</span>, </span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a>                          current_round: <span class="bu">int</span>,</span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a>                          memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, Tuple[<span class="bu">float</span>, <span class="bu">float</span>]]:</span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Judge the debate between two answers."""</span></span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a>        feedback_prompt <span class="op">=</span> <span class="va">self</span>.judge_prompt.<span class="bu">format</span>(</span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a>            answer_1<span class="op">=</span>answer_1,</span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a>            answer_2<span class="op">=</span>answer_2,</span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a>            current_round<span class="op">=</span>current_round,</span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a>            total_rounds<span class="op">=</span><span class="va">self</span>.max_rounds,</span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a>            previous_scores<span class="op">=</span>memory.scores,</span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a>            defense_1<span class="op">=</span>defense_1,</span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a>            defense_2<span class="op">=</span>defense_2</span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a>        feedback <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(feedback_prompt)</span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a>        score_prompt <span class="op">=</span> PROMPTS[<span class="st">"score_prompt_samre"</span>].<span class="bu">format</span>(</span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a>            question<span class="op">=</span>question,</span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a>            answer_1<span class="op">=</span>answer_1,</span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a>            answer_2<span class="op">=</span>answer_2,</span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a>            defense_1<span class="op">=</span>defense_1,</span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a>            defense_2<span class="op">=</span>defense_2,</span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a>            total_rounds<span class="op">=</span><span class="va">self</span>.max_rounds,</span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a>            previous_scores<span class="op">=</span>memory.scores,</span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>feedback</span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a>        score_response <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.get_completion(score_prompt)    </span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="va">self</span>._extract_final_scores(score_response)</span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Score parsing error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.error(<span class="ss">f"Raw score response: </span><span class="sc">{</span>score_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> (<span class="fl">10.0</span>, <span class="fl">10.0</span>)</span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> feedback, scores</span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _run_debate_round(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>, </span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a>                               round_num: <span class="bu">int</span>, memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Executes single debate round in SAMRE evaluation"""</span></span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a>        defenses <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>._get_advocate_defenses(question, answer_1, answer_2, memory)</span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a>        memory.arguments.append(defenses)</span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a>        feedback, scores <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.judge_debate(</span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, defenses[<span class="dv">0</span>], defenses[<span class="dv">1</span>], round_num <span class="op">+</span> <span class="dv">1</span>, memory</span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._store_round_results(feedback, scores, memory)</span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._display_round_results(defenses, feedback, scores)</span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scores</span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="kw">def</span> _get_advocate_defenses(<span class="va">self</span>, question: <span class="bu">str</span>, answer_1: <span class="bu">str</span>, answer_2: <span class="bu">str</span>,</span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a>                                   memory: Memory) <span class="op">-&gt;</span> Tuple[<span class="bu">str</span>, <span class="bu">str</span>]:</span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get defenses from both advocates."""</span></span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a>        defense_1 <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.defend_answer(</span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, <span class="dv">1</span>,</span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>memory.feedback[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory.feedback <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>memory.arguments[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>] <span class="cf">if</span> memory.arguments <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span>[args[<span class="dv">0</span>] <span class="cf">for</span> args <span class="kw">in</span> memory.arguments]</span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a>        defense_2 <span class="op">=</span> <span class="cf">await</span> <span class="va">self</span>.defend_answer(</span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a>            question, answer_1, answer_2, <span class="dv">2</span>,</span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a>            feedback<span class="op">=</span>memory.feedback[<span class="op">-</span><span class="dv">1</span>] <span class="cf">if</span> memory.feedback <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a>            opponent_argument<span class="op">=</span>memory.arguments[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="cf">if</span> memory.arguments <span class="cf">else</span> <span class="st">""</span>,</span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a>            team_arguments<span class="op">=</span>[args[<span class="dv">1</span>] <span class="cf">for</span> args <span class="kw">in</span> memory.arguments]</span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (defense_1, defense_2)</span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _store_round_results(<span class="va">self</span>, feedback: <span class="bu">str</span>, scores: Tuple[<span class="bu">float</span>, <span class="bu">float</span>],</span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a>                           memory: Memory) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Store feedback and scores from the round."""</span></span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a>        memory.feedback.append(feedback)</span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a>        memory.scores.append(scores)</span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _display_round_results(<span class="va">self</span>, defenses: Tuple[<span class="bu">str</span>, <span class="bu">str</span>], </span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a>                             feedback: <span class="bu">str</span>, scores: Tuple[<span class="bu">float</span>, <span class="bu">float</span>]) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Display the results of the current round."""</span></span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Advocate 1's defense:</span><span class="ch">\n</span><span class="sc">{</span>defenses[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Advocate 2's defense:</span><span class="ch">\n</span><span class="sc">{</span>defenses[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Judge's feedback:</span><span class="ch">\n</span><span class="sc">{</span>feedback<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.info(<span class="ss">f"Scores for this round: Answer 1 = </span><span class="sc">{</span><span class="bu">round</span>(scores[<span class="dv">0</span>], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">, Answer 2 = </span><span class="sc">{</span><span class="bu">round</span>(scores[<span class="dv">1</span>], <span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _has_scores_converged(<span class="va">self</span>, round_num: <span class="bu">int</span>, memory: Memory) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Checks if debate scores have converged by comparing last two rounds"""</span></span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> round_num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a>            prev_diff <span class="op">=</span> memory.scores[<span class="op">-</span><span class="dv">2</span>][<span class="dv">0</span>] <span class="op">-</span> memory.scores[<span class="op">-</span><span class="dv">2</span>][<span class="dv">1</span>]</span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a>            curr_diff <span class="op">=</span> memory.scores[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="op">-</span> memory.scores[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>]</span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (prev_diff <span class="op">*</span> curr_diff) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _prepare_results(<span class="va">self</span>, memory: Memory) <span class="op">-&gt;</span> Dict:</span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Prepare the final results dictionary."""</span></span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a>        avg_scores <span class="op">=</span> [</span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a>            <span class="bu">round</span>(<span class="bu">sum</span>(scores[i] <span class="cf">for</span> scores <span class="kw">in</span> memory.scores) <span class="op">/</span> <span class="bu">len</span>(memory.scores), <span class="dv">2</span>)</span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a>        winner <span class="op">=</span> (</span>
<span id="cb14-640"><a href="#cb14-640" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&gt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb14-641"><a href="#cb14-641" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> avg_scores[<span class="dv">0</span>] <span class="op">&lt;</span> avg_scores[<span class="dv">1</span>]</span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="st">'tie'</span></span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a>            <span class="st">"winner"</span>: winner,</span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a>            <span class="st">"average_scores"</span>: avg_scores,</span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a>            <span class="st">"rounds"</span>: <span class="bu">len</span>(memory.scores),</span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a>            <span class="st">"score_history"</span>: [[<span class="bu">round</span>(s[<span class="dv">0</span>], <span class="dv">2</span>), <span class="bu">round</span>(s[<span class="dv">1</span>], <span class="dv">2</span>)] <span class="cf">for</span> s <span class="kw">in</span> memory.scores],</span>
<span id="cb14-650"><a href="#cb14-650" aria-hidden="true" tabindex="-1"></a>            <span class="st">"argument_history"</span>: memory.arguments,</span>
<span id="cb14-651"><a href="#cb14-651" aria-hidden="true" tabindex="-1"></a>            <span class="st">"feedback_history"</span>: memory.feedback</span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a><span class="fu"># Load the MT-bench dataset</span></span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a>Next I will read in the MT-bench dataset from disk and prepare it for evaluation. I will use <span class="co">[</span><span class="ot">MtBenchHumanJudgementDataset</span><span class="co">](https://llamahub.ai/l/llama_datasets/MT%20Bench%20Human%20Judgement%20Dataset?from=)</span> from Llamahub.</span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-661"><a href="#cb14-661" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-662"><a href="#cb14-662" aria-hidden="true" tabindex="-1"></a><span class="co"># Commented out since the dataset is already downloaded</span></span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a><span class="co">#!llamaindex-cli download-llamadataset MtBenchHumanJudgementDataset --download-dir ./data</span></span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a>Next, I will load the dataset into a pandas dataframe and take a random sample of 300 rows.</span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb14-673"><a href="#cb14-673" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that loads the dataset"</span></span>
<span id="cb14-674"><a href="#cb14-674" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb14-675"><a href="#cb14-675" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-676"><a href="#cb14-676" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.llama_dataset <span class="im">import</span> LabelledPairwiseEvaluatorDataset</span>
<span id="cb14-677"><a href="#cb14-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-678"><a href="#cb14-678" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> LabelledPairwiseEvaluatorDataset.from_json(</span>
<span id="cb14-679"><a href="#cb14-679" aria-hidden="true" tabindex="-1"></a>    <span class="st">"./data/pairwise_evaluator_dataset.json"</span></span>
<span id="cb14-680"><a href="#cb14-680" aria-hidden="true" tabindex="-1"></a>).to_pandas()</span>
<span id="cb14-681"><a href="#cb14-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-682"><a href="#cb14-682" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[[<span class="st">'query'</span>, <span class="st">'answer'</span>, <span class="st">'second_answer'</span>, <span class="st">'answer_by'</span>, <span class="st">'second_answer_by'</span>, <span class="st">'reference_score'</span>]]</span>
<span id="cb14-683"><a href="#cb14-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-684"><a href="#cb14-684" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename as follows: query =&gt; question, answer =&gt; model_a_answer, second_answer =&gt; model_b_answer, answer_by =&gt; model_a, second_answer_by =&gt; model_b, reference_score =&gt; human_winner</span></span>
<span id="cb14-685"><a href="#cb14-685" aria-hidden="true" tabindex="-1"></a>df.rename(columns<span class="op">=</span>{<span class="st">'query'</span>: <span class="st">'question'</span>, <span class="st">'answer'</span>: <span class="st">'model_a_answer'</span>, <span class="st">'second_answer'</span>: <span class="st">'model_b_answer'</span>, <span class="st">'answer_by'</span>: <span class="st">'model_a'</span>, <span class="st">'second_answer_by'</span>: <span class="st">'model_b'</span>, <span class="st">'reference_score'</span>: <span class="st">'human_winner'</span>}, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-686"><a href="#cb14-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-687"><a href="#cb14-687" aria-hidden="true" tabindex="-1"></a><span class="co"># Reencode human winner as "model_a" if 1, "model_b" if 0, and "tie" if 0.5</span></span>
<span id="cb14-688"><a href="#cb14-688" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'human_winner'</span>] <span class="op">=</span> df[<span class="st">'human_winner'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">'model_a'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">'model_b'</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'tie'</span>)</span>
<span id="cb14-689"><a href="#cb14-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-690"><a href="#cb14-690" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a random sample of 300 rows</span></span>
<span id="cb14-691"><a href="#cb14-691" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.sample(n<span class="op">=</span><span class="dv">300</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-692"><a href="#cb14-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-693"><a href="#cb14-693" aria-hidden="true" tabindex="-1"></a>df.head()</span>
<span id="cb14-694"><a href="#cb14-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-695"><a href="#cb14-695" aria-hidden="true" tabindex="-1"></a><span class="co"># Take first 180 rows</span></span>
<span id="cb14-696"><a href="#cb14-696" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.iloc[:<span class="dv">180</span>]</span>
<span id="cb14-697"><a href="#cb14-697" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-698"><a href="#cb14-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-699"><a href="#cb14-699" aria-hidden="true" tabindex="-1"></a><span class="fu"># Use methods to evaluate MT-bench dataset</span></span>
<span id="cb14-700"><a href="#cb14-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-701"><a href="#cb14-701" aria-hidden="true" tabindex="-1"></a>Using the MT-bench dataset, I will run the three LLM models (Baseline-Weak, Baseline-Strong, and SAMRE) on each set of question and answers.</span>
<span id="cb14-702"><a href="#cb14-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-703"><a href="#cb14-703" aria-hidden="true" tabindex="-1"></a>The code below is the main evaluation loop, designed to run multiple evaluations asynchronously (to save time). It will evaluate each item in the dataset, and save the results to disk as a checkpoint. If the evaluation is interrupted, the code can be resumed from the last checkpoint.</span>
<span id="cb14-704"><a href="#cb14-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-707"><a href="#cb14-707" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-708"><a href="#cb14-708" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-709"><a href="#cb14-709" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb14-710"><a href="#cb14-710" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that runs the evaluations"</span></span>
<span id="cb14-711"><a href="#cb14-711" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> asyncio</span>
<span id="cb14-712"><a href="#cb14-712" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> asyncio <span class="im">import</span> Semaphore</span>
<span id="cb14-713"><a href="#cb14-713" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb14-714"><a href="#cb14-714" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb14-715"><a href="#cb14-715" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> hashlib</span>
<span id="cb14-716"><a href="#cb14-716" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb14-717"><a href="#cb14-717" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.WARNING)</span>
<span id="cb14-718"><a href="#cb14-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-719"><a href="#cb14-719" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> evaluate_conversation_pair(row, evaluators, semaphore, idx, total):</span>
<span id="cb14-720"><a href="#cb14-720" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate a single conversation pair with all evaluators"""</span></span>
<span id="cb14-721"><a href="#cb14-721" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="cf">with</span> semaphore:</span>
<span id="cb14-722"><a href="#cb14-722" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add delay between API calls</span></span>
<span id="cb14-723"><a href="#cb14-723" aria-hidden="true" tabindex="-1"></a>        <span class="co">#await asyncio.sleep(1)  # Add small delay between conversations</span></span>
<span id="cb14-724"><a href="#cb14-724" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-725"><a href="#cb14-725" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate pair_id from conversation hash</span></span>
<span id="cb14-726"><a href="#cb14-726" aria-hidden="true" tabindex="-1"></a>        pair_id <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>row[<span class="st">'model_a'</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>row[<span class="st">'model_b'</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>hashlib<span class="sc">.</span>sha256(<span class="bu">str</span>(row[<span class="st">'question'</span>]).encode())<span class="sc">.</span>hexdigest()[:<span class="dv">12</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb14-727"><a href="#cb14-727" aria-hidden="true" tabindex="-1"></a>        checkpoint_file <span class="op">=</span> <span class="ss">f'checkpoints/</span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">.json'</span></span>
<span id="cb14-728"><a href="#cb14-728" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-729"><a href="#cb14-729" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return existing checkpoint if available</span></span>
<span id="cb14-730"><a href="#cb14-730" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> os.path.exists(checkpoint_file):</span>
<span id="cb14-731"><a href="#cb14-731" aria-hidden="true" tabindex="-1"></a>            logging.info(<span class="ss">f"Found existing checkpoint file for </span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-732"><a href="#cb14-732" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> json.load(<span class="bu">open</span>(checkpoint_file))</span>
<span id="cb14-733"><a href="#cb14-733" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-734"><a href="#cb14-734" aria-hidden="true" tabindex="-1"></a>        logging.info(<span class="ss">f"No checkpoint file found for </span><span class="sc">{</span>pair_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-735"><a href="#cb14-735" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> {</span>
<span id="cb14-736"><a href="#cb14-736" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_a'</span>: row[<span class="st">'model_a'</span>],</span>
<span id="cb14-737"><a href="#cb14-737" aria-hidden="true" tabindex="-1"></a>            <span class="st">'model_b'</span>: row[<span class="st">'model_b'</span>],</span>
<span id="cb14-738"><a href="#cb14-738" aria-hidden="true" tabindex="-1"></a>            <span class="st">'human_winner'</span>: row[<span class="st">'human_winner'</span>],</span>
<span id="cb14-739"><a href="#cb14-739" aria-hidden="true" tabindex="-1"></a>            <span class="st">'pair_id'</span>: pair_id</span>
<span id="cb14-740"><a href="#cb14-740" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb14-741"><a href="#cb14-741" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-742"><a href="#cb14-742" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb14-743"><a href="#cb14-743" aria-hidden="true" tabindex="-1"></a>            <span class="co"># First run SAMRE evaluation with retries</span></span>
<span id="cb14-744"><a href="#cb14-744" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):  <span class="co"># Try up to 3 times</span></span>
<span id="cb14-745"><a href="#cb14-745" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb14-746"><a href="#cb14-746" aria-hidden="true" tabindex="-1"></a>                    samre_evaluator <span class="op">=</span> evaluators[<span class="st">'samre'</span>]</span>
<span id="cb14-747"><a href="#cb14-747" aria-hidden="true" tabindex="-1"></a>                    samre_result <span class="op">=</span> <span class="cf">await</span> samre_evaluator.evaluate(</span>
<span id="cb14-748"><a href="#cb14-748" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>], </span>
<span id="cb14-749"><a href="#cb14-749" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>], </span>
<span id="cb14-750"><a href="#cb14-750" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>]</span>
<span id="cb14-751"><a href="#cb14-751" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb14-752"><a href="#cb14-752" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'samre_winner'</span>] <span class="op">=</span> samre_result[<span class="st">'winner'</span>]</span>
<span id="cb14-753"><a href="#cb14-753" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'samre_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: samre_result[k] <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb14-754"><a href="#cb14-754" aria-hidden="true" tabindex="-1"></a>                    result.update({</span>
<span id="cb14-755"><a href="#cb14-755" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'samre_argument_history'</span>: samre_result[<span class="st">'argument_history'</span>],</span>
<span id="cb14-756"><a href="#cb14-756" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'samre_feedback_history'</span>: samre_result[<span class="st">'feedback_history'</span>]</span>
<span id="cb14-757"><a href="#cb14-757" aria-hidden="true" tabindex="-1"></a>                    })</span>
<span id="cb14-758"><a href="#cb14-758" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span>  <span class="co"># If successful, break retry loop</span></span>
<span id="cb14-759"><a href="#cb14-759" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-760"><a href="#cb14-760" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb14-761"><a href="#cb14-761" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span>  <span class="co"># Exponential backoff</span></span>
<span id="cb14-762"><a href="#cb14-762" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on SAMRE, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb14-763"><a href="#cb14-763" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb14-764"><a href="#cb14-764" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:  <span class="co"># Last attempt failed</span></span>
<span id="cb14-765"><a href="#cb14-765" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb14-766"><a href="#cb14-766" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb14-767"><a href="#cb14-767" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span>  <span class="co"># Re-raise non-rate-limit errors</span></span>
<span id="cb14-768"><a href="#cb14-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-769"><a href="#cb14-769" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> asyncio.sleep(<span class="fl">0.5</span>)  <span class="co"># Add small delay between evaluator calls</span></span>
<span id="cb14-770"><a href="#cb14-770" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-771"><a href="#cb14-771" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run baseline strong with same number of rounds as SAMRE</span></span>
<span id="cb14-772"><a href="#cb14-772" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb14-773"><a href="#cb14-773" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb14-774"><a href="#cb14-774" aria-hidden="true" tabindex="-1"></a>                    baseline_strong_evaluator <span class="op">=</span> evaluators[<span class="st">'baseline_strong'</span>]</span>
<span id="cb14-775"><a href="#cb14-775" aria-hidden="true" tabindex="-1"></a>                    baseline_strong_result <span class="op">=</span> <span class="cf">await</span> baseline_strong_evaluator.evaluate(</span>
<span id="cb14-776"><a href="#cb14-776" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>],</span>
<span id="cb14-777"><a href="#cb14-777" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>],</span>
<span id="cb14-778"><a href="#cb14-778" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>],</span>
<span id="cb14-779"><a href="#cb14-779" aria-hidden="true" tabindex="-1"></a>                        num_rounds<span class="op">=</span>result[<span class="st">'samre_rounds'</span>]</span>
<span id="cb14-780"><a href="#cb14-780" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb14-781"><a href="#cb14-781" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_strong_winner'</span>] <span class="op">=</span> baseline_strong_result[<span class="st">'winner'</span>]</span>
<span id="cb14-782"><a href="#cb14-782" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'baseline_strong_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: baseline_strong_result[k] </span>
<span id="cb14-783"><a href="#cb14-783" aria-hidden="true" tabindex="-1"></a>                                 <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb14-784"><a href="#cb14-784" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_strong_full_response'</span>] <span class="op">=</span> baseline_strong_result[<span class="st">'full_response'</span>]</span>
<span id="cb14-785"><a href="#cb14-785" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb14-786"><a href="#cb14-786" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-787"><a href="#cb14-787" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb14-788"><a href="#cb14-788" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span></span>
<span id="cb14-789"><a href="#cb14-789" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on baseline strong, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb14-790"><a href="#cb14-790" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb14-791"><a href="#cb14-791" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb14-792"><a href="#cb14-792" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb14-793"><a href="#cb14-793" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb14-794"><a href="#cb14-794" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span></span>
<span id="cb14-795"><a href="#cb14-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-796"><a href="#cb14-796" aria-hidden="true" tabindex="-1"></a>            <span class="cf">await</span> asyncio.sleep(<span class="fl">0.5</span>)  <span class="co"># Add small delay between evaluator calls</span></span>
<span id="cb14-797"><a href="#cb14-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-798"><a href="#cb14-798" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run baseline weak with 1 round</span></span>
<span id="cb14-799"><a href="#cb14-799" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb14-800"><a href="#cb14-800" aria-hidden="true" tabindex="-1"></a>                <span class="cf">try</span>:</span>
<span id="cb14-801"><a href="#cb14-801" aria-hidden="true" tabindex="-1"></a>                    baseline_weak_evaluator <span class="op">=</span> evaluators[<span class="st">'baseline_weak'</span>]</span>
<span id="cb14-802"><a href="#cb14-802" aria-hidden="true" tabindex="-1"></a>                    baseline_weak_result <span class="op">=</span> <span class="cf">await</span> baseline_weak_evaluator.evaluate(</span>
<span id="cb14-803"><a href="#cb14-803" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'question'</span>],</span>
<span id="cb14-804"><a href="#cb14-804" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_a_answer'</span>],</span>
<span id="cb14-805"><a href="#cb14-805" aria-hidden="true" tabindex="-1"></a>                        row[<span class="st">'model_b_answer'</span>],</span>
<span id="cb14-806"><a href="#cb14-806" aria-hidden="true" tabindex="-1"></a>                        num_rounds<span class="op">=</span><span class="dv">1</span></span>
<span id="cb14-807"><a href="#cb14-807" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb14-808"><a href="#cb14-808" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_weak_winner'</span>] <span class="op">=</span> baseline_weak_result[<span class="st">'winner'</span>]</span>
<span id="cb14-809"><a href="#cb14-809" aria-hidden="true" tabindex="-1"></a>                    result.update({<span class="ss">f'baseline_weak_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>: baseline_weak_result[k] </span>
<span id="cb14-810"><a href="#cb14-810" aria-hidden="true" tabindex="-1"></a>                                 <span class="cf">for</span> k <span class="kw">in</span> [<span class="st">'average_scores'</span>, <span class="st">'rounds'</span>, <span class="st">'score_history'</span>]})</span>
<span id="cb14-811"><a href="#cb14-811" aria-hidden="true" tabindex="-1"></a>                    result[<span class="st">'baseline_weak_full_response'</span>] <span class="op">=</span> baseline_weak_result[<span class="st">'full_response'</span>]</span>
<span id="cb14-812"><a href="#cb14-812" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb14-813"><a href="#cb14-813" aria-hidden="true" tabindex="-1"></a>                <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-814"><a href="#cb14-814" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="st">"rate limit"</span> <span class="kw">in</span> <span class="bu">str</span>(e).lower():</span>
<span id="cb14-815"><a href="#cb14-815" aria-hidden="true" tabindex="-1"></a>                        wait_time <span class="op">=</span> (<span class="dv">2</span> <span class="op">**</span> attempt) <span class="op">*</span> <span class="dv">1</span></span>
<span id="cb14-816"><a href="#cb14-816" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Rate limit hit on baseline weak, waiting </span><span class="sc">{</span>wait_time<span class="sc">}</span><span class="ss"> seconds..."</span>)</span>
<span id="cb14-817"><a href="#cb14-817" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">await</span> asyncio.sleep(wait_time)</span>
<span id="cb14-818"><a href="#cb14-818" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> attempt <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb14-819"><a href="#cb14-819" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">raise</span></span>
<span id="cb14-820"><a href="#cb14-820" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb14-821"><a href="#cb14-821" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">raise</span></span>
<span id="cb14-822"><a href="#cb14-822" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb14-823"><a href="#cb14-823" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-824"><a href="#cb14-824" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Error evaluating row </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-825"><a href="#cb14-825" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'samre_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-826"><a href="#cb14-826" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'baseline_strong_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-827"><a href="#cb14-827" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'baseline_weak_winner'</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-828"><a href="#cb14-828" aria-hidden="true" tabindex="-1"></a>            result[<span class="st">'error'</span>] <span class="op">=</span> <span class="bu">str</span>(e)</span>
<span id="cb14-829"><a href="#cb14-829" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-830"><a href="#cb14-830" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save checkpoint after each evaluation</span></span>
<span id="cb14-831"><a href="#cb14-831" aria-hidden="true" tabindex="-1"></a>        os.makedirs(<span class="st">'checkpoints'</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-832"><a href="#cb14-832" aria-hidden="true" tabindex="-1"></a>        json.dump(result, <span class="bu">open</span>(checkpoint_file, <span class="st">'w'</span>))</span>
<span id="cb14-833"><a href="#cb14-833" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-834"><a href="#cb14-834" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (idx <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-835"><a href="#cb14-835" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Processed </span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>total<span class="sc">}</span><span class="ss"> conversations"</span>)</span>
<span id="cb14-836"><a href="#cb14-836" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-837"><a href="#cb14-837" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result</span>
<span id="cb14-838"><a href="#cb14-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-839"><a href="#cb14-839" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> evaluate_conversations_async(df, evaluators, semaphore_limit<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb14-840"><a href="#cb14-840" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate conversations asynchronously"""</span></span>
<span id="cb14-841"><a href="#cb14-841" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reduce semaphore limit</span></span>
<span id="cb14-842"><a href="#cb14-842" aria-hidden="true" tabindex="-1"></a>    semaphore_limit <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Process one at a time to avoid rate limits</span></span>
<span id="cb14-843"><a href="#cb14-843" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-844"><a href="#cb14-844" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process in smaller batches</span></span>
<span id="cb14-845"><a href="#cb14-845" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb14-846"><a href="#cb14-846" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb14-847"><a href="#cb14-847" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-848"><a href="#cb14-848" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(df), batch_size):</span>
<span id="cb14-849"><a href="#cb14-849" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> df.iloc[i:i<span class="op">+</span>batch_size]</span>
<span id="cb14-850"><a href="#cb14-850" aria-hidden="true" tabindex="-1"></a>        tasks <span class="op">=</span> [</span>
<span id="cb14-851"><a href="#cb14-851" aria-hidden="true" tabindex="-1"></a>            evaluate_conversation_pair(row[<span class="dv">1</span>], evaluators, Semaphore(semaphore_limit), idx, <span class="bu">len</span>(df))</span>
<span id="cb14-852"><a href="#cb14-852" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, row <span class="kw">in</span> <span class="bu">enumerate</span>(batch.iterrows(), start<span class="op">=</span>i)</span>
<span id="cb14-853"><a href="#cb14-853" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb14-854"><a href="#cb14-854" aria-hidden="true" tabindex="-1"></a>        batch_results <span class="op">=</span> <span class="cf">await</span> asyncio.gather(<span class="op">*</span>tasks)</span>
<span id="cb14-855"><a href="#cb14-855" aria-hidden="true" tabindex="-1"></a>        results.extend(batch_results)</span>
<span id="cb14-856"><a href="#cb14-856" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-857"><a href="#cb14-857" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add delay between batches</span></span>
<span id="cb14-858"><a href="#cb14-858" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">+</span> batch_size <span class="op">&lt;</span> <span class="bu">len</span>(df):</span>
<span id="cb14-859"><a href="#cb14-859" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Completed batch </span><span class="sc">{</span>i<span class="op">//</span>batch_size <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, waiting before next batch..."</span>)</span>
<span id="cb14-860"><a href="#cb14-860" aria-hidden="true" tabindex="-1"></a>            <span class="co">#await asyncio.sleep(5)  # 5 second delay between batches</span></span>
<span id="cb14-861"><a href="#cb14-861" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-862"><a href="#cb14-862" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(results)</span>
<span id="cb14-863"><a href="#cb14-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-864"><a href="#cb14-864" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> main():</span>
<span id="cb14-865"><a href="#cb14-865" aria-hidden="true" tabindex="-1"></a>    <span class="cf">async</span> <span class="cf">with</span> ModelEvaluator.create(mode<span class="op">=</span><span class="st">"samre"</span>) <span class="im">as</span> samre_evaluator, <span class="op">\</span></span>
<span id="cb14-866"><a href="#cb14-866" aria-hidden="true" tabindex="-1"></a>               ModelEvaluator.create(mode<span class="op">=</span><span class="st">"baseline_strong"</span>) <span class="im">as</span> baseline_strong_evaluator, <span class="op">\</span></span>
<span id="cb14-867"><a href="#cb14-867" aria-hidden="true" tabindex="-1"></a>               ModelEvaluator.create(mode<span class="op">=</span><span class="st">"baseline_weak"</span>) <span class="im">as</span> baseline_weak_evaluator:</span>
<span id="cb14-868"><a href="#cb14-868" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="cf">await</span> evaluate_conversations_async(</span>
<span id="cb14-869"><a href="#cb14-869" aria-hidden="true" tabindex="-1"></a>            df,</span>
<span id="cb14-870"><a href="#cb14-870" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb14-871"><a href="#cb14-871" aria-hidden="true" tabindex="-1"></a>                <span class="st">'samre'</span>: samre_evaluator, </span>
<span id="cb14-872"><a href="#cb14-872" aria-hidden="true" tabindex="-1"></a>                <span class="st">'baseline_strong'</span>: baseline_strong_evaluator,</span>
<span id="cb14-873"><a href="#cb14-873" aria-hidden="true" tabindex="-1"></a>                <span class="st">'baseline_weak'</span>: baseline_weak_evaluator</span>
<span id="cb14-874"><a href="#cb14-874" aria-hidden="true" tabindex="-1"></a>            },</span>
<span id="cb14-875"><a href="#cb14-875" aria-hidden="true" tabindex="-1"></a>            semaphore_limit<span class="op">=</span><span class="dv">1</span></span>
<span id="cb14-876"><a href="#cb14-876" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-877"><a href="#cb14-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-878"><a href="#cb14-878" aria-hidden="true" tabindex="-1"></a><span class="co"># Run evaluation with checkpoint recovery</span></span>
<span id="cb14-879"><a href="#cb14-879" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb14-880"><a href="#cb14-880" aria-hidden="true" tabindex="-1"></a>    eval_df <span class="op">=</span> <span class="cf">await</span> main()</span>
<span id="cb14-881"><a href="#cb14-881" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-882"><a href="#cb14-882" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Error during evaluation: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ch">\n</span><span class="ss">Recovering from checkpoints..."</span>)</span>
<span id="cb14-883"><a href="#cb14-883" aria-hidden="true" tabindex="-1"></a>    eval_df <span class="op">=</span> pd.DataFrame([json.load(<span class="bu">open</span>(<span class="ss">f'checkpoints/</span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">'</span>)) </span>
<span id="cb14-884"><a href="#cb14-884" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">for</span> f <span class="kw">in</span> os.listdir(<span class="st">'checkpoints'</span>) </span>
<span id="cb14-885"><a href="#cb14-885" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">if</span> f.endswith(<span class="st">'.json'</span>)])</span>
<span id="cb14-886"><a href="#cb14-886" aria-hidden="true" tabindex="-1"></a><span class="cf">finally</span>:</span>
<span id="cb14-887"><a href="#cb14-887" aria-hidden="true" tabindex="-1"></a>    eval_df.to_csv(<span class="st">'eval_df.csv'</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-888"><a href="#cb14-888" aria-hidden="true" tabindex="-1"></a>    eval_df.head()</span>
<span id="cb14-889"><a href="#cb14-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-890"><a href="#cb14-890" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop rows with any null values on the model winner columns</span></span>
<span id="cb14-891"><a href="#cb14-891" aria-hidden="true" tabindex="-1"></a>eval_df <span class="op">=</span> eval_df.dropna(subset<span class="op">=</span>[<span class="st">'baseline_strong_winner'</span>, <span class="st">'baseline_weak_winner'</span>, <span class="st">'samre_winner'</span>])</span>
<span id="cb14-892"><a href="#cb14-892" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-893"><a href="#cb14-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-894"><a href="#cb14-894" aria-hidden="true" tabindex="-1"></a><span class="fu"># Results</span></span>
<span id="cb14-895"><a href="#cb14-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-896"><a href="#cb14-896" aria-hidden="true" tabindex="-1"></a>Now that the evaluation is complete, I will evaluate the performance of each of the three methods by looking at how well each method agreed with the human judgments. I'll use Krippendorff's alpha to measure agreement, since it is a robust measure of agreement that can handle non-binary ratings (among other things).</span>
<span id="cb14-897"><a href="#cb14-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-900"><a href="#cb14-900" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-901"><a href="#cb14-901" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-902"><a href="#cb14-902" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb14-903"><a href="#cb14-903" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that calculates agreement"</span></span>
<span id="cb14-904"><a href="#cb14-904" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> krippendorff <span class="im">import</span> alpha</span>
<span id="cb14-905"><a href="#cb14-905" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-906"><a href="#cb14-906" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb14-907"><a href="#cb14-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-908"><a href="#cb14-908" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_agreement(df, rater1_col, rater2_col):</span>
<span id="cb14-909"><a href="#cb14-909" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-910"><a href="#cb14-910" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate Krippendorff's alpha between two raters.</span></span>
<span id="cb14-911"><a href="#cb14-911" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-912"><a href="#cb14-912" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb14-913"><a href="#cb14-913" aria-hidden="true" tabindex="-1"></a><span class="co">        df: DataFrame containing the ratings</span></span>
<span id="cb14-914"><a href="#cb14-914" aria-hidden="true" tabindex="-1"></a><span class="co">        rater1_col: Name of first rater's column</span></span>
<span id="cb14-915"><a href="#cb14-915" aria-hidden="true" tabindex="-1"></a><span class="co">        rater2_col: Name of second rater's column</span></span>
<span id="cb14-916"><a href="#cb14-916" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-917"><a href="#cb14-917" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-918"><a href="#cb14-918" aria-hidden="true" tabindex="-1"></a><span class="co">        float: Krippendorff's alpha score</span></span>
<span id="cb14-919"><a href="#cb14-919" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-920"><a href="#cb14-920" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create label encoder</span></span>
<span id="cb14-921"><a href="#cb14-921" aria-hidden="true" tabindex="-1"></a>    le <span class="op">=</span> LabelEncoder()</span>
<span id="cb14-922"><a href="#cb14-922" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-923"><a href="#cb14-923" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine all unique values from both columns</span></span>
<span id="cb14-924"><a href="#cb14-924" aria-hidden="true" tabindex="-1"></a>    all_values <span class="op">=</span> pd.concat([df[rater1_col], df[rater2_col]]).unique()</span>
<span id="cb14-925"><a href="#cb14-925" aria-hidden="true" tabindex="-1"></a>    le.fit(all_values)</span>
<span id="cb14-926"><a href="#cb14-926" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-927"><a href="#cb14-927" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Transform the ratings to numeric values</span></span>
<span id="cb14-928"><a href="#cb14-928" aria-hidden="true" tabindex="-1"></a>    ratings1 <span class="op">=</span> le.transform(df[rater1_col].fillna(<span class="st">'missing'</span>))</span>
<span id="cb14-929"><a href="#cb14-929" aria-hidden="true" tabindex="-1"></a>    ratings2 <span class="op">=</span> le.transform(df[rater2_col].fillna(<span class="st">'missing'</span>))</span>
<span id="cb14-930"><a href="#cb14-930" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-931"><a href="#cb14-931" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape data for krippendorff alpha calculation</span></span>
<span id="cb14-932"><a href="#cb14-932" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each row represents one item, each column represents one rater</span></span>
<span id="cb14-933"><a href="#cb14-933" aria-hidden="true" tabindex="-1"></a>    reliability_data <span class="op">=</span> np.vstack([ratings1, ratings2])</span>
<span id="cb14-934"><a href="#cb14-934" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-935"><a href="#cb14-935" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha(reliability_data<span class="op">=</span>reliability_data, level_of_measurement<span class="op">=</span><span class="st">'nominal'</span>)</span>
<span id="cb14-936"><a href="#cb14-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-937"><a href="#cb14-937" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate agreement scores for all methods</span></span>
<span id="cb14-938"><a href="#cb14-938" aria-hidden="true" tabindex="-1"></a>human_baseline_strong_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_strong_winner'</span>)</span>
<span id="cb14-939"><a href="#cb14-939" aria-hidden="true" tabindex="-1"></a>human_baseline_weak_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'baseline_weak_winner'</span>)</span>
<span id="cb14-940"><a href="#cb14-940" aria-hidden="true" tabindex="-1"></a>human_samre_agreement <span class="op">=</span> calculate_agreement(eval_df, <span class="st">'human_winner'</span>, <span class="st">'samre_winner'</span>)</span>
<span id="cb14-941"><a href="#cb14-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-942"><a href="#cb14-942" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame with the agreement scores</span></span>
<span id="cb14-943"><a href="#cb14-943" aria-hidden="true" tabindex="-1"></a>agreement_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb14-944"><a href="#cb14-944" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Evaluator Pair'</span>: [<span class="st">'Human vs. Baseline-Strong'</span>, <span class="st">'Human vs. Baseline-Weak'</span>, <span class="st">'Human vs. SAMRE'</span>],</span>
<span id="cb14-945"><a href="#cb14-945" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Krippendorff Alpha'</span>: [human_baseline_strong_agreement, human_baseline_weak_agreement, human_samre_agreement]</span>
<span id="cb14-946"><a href="#cb14-946" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb14-947"><a href="#cb14-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-948"><a href="#cb14-948" aria-hidden="true" tabindex="-1"></a><span class="co"># Round the scores to 3 decimal places</span></span>
<span id="cb14-949"><a href="#cb14-949" aria-hidden="true" tabindex="-1"></a>agreement_df[<span class="st">'Krippendorff Alpha'</span>] <span class="op">=</span> agreement_df[<span class="st">'Krippendorff Alpha'</span>].<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb14-950"><a href="#cb14-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-951"><a href="#cb14-951" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percent difference between Baseline-Strong and Baseline-Weak, and SAMRE and Baseline-Strong</span></span>
<span id="cb14-952"><a href="#cb14-952" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_diff <span class="op">=</span> (human_baseline_strong_agreement <span class="op">-</span> human_baseline_weak_agreement) <span class="op">/</span> human_baseline_strong_agreement</span>
<span id="cb14-953"><a href="#cb14-953" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_diff <span class="op">=</span> (human_baseline_strong_agreement <span class="op">-</span> human_samre_agreement) <span class="op">/</span> human_baseline_strong_agreement</span>
<span id="cb14-954"><a href="#cb14-954" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_diff <span class="op">=</span> (human_samre_agreement <span class="op">-</span> human_baseline_weak_agreement) <span class="op">/</span> human_samre_agreement</span>
<span id="cb14-955"><a href="#cb14-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-956"><a href="#cb14-956" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb14-957"><a href="#cb14-957" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(agreement_df)</span>
<span id="cb14-958"><a href="#cb14-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-959"><a href="#cb14-959" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the percent difference</span></span>
<span id="cb14-960"><a href="#cb14-960" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Krippendorff Alpha Improvements:"</span>)</span>
<span id="cb14-961"><a href="#cb14-961" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb14-962"><a href="#cb14-962" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb14-963"><a href="#cb14-963" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_diff<span class="sc">:.0%}</span><span class="ss">"</span>)</span>
<span id="cb14-964"><a href="#cb14-964" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-965"><a href="#cb14-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-966"><a href="#cb14-966" aria-hidden="true" tabindex="-1"></a>Although none of the methods yielded particularly strong agreement with the human judges, we can observe a few things:</span>
<span id="cb14-967"><a href="#cb14-967" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>As reported in the paper, SAMRE yielded significantly better agreement than Baseline-Weak (0.307 vs. 0.252, an increase of ~18%).</span>
<span id="cb14-968"><a href="#cb14-968" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Strong yielded significantly better agreement than Baseline-Weak (0.391 vs. 0.252, an increase of ~36%).</span>
<span id="cb14-969"><a href="#cb14-969" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Importantly, Baseline-Strong also yielded significantly better agreement than SAMRE (0.391 vs. 0.252, an increase of ~21%)!</span>
<span id="cb14-970"><a href="#cb14-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-971"><a href="#cb14-971" aria-hidden="true" tabindex="-1"></a>We can also measure performance in terms of binary classification accuracy, using Matthews Correlation Coefficient (MCC) as the metric, while re-encoding the "winner" columns to indicate whether model_a was selected as better (1) or not better (0) in each case.</span>
<span id="cb14-972"><a href="#cb14-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-975"><a href="#cb14-975" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-976"><a href="#cb14-976" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-977"><a href="#cb14-977" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb14-978"><a href="#cb14-978" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that calculates Matthews Correlation Coefficient (MCC)"</span></span>
<span id="cb14-979"><a href="#cb14-979" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode winner as binary</span></span>
<span id="cb14-980"><a href="#cb14-980" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_winner_as_binary(winner):</span>
<span id="cb14-981"><a href="#cb14-981" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> winner <span class="op">==</span> <span class="st">'model_a'</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb14-982"><a href="#cb14-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-983"><a href="#cb14-983" aria-hidden="true" tabindex="-1"></a><span class="co"># Create binary columns for each evaluator</span></span>
<span id="cb14-984"><a href="#cb14-984" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'human_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'human_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb14-985"><a href="#cb14-985" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'baseline_strong_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'baseline_strong_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb14-986"><a href="#cb14-986" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'baseline_weak_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'baseline_weak_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb14-987"><a href="#cb14-987" aria-hidden="true" tabindex="-1"></a>eval_df[<span class="st">'samre_model_a_better'</span>] <span class="op">=</span> eval_df[<span class="st">'samre_winner'</span>].<span class="bu">apply</span>(encode_winner_as_binary)</span>
<span id="cb14-988"><a href="#cb14-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-989"><a href="#cb14-989" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> matthews_corrcoef</span>
<span id="cb14-990"><a href="#cb14-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-991"><a href="#cb14-991" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate MCC for each method</span></span>
<span id="cb14-992"><a href="#cb14-992" aria-hidden="true" tabindex="-1"></a>metrics_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb14-993"><a href="#cb14-993" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Method'</span>: [<span class="st">'Baseline-Strong'</span>, <span class="st">'Baseline-Weak'</span>, <span class="st">'SAMRE'</span>],</span>
<span id="cb14-994"><a href="#cb14-994" aria-hidden="true" tabindex="-1"></a>    <span class="st">'MCC'</span>: [</span>
<span id="cb14-995"><a href="#cb14-995" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb14-996"><a href="#cb14-996" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb14-997"><a href="#cb14-997" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'baseline_strong_model_a_better'</span>]</span>
<span id="cb14-998"><a href="#cb14-998" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb14-999"><a href="#cb14-999" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb14-1000"><a href="#cb14-1000" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb14-1001"><a href="#cb14-1001" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'baseline_weak_model_a_better'</span>]</span>
<span id="cb14-1002"><a href="#cb14-1002" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb14-1003"><a href="#cb14-1003" aria-hidden="true" tabindex="-1"></a>        matthews_corrcoef(</span>
<span id="cb14-1004"><a href="#cb14-1004" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb14-1005"><a href="#cb14-1005" aria-hidden="true" tabindex="-1"></a>            eval_df[<span class="st">'samre_model_a_better'</span>]</span>
<span id="cb14-1006"><a href="#cb14-1006" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-1007"><a href="#cb14-1007" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb14-1008"><a href="#cb14-1008" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb14-1009"><a href="#cb14-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1010"><a href="#cb14-1010" aria-hidden="true" tabindex="-1"></a><span class="co"># Round the scores to 3 decimal places</span></span>
<span id="cb14-1011"><a href="#cb14-1011" aria-hidden="true" tabindex="-1"></a>metrics_df[<span class="st">'MCC'</span>] <span class="op">=</span> metrics_df[<span class="st">'MCC'</span>].<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb14-1012"><a href="#cb14-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1013"><a href="#cb14-1013" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percent differences</span></span>
<span id="cb14-1014"><a href="#cb14-1014" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_percent_diff(new, old):</span>
<span id="cb14-1015"><a href="#cb14-1015" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (new <span class="op">-</span> old) <span class="op">/</span> old <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb14-1016"><a href="#cb14-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1017"><a href="#cb14-1017" aria-hidden="true" tabindex="-1"></a><span class="co"># MCC differences</span></span>
<span id="cb14-1018"><a href="#cb14-1018" aria-hidden="true" tabindex="-1"></a>samre_baseline_weak_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb14-1019"><a href="#cb14-1019" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb14-1020"><a href="#cb14-1020" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb14-1021"><a href="#cb14-1021" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-1022"><a href="#cb14-1022" aria-hidden="true" tabindex="-1"></a>baseline_strong_baseline_weak_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb14-1023"><a href="#cb14-1023" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb14-1024"><a href="#cb14-1024" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Weak'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb14-1025"><a href="#cb14-1025" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-1026"><a href="#cb14-1026" aria-hidden="true" tabindex="-1"></a>baseline_strong_samre_mcc_diff <span class="op">=</span> calc_percent_diff(</span>
<span id="cb14-1027"><a href="#cb14-1027" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'Baseline-Strong'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>],</span>
<span id="cb14-1028"><a href="#cb14-1028" aria-hidden="true" tabindex="-1"></a>    metrics_df.loc[metrics_df[<span class="st">'Method'</span>] <span class="op">==</span> <span class="st">'SAMRE'</span>, <span class="st">'MCC'</span>].iloc[<span class="dv">0</span>]</span>
<span id="cb14-1029"><a href="#cb14-1029" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-1030"><a href="#cb14-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1031"><a href="#cb14-1031" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw values</span></span>
<span id="cb14-1032"><a href="#cb14-1032" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(metrics_df)</span>
<span id="cb14-1033"><a href="#cb14-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1034"><a href="#cb14-1034" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">MCC Improvements:"</span>)</span>
<span id="cb14-1035"><a href="#cb14-1035" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SAMRE vs. Baseline-Weak: </span><span class="sc">{</span>samre_baseline_weak_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb14-1036"><a href="#cb14-1036" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. Baseline-Weak: </span><span class="sc">{</span>baseline_strong_baseline_weak_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb14-1037"><a href="#cb14-1037" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline-Strong vs. SAMRE: </span><span class="sc">{</span>baseline_strong_samre_mcc_diff<span class="sc">:.0f}</span><span class="ss">%"</span>)</span>
<span id="cb14-1038"><a href="#cb14-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1039"><a href="#cb14-1039" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1040"><a href="#cb14-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1041"><a href="#cb14-1041" aria-hidden="true" tabindex="-1"></a>Looking at MCC values, we observe the following:</span>
<span id="cb14-1042"><a href="#cb14-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1043"><a href="#cb14-1043" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>SAMRE did not perform better than Baseline-Weak (MCCs = 0.331 in both cases).</span>
<span id="cb14-1044"><a href="#cb14-1044" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Baseline-Strong performed better than Baseline-Weak (0.464 vs. 0.331, an increase of ~40%).</span>
<span id="cb14-1045"><a href="#cb14-1045" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Baseline-Strong did not perform better than SAMRE (0.464 vs. 0.331, an increase of ~40%).</span>
<span id="cb14-1046"><a href="#cb14-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1047"><a href="#cb14-1047" aria-hidden="true" tabindex="-1"></a>Why does this metric disagree with the Krippendorff alpha results on the SAMRE vs. Baseline-Weak comparison? I would guess this is due to how ties were resolved when encoding the winner as binary. Also note that the same MCC values for SAMRE and Baseline-Weak is not an error. We can see that the confusion matrices were different.</span>
<span id="cb14-1048"><a href="#cb14-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1051"><a href="#cb14-1051" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1052"><a href="#cb14-1052" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb14-1053"><a href="#cb14-1053" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold-show: false</span></span>
<span id="cb14-1054"><a href="#cb14-1054" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Click to view the code that generates confusion matrices"</span></span>
<span id="cb14-1055"><a href="#cb14-1055" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb14-1056"><a href="#cb14-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1057"><a href="#cb14-1057" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Baseline-Weak Confusion Matrix:"</span>)</span>
<span id="cb14-1058"><a href="#cb14-1058" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(</span>
<span id="cb14-1059"><a href="#cb14-1059" aria-hidden="true" tabindex="-1"></a>    eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb14-1060"><a href="#cb14-1060" aria-hidden="true" tabindex="-1"></a>    eval_df[<span class="st">'baseline_weak_model_a_better'</span>]</span>
<span id="cb14-1061"><a href="#cb14-1061" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb14-1062"><a href="#cb14-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1063"><a href="#cb14-1063" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">SAMRE Confusion Matrix:"</span>)</span>
<span id="cb14-1064"><a href="#cb14-1064" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(</span>
<span id="cb14-1065"><a href="#cb14-1065" aria-hidden="true" tabindex="-1"></a>    eval_df[<span class="st">'human_model_a_better'</span>], </span>
<span id="cb14-1066"><a href="#cb14-1066" aria-hidden="true" tabindex="-1"></a>    eval_df[<span class="st">'samre_model_a_better'</span>]</span>
<span id="cb14-1067"><a href="#cb14-1067" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb14-1068"><a href="#cb14-1068" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1069"><a href="#cb14-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1070"><a href="#cb14-1070" aria-hidden="true" tabindex="-1"></a>Thus, across both of these measures of performance, we see that SAMRE did not perform better than a baseline that is designed with best practices.</span>
<span id="cb14-1071"><a href="#cb14-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1072"><a href="#cb14-1072" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion</span></span>
<span id="cb14-1073"><a href="#cb14-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1074"><a href="#cb14-1074" aria-hidden="true" tabindex="-1"></a>In this post, I have shown that SAMRE does not perform better than a well-engineered baseline method. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><a href="https://creativecommons.org/licenses/by/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i></a> 2014–2025 Tyler Burleigh</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <i class="fa-brands fa-r-project" aria-label="r-project"></i>, <a href="https://quarto.org/">Quarto</a>, and the <a href="https://github.com/andrewheiss/ath-quarto">ath-quarto</a> theme</span></p>
</div>
  </div>
</footer>




</body></html>