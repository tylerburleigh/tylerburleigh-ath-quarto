---
title: "A test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices"
date: 2025-01-12
description: "In this post, I re-evaluate a method that was recently published in arXiv, critiquing the baseline model used in the paper and then implementing a new baseline model that implements standard best practices and similar multi-round aggregation. I find that the SAMRE method does not perform better than the new baseline model. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as the being skeptical of claims in research papers that compare new methods to a baseline."
categories:
  - prompt-engineering
  - python
  - LLM-as-judge
  - LLM-evals
freeze: true
---

I've been doing a lot of work with LLM-based evaluations lately, and I've been thinking about how to improve the quality of these evaluations.

I like to read research papers from arXiv for inspiration, and I recently came across a paper called [Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates](https://arxiv.org/abs/2410.04663), which introduces a new method inspired by judicial process called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process.

I was initially impressed by the results, as they reported a gain of 6-8% over the baseline method(!!). However, I am often skeptical of comparisons to "baseline" models in these research papers, as I find that they often fail to implement standard best practices and are therefore not represenative of true gains over baseline.

Given this skepticism of mine, I decided that it might be interesting to put it to the test: What if I implemented the SAMRE method, and compared it to a baseline model that does implement standard best practices for prompt engineering? Would I find that the SAMRE method is indeed an improvement over the baseline? Or would I find that SAMRE is inferior to a properly implemented baseline?

Using a sample of 180 conversations from MT-bench for testing and evaluation, I evaluated three methods:

1. SAMRE, as implemented in the paper
2. Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)
3. Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.

After running the evaluations and calculating agreement with human judges, I found that although SAMRE did yield better agreement than Baseline-Weak (18% improvement), it was inferior to Baseline-Strong (which was 36% better than SAMRE!).

These results serve to highlight the importance of implementing standard best practices in baseline models, as well as being skeptical of claims in research papers that compare new methods to a "baseline model". Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.

# Baseline model inadequacies

First, let's consider some of the inadequacies in the Baseline model's prompt reported in the paper. The prompt they used was as follows:

```prompt
You are a fair, impartial judge scoring a debate on the following question:
question.
Answer 1: answer_1
Answer 2: answer_2
Score each answer on a scale of 1-20 for each of the following criteria:
1. Relevance to the question
2. Accuracy of information and use of credible sources
3. Depth of analysis and completeness of argument
4. Clarity of expression and logical flow
5. Strength of reasoning and factual support
6. Effectiveness in addressing opponent’s points
Provide scores as [answer_1_score, answer_2_score] for each criterion in a list format, then sum for final scores. Please keep an eye on the slightest difference that should make a difference in the scoring. Don’t overthink!
Relevance:
Accuracy:
Depth:
Clarity:
Logic and Factuality:
Addressing opponent’s points:
Final Scores (sum of above) as a tuple (example: (18, 9)):
Explain your scoring, focusing on why one answer is better than the other based on the criteria above. Keep your explanation concise but informative.
Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).
Example: (18, 9)
Your scores and explanation:
```

Here are the issues I see with this prompt:

1. The prompt does not use delimiters for most of the inputs. I would enclose the inputs inside XML tags like <Question></Question>, <Answer1></Answer1>, and <Answer2></Answer2>, but in a pinch delimiters like triple backticks can be used.

2. The prompt instructs the model to first generate scores in list format, and then to sum them. But as we know, language models models often make arithmetic mistakes. It would be better to ask the model to generate scores for each criterion, and then to programmatically extract and summarize them in python (or another programming language) from which the routine is run.

3. Although the prompt asks the model to "explain your scoring", it is not clear if the model should be reasoning about each criterion before it scores them, or if it should provide reasoning at the end when giving its final score. I would ask the model to provide reasoning for each criterion that it is asked to score, and ask it to reason before scoring.

4. It's unclear why a scale of 1-20 is used. This is not a standard scale for scoring. I would use a scale of 1-10 which is likely more familiar to the model and can be expected to be used more consistently.

5. Although the prompt does suggest that the model provide its scores in tuple format, it would be better to provide more explicit format instructions.

6. The prompt includes an "Effectiveness in addressing opponent's points" criterion, but this is almost certainly irrelevant given that the answers to the question were not generated with the goal of addressing an opponent.

7. Finally, although this goes beyond the prompt itself, the authors of the paper are comparing a multi-round method to a single-round method. This is obviously an unfair comparison. Instead, it would be better to compare the SAMRE method to a baseline that uses the same number of rounds and then similarly averages its scores.

With all of that in mind, here's how I would rewrite the prompt:

```prompt
You are a fair, impartial judge scoring a debate on Question.

<Question>
{question}
</Question>

Two Answers have been given to the Question.

<Answer1>
{answer_1}
</Answer1>

<Answer2>
{answer_2}
</Answer2>

The Answers are being judged on the following Criteria:

<Criteria>
<Criterion1>Relevance to their task</Criterion1>
<Criterion2>Accuracy and credible sources</Criterion2>
<Criterion3>Depth and completeness</Criterion3>
<Criterion4>Clarity and logical flow</Criterion4>
<Criterion5>Reasoning and factual support</Criterion5>
</Criteria>

For each Criterion, briefly analyze the performance of 
the two Answers, then give a score between 1 and 10.

Respond as follows:
<Criterion1>
<CriterionName>Relevance to their task</CriterionName>
<Analysis>
Answer 1: [Analysis of Answer 1 performance on the Criterion]
Answer 2: [Analysis of Answer 2 performance on the Criterion]
</Analysis>
<Scores>
<Answer1Score>[score between 1 and 10]</Answer1Score>
<Answer2Score>[score between 1 and 10]</Answer2Score>
</Scores>
</Criterion1>
<Criterion2>
<CriterionName>Accuracy and credible sources</CriterionName>
<Analysis>
Answer 1: [Analysis of Answer 1 performance on the Criterion]
Answer 2: [Analysis of Answer 2 performance on the Criterion]
</Analysis>
<Scores>
<Answer1Score>[score between 1 and 10]</Answer1Score>
<Answer2Score>[score between 1 and 10]</Answer2Score>
</Scores>
</Criterion2>
...
```

Notice that the prompt now uses XML tags to structure the instructions, that it asks the model to provide reasoning for each criterion before scoring, and that it gives the model a clear format for its response that reinforces analysis before scoring for each criterion.

I've also change the scale from 1-20 to 1-10, removed the unnecessary "Effectiveness in addressing opponent's points" criterion, and removed the instruction to summarize the scores, as I would handle this within the code.

# Hypothesis and predictions

I hypothesize that SAMRE will not perform better than a baseline model that implements standard best practices for prompt engineering.

My predictions are as follows:

1. SAMRE will perform better than Baseline-Weak.
2. Baseline-Strong will perform better than Baseline-Weak.
3. Baseline-Strong will perform equal to or better than SAMRE.

# My implementation of SAMRE and Baseline

Okay, so with those criticisms out of the way, let's design evaluators to implement three methods:

1. SAMRE
2. Baseline-Weak: The baseline model used in the paper that does not implement standard best practices for prompt engineering.
3. Baseline-Strong: A baseline model that does implement standard best practices for prompt engineering.

Below is my python implementation of the evaluators. To the best of my ability, I have implemented the SAMRE and Baseline methods as described in the paper (I call the paper's Baseline method "Baseline-Weak"). And I have implemented a Baseline-Strong evaluator that implements what I would consider to be standard best practices for prompt engineering.

This will allow me to test my hypothesis and predictions.

```{python}
#| code-fold: true
#| code-fold-show: false
#| code-summary: "Click to view the implementation code"
from dataclasses import dataclass, field
from typing import List, Tuple, Dict
import os
from openai import AsyncOpenAI
import logging
import re
from contextlib import asynccontextmanager

# Define the prompts
PROMPTS = {
# Prompt for advocates to defend their assigned answer, used in SAMRE
"defend_prompt": """
You’re an eloquent advocate (Advocate {advocate_id}) in a group defending this answer:
{answer} to the question: {question}. Your opponent group defends: {opponent_answer}.
Collaborate with your fellow advocates to convince the audience your argument is
better. Use the latest feedback, your opponent’s last argument, and your team’s previous
arguments to improve your case.
Latest feedback: {feedback}
Opponent’s last argument: {opponent_argument}
Your team’s previous arguments: team_arguments
Respond in under 80 words.
Your defense:
""",
# Prompt for judge to provide feedback on debate progress, used in SAMRE
"judge_prompt": """
You’re a fair, impartial judge in a debate on: "{question}". Answer 1: "{answer_1}".
Answer 2: "{answer_2}". Your goal is to provide feedback that will help advocate groups
improve and differentiate their arguments more clearly.
Current round: {current_round}
Total rounds: {total_rounds}
Previous scores: {previous_scores}
Defense for 1st answer: {defense_1}
Defense for 2nd answer: {defense_2}
Provide specific, constructive feedback to help each advocate group strengthen their
unique position. Encourage them to address weaknesses and highlight distinctions. Aim
for your feedback to lead to more divergent scores in future rounds.
Give your feedback in under 50 words:
""",
# Prompt for SAMRE method scoring
"score_prompt_samre": """
You’re a critical, impartial judge in a high-stakes debate on: "{question}". Answer
1: "{answer_1}". Answer 2: "{answer_2}". Your goal is to provide detailed, constructive
feedback that will push advocates to significantly improve their arguments.
Total rounds: {total_rounds}
Previous scores: {previous_scores}
Defense for 1st answer: {defense_1}
Defense for 2nd answer: {defense_2}
Analyze each argument meticulously. Be thorough and unbiased in your assessment of:
1. Relevance to the question
2. Accuracy of information and use of credible sources
3. Depth of analysis and completeness of argument
4. Clarity of expression and logical flow
5. Strength of reasoning and factual support
6. Effectiveness in addressing opponent’s points
For each criterion, provide a score on a scale of 1-20 and detailed justification.
Scores should be given as [answer_1_score, answer_2_score] for each criterion.
Your comprehensive feedback for each advocate (50 words each):
Feedback for Advocate 1:
Feedback for Advocate 2:
Sum up the scores and return the final score tuple (score1, score2). Example: (95, 87)
Your detailed scores and final tally:
""",
# Prompt for Baseline-Weak method scoring, which represents the baseline model used in the paper
"score_prompt_baseline_weak": """
You are a fair, impartial judge scoring a debate on the following question:
question.
Answer 1: {answer_1}
Answer 2: {answer_2}
Score each answer on a scale of 1-20 for each of the following criteria:
1. Relevance to the question
2. Accuracy of information and use of credible sources
3. Depth of analysis and completeness of argument
4. Clarity of expression and logical flow
5. Strength of reasoning and factual support
6. Effectiveness in addressing opponent’s points
Provide scores as [Answer1_score, Answer2_score] for each criterion in a list format,
then sum for final scores. Please keep an eye on the slightest difference that should
make a difference in the scoring. Don’t overthink!
Relevance:
Accuracy:
Depth:
Clarity:
Logic and Factuality:
Addressing opponent’s points:
Final Scores (sum of above) as a tuple (example: (18, 9)):
Explain your scoring, focusing on why one answer is better than the other based on the
criteria above. Keep your explanation concise but informative.
Finally, return the final score tuple (score1, score2) as a tuple (in parentheses).
Example: (18, 9)
Your scores and explanation:
""",
# Prompt for Baseline-Strong method scoring, which implements what I consider to be standard best practices for prompt engineering
"score_prompt_baseline_strong": """
You are a fair, impartial judge scoring a debate on Question.

<Question>
{question}
</Question>

Two Answers have been given to the Question.

<Answer1>
{answer_1}
</Answer1>

<Answer2>
{answer_2}
</Answer2>

The Answers are being judged on the following Criteria:

<Criteria>
<Criterion1>Relevance to their task</Criterion1>
<Criterion2>Accuracy and credible sources</Criterion2>
<Criterion3>Depth and completeness</Criterion3>
<Criterion4>Clarity and logical flow</Criterion4>
<Criterion5>Reasoning and factual support</Criterion5>
</Criteria>

For each Criterion, briefly analyze the performance of 
the two Answers, then give a score between 1 and 10.

Respond as follows:
<Criterion1>
<CriterionName>Relevance to their task</CriterionName>
<Analysis>
Answer 1: [Analysis of Answer 1 performance on the Criterion]
Answer 2: [Analysis of Answer 2 performance on the Criterion]
</Analysis>
<Scores>
<Answer1Score>[score between 1 and 10]</Answer1Score>
<Answer2Score>[score between 1 and 10]</Answer2Score>
</Scores>
</Criterion1>
<Criterion2>
<CriterionName>Accuracy and credible sources</CriterionName>
<Analysis>
Answer 1: [Analysis of Answer 1 performance on the Criterion]
Answer 2: [Analysis of Answer 2 performance on the Criterion]
</Analysis>
<Scores>
<Answer1Score>[score between 1 and 10]</Answer1Score>
<Answer2Score>[score between 1 and 10]</Answer2Score>
</Scores>
</Criterion2>
...
"""
}

@dataclass
class Memory:
    """Stores debate history including arguments, scores, and feedback for each round, used in SAMRE"""
    arguments: List[Tuple[str, str]] = field(default_factory=list)
    scores: List[Tuple[float, float]] = field(default_factory=list)
    feedback: List[str] = field(default_factory=list)

class ModelEvaluator:
    @classmethod
    @asynccontextmanager
    async def create(cls, mode="samre", model="gpt-4o-mini", logging_level=logging.WARNING):
        """Factory method to create evaluator instance with proper async context management"""
        instance = cls(mode=mode, model=model, logging_level=logging_level)
        instance.client = AsyncOpenAI()
        try:
            yield instance
        finally:
            await instance.client.close()

    def _setup_logger(self, logging_level):
        """Setup logger with word wrapping."""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging_level)
        if not logger.handlers:
            handler = logging.StreamHandler()
            class WrapFormatter(logging.Formatter):
                def format(self, record):
                    import textwrap
                    message = super().format(record)
                    return '\n'.join(textwrap.fill(line, width=80) 
                                for line in message.split('\n'))
            
            formatter = WrapFormatter('%(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger

    def __init__(self, mode="samre", model="gpt-4o-mini", logging_level=logging.WARNING):
        self.mode = mode
        self.model = model
        # Modify to handle both baseline modes
        self.max_rounds = 1 if mode.startswith("baseline") else 4
        self.logger = self._setup_logger(logging_level)
        
        # Initialize all prompts
        self.defend_prompt = PROMPTS["defend_prompt"]
        self.judge_prompt = PROMPTS["judge_prompt"]


    async def get_completion(self, prompt: str) -> str:
        """Get a completion from the OpenAI API."""
        if not self.client:
            raise RuntimeError("Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'")
            
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "system", "content": prompt}],
            temperature=0
        )
        return response.choices[0].message.content

    def _extract_final_scores(self, score_response: str) -> Tuple[float, float]:
        """Extracts final scores from model response based on evaluation mode"""
        if self.mode == "samre":
            # Look for final tuple in format (score1, score2)
            tuple_pattern = r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'
            match = re.search(tuple_pattern, score_response)
            if match:
                return (float(match.group(1)), float(match.group(2)))
            raise ValueError("Could not find score tuple in SAMRE response")
        
        elif self.mode == "baseline_weak":
            # Look for final tuple in format (score1, score2)
            tuple_pattern = r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)'
            match = re.search(tuple_pattern, score_response)
            if match:
                return (float(match.group(1)), float(match.group(2)))
            raise ValueError("Could not find score tuple in weak baseline response")
        
        elif self.mode == "baseline_strong":
            # Use XML parsing for strong baseline
            score_a_pattern = r'<Answer1Score>\s*(\d+\.?\d*)\s*</Answer1Score>'
            score_b_pattern = r'<Answer2Score>\s*(\d+\.?\d*)\s*</Answer2Score>'
            
            scores_a = [float(match.group(1)) for match in re.finditer(score_a_pattern, score_response)]
            scores_b = [float(match.group(1)) for match in re.finditer(score_b_pattern, score_response)]
            
            if not scores_a or not scores_b:
                raise ValueError("Could not find scores for both candidates")
            
            if len(scores_a) != len(scores_b):
                raise ValueError(f"Mismatched number of scores: A={len(scores_a)}, B={len(scores_b)}")
            
            final_score_a = sum(scores_a) / len(scores_a)
            final_score_b = sum(scores_b) / len(scores_b)
            
            return (final_score_a, final_score_b)
        
        else:
            raise ValueError(f"Unknown mode: {self.mode}")

    async def evaluate(self, question: str, answer_1: str, answer_2: str, num_rounds: int = 1) -> Dict:
        """Main evaluation entry point that routes to appropriate evaluation method based on mode"""
        if not self.client:
            raise RuntimeError("Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'")
            
        if self.mode.startswith("baseline"):
            self.logger.info(f"\n=== Starting {self.mode.title()} Evaluation ===\n")
            return await self._evaluate_baseline(question, answer_1, answer_2, num_rounds)
        else:
            self.logger.info("\n=== Starting SAMRE Evaluation ===\n")
            return await self._evaluate_samre(question, answer_1, answer_2)

    async def _evaluate_baseline(self, question: str, answer_1: str, answer_2: str, num_rounds: int = 1) -> Dict:
        """Implements baseline evaluation methods (both weak and strong)"""
        score_history = []
        
        num_rounds = 1 if self.mode == "baseline_weak" else num_rounds
        for _ in range(num_rounds):
            # Select appropriate prompt based on mode
            prompt_key = "score_prompt_" + self.mode
            score_prompt = PROMPTS[prompt_key].format(
                question=question,
                answer_1=answer_1,
                answer_2=answer_2
            )
            score_response = await self.get_completion(score_prompt)
            self.logger.info(f"Score response: {score_response}")
            
            try:
                round_scores = self._extract_final_scores(score_response)
                score_history.append(list(round_scores))
            except Exception as e:
                self.logger.error(f"Score parsing error: {e}")
                self.logger.error(f"Raw score response: {score_response}")
                score_history.append([10.0, 10.0])

        # Calculate average scores across all rounds
        avg_scores = [
            sum(scores[i] for scores in score_history) / len(score_history)
            for i in range(2)
        ]

        # Determine winner based on average scores
        winner = (
            'model_a' if avg_scores[0] > avg_scores[1]
            else 'model_b' if avg_scores[0] < avg_scores[1]
            else 'tie'
        )

        return {
            "winner": winner,
            "average_scores": [round(score, 2) for score in avg_scores] ,
            "rounds": len(score_history),
            "score_history": score_history,
            "full_response": score_response  # Include the final response for analysis
        }
        
    async def _evaluate_samre(self, question: str, answer_1: str, answer_2: str) -> Dict:
        """Implements SAMRE evaluation with multi-round debate process
        
        Flow:
        1. Get defenses from both advocates
        2. Judge provides feedback and scores
        3. Repeat until max rounds or convergence
        4. Return averaged results
        """
        local_memory = Memory()
        
        self.logger.info("\n=== Starting SAMRE Evaluation ===\n")
        
        for round_num in range(self.max_rounds):
            self.logger.info(f"\n--- Round {round_num + 1} ---")
            
            scores = await self._run_debate_round(
                question,
                answer_1, 
                answer_2, 
                round_num,
                local_memory
            )
            
            if self._has_scores_converged(round_num, local_memory):
                self.logger.info("\nScores have converged - ending debate early.")
                break
        
        return self._prepare_results(local_memory)

    async def defend_answer(self, question: str, answer_1: str, answer_2: str, 
                        advocate_id: int, feedback: str = "", 
                        opponent_argument: str = "",
                        team_arguments: List[str] = None) -> str:
        """Get defense from an advocate.
        
        Args:
            question: The question being debated
            answer_1: First answer in the debate
            answer_2: Second answer in the debate
            advocate_id: Which advocate (1 or 2) is defending
            feedback: Previous feedback from judge
            opponent_argument: Last argument from opponent
            team_arguments: List of previous arguments from this advocate's team
        """
        if team_arguments is None:
            team_arguments = []
            
        # Map answers based on advocate_id
        answer = answer_1 if advocate_id == 1 else answer_2
        opponent_answer = answer_2 if advocate_id == 1 else answer_1
            
        prompt = self.defend_prompt.format(
            question=question,
            advocate_id=advocate_id,
            answer=answer,  # The answer this advocate is defending
            opponent_answer=opponent_answer,  # The opposing answer
            feedback=feedback,
            opponent_argument=opponent_argument,
            team_arguments="\n".join(team_arguments)
        )
        return await self.get_completion(prompt)

    async def judge_debate(self, question: str, answer_1: str, answer_2: str,
                          defense_1: str, defense_2: str, 
                          current_round: int,
                          memory: Memory) -> Tuple[str, Tuple[float, float]]:
        """Judge the debate between two answers."""
        feedback_prompt = self.judge_prompt.format(
            question=question,
            answer_1=answer_1,
            answer_2=answer_2,
            current_round=current_round,
            total_rounds=self.max_rounds,
            previous_scores=memory.scores,
            defense_1=defense_1,
            defense_2=defense_2
        )
        feedback = await self.get_completion(feedback_prompt)
        
        score_prompt = PROMPTS["score_prompt_samre"].format(
            question=question,
            answer_1=answer_1,
            answer_2=answer_2,
            defense_1=defense_1,
            defense_2=defense_2,
            total_rounds=self.max_rounds,
            previous_scores=memory.scores,
            feedback=feedback
        )
        score_response = await self.get_completion(score_prompt)    
        self.logger.info(f"Score response: {score_response}")
        
        try:
            scores = self._extract_final_scores(score_response)
        except Exception as e:
            self.logger.error(f"Score parsing error: {e}")
            self.logger.error(f"Raw score response: {score_response}")
            scores = (10.0, 10.0)
        
        return feedback, scores

    async def _run_debate_round(self, question: str, answer_1: str, answer_2: str, 
                               round_num: int, memory: Memory) -> Tuple[float, float]:
        """Executes single debate round in SAMRE evaluation"""
        defenses = await self._get_advocate_defenses(question, answer_1, answer_2, memory)
        memory.arguments.append(defenses)
        
        feedback, scores = await self.judge_debate(
            question, answer_1, answer_2, defenses[0], defenses[1], round_num + 1, memory
        )
        
        self._store_round_results(feedback, scores, memory)
        self._display_round_results(defenses, feedback, scores)
        
        return scores

    async def _get_advocate_defenses(self, question: str, answer_1: str, answer_2: str,
                                   memory: Memory) -> Tuple[str, str]:
        """Get defenses from both advocates."""
        defense_1 = await self.defend_answer(
            question, answer_1, answer_2, 1,
            feedback=memory.feedback[-1] if memory.feedback else "",
            opponent_argument=memory.arguments[-1][1] if memory.arguments else "",
            team_arguments=[args[0] for args in memory.arguments]
        )
        
        defense_2 = await self.defend_answer(
            question, answer_1, answer_2, 2,
            feedback=memory.feedback[-1] if memory.feedback else "",
            opponent_argument=memory.arguments[-1][0] if memory.arguments else "",
            team_arguments=[args[1] for args in memory.arguments]
        )
        
        return (defense_1, defense_2)

    def _store_round_results(self, feedback: str, scores: Tuple[float, float],
                           memory: Memory) -> None:
        """Store feedback and scores from the round."""
        memory.feedback.append(feedback)
        memory.scores.append(scores)

    def _display_round_results(self, defenses: Tuple[str, str], 
                             feedback: str, scores: Tuple[float, float]) -> None:
        """Display the results of the current round."""
        self.logger.info(f"\nAdvocate 1's defense:\n{defenses[0]}")
        self.logger.info(f"\nAdvocate 2's defense:\n{defenses[1]}")
        self.logger.info(f"\nJudge's feedback:\n{feedback}")
        self.logger.info(f"Scores for this round: Answer 1 = {round(scores[0], 2)}, Answer 2 = {round(scores[1], 2)}")

    def _has_scores_converged(self, round_num: int, memory: Memory) -> bool:
        """Checks if debate scores have converged by comparing last two rounds"""
        if round_num > 0:
            prev_diff = memory.scores[-2][0] - memory.scores[-2][1]
            curr_diff = memory.scores[-1][0] - memory.scores[-1][1]
            return (prev_diff * curr_diff) > 0
        return False

    def _prepare_results(self, memory: Memory) -> Dict:
        """Prepare the final results dictionary."""
        avg_scores = [
            round(sum(scores[i] for scores in memory.scores) / len(memory.scores), 2)
            for i in range(2)
        ]
        
        winner = (
            'model_a' if avg_scores[0] > avg_scores[1]
            else 'model_b' if avg_scores[0] < avg_scores[1]
            else 'tie'
        )
        
        return {
            "winner": winner,
            "average_scores": avg_scores,
            "rounds": len(memory.scores),
            "score_history": [[round(s[0], 2), round(s[1], 2)] for s in memory.scores],
            "argument_history": memory.arguments,
            "feedback_history": memory.feedback
        }
```

# Load the MT-bench dataset

Next I will read in the MT-bench dataset from disk and prepare it for evaluation. I will use [MtBenchHumanJudgementDataset](https://llamahub.ai/l/llama_datasets/MT%20Bench%20Human%20Judgement%20Dataset?from=) from Llamahub.

```{python}
# Commented out since the dataset is already downloaded
#!llamaindex-cli download-llamadataset MtBenchHumanJudgementDataset --download-dir ./data
```

Next, I will load the dataset into a pandas dataframe and take a random sample of 300 rows.

```{python}
#| code-fold: true
#| code-fold-show: false
#| code-summary: "Click to view the code that loads the dataset"
import json
import pandas as pd
from llama_index.core.llama_dataset import LabelledPairwiseEvaluatorDataset

df = LabelledPairwiseEvaluatorDataset.from_json(
    "./data/pairwise_evaluator_dataset.json"
).to_pandas()

df = df[['query', 'answer', 'second_answer', 'answer_by', 'second_answer_by', 'reference_score']]

# Rename as follows: query => question, answer => model_a_answer, second_answer => model_b_answer, answer_by => model_a, second_answer_by => model_b, reference_score => human_winner
df.rename(columns={'query': 'question', 'answer': 'model_a_answer', 'second_answer': 'model_b_answer', 'answer_by': 'model_a', 'second_answer_by': 'model_b', 'reference_score': 'human_winner'}, inplace=True)

# Reencode human winner as "model_a" if 1, "model_b" if 0, and "tie" if 0.5
df['human_winner'] = df['human_winner'].apply(lambda x: 'model_a' if x == 1 else 'model_b' if x == 0 else 'tie')

# Take a random sample of 300 rows
df = df.sample(n=300, random_state=42)

df.head()

# Take first 180 rows
df = df.iloc[:180]
```

# Use methods to evaluate MT-bench dataset

Using the MT-bench dataset, I will run the three LLM models (Baseline-Weak, Baseline-Strong, and SAMRE) on each set of question and answers.

The code below is the main evaluation loop, designed to run multiple evaluations asynchronously (to save time). It will evaluate each item in the dataset, and save the results to disk as a checkpoint. If the evaluation is interrupted, the code can be resumed from the last checkpoint.

```{python}
#| code-fold: true
#| code-fold-show: false
#| code-summary: "Click to view the code that runs the evaluations"
import asyncio
from asyncio import Semaphore
import logging
import os
import hashlib
import json
logging.basicConfig(level=logging.WARNING)

async def evaluate_conversation_pair(row, evaluators, semaphore, idx, total):
    """Evaluate a single conversation pair with all evaluators"""
    async with semaphore:
        # Add delay between API calls
        #await asyncio.sleep(1)  # Add small delay between conversations
        
        # Generate pair_id from conversation hash
        pair_id = f"{row['model_a']}_{row['model_b']}_{hashlib.sha256(str(row['question']).encode()).hexdigest()[:12]}"
        checkpoint_file = f'checkpoints/{pair_id}.json'
        
        # Return existing checkpoint if available
        if os.path.exists(checkpoint_file):
            logging.info(f"Found existing checkpoint file for {pair_id}")
            return json.load(open(checkpoint_file))
        
        logging.info(f"No checkpoint file found for {pair_id}")
        result = {
            'model_a': row['model_a'],
            'model_b': row['model_b'],
            'human_winner': row['human_winner'],
            'pair_id': pair_id
        }
        
        try:
            # First run SAMRE evaluation with retries
            for attempt in range(3):  # Try up to 3 times
                try:
                    samre_evaluator = evaluators['samre']
                    samre_result = await samre_evaluator.evaluate(
                        row['question'], 
                        row['model_a_answer'], 
                        row['model_b_answer']
                    )
                    result['samre_winner'] = samre_result['winner']
                    result.update({f'samre_{k}': samre_result[k] for k in ['average_scores', 'rounds', 'score_history']})
                    result.update({
                        'samre_argument_history': samre_result['argument_history'],
                        'samre_feedback_history': samre_result['feedback_history']
                    })
                    break  # If successful, break retry loop
                except Exception as e:
                    if "rate limit" in str(e).lower():
                        wait_time = (2 ** attempt) * 1  # Exponential backoff
                        print(f"Rate limit hit on SAMRE, waiting {wait_time} seconds...")
                        await asyncio.sleep(wait_time)
                        if attempt == 2:  # Last attempt failed
                            raise
                    else:
                        raise  # Re-raise non-rate-limit errors

            await asyncio.sleep(0.5)  # Add small delay between evaluator calls
            
            # Run baseline strong with same number of rounds as SAMRE
            for attempt in range(3):
                try:
                    baseline_strong_evaluator = evaluators['baseline_strong']
                    baseline_strong_result = await baseline_strong_evaluator.evaluate(
                        row['question'],
                        row['model_a_answer'],
                        row['model_b_answer'],
                        num_rounds=result['samre_rounds']
                    )
                    result['baseline_strong_winner'] = baseline_strong_result['winner']
                    result.update({f'baseline_strong_{k}': baseline_strong_result[k] 
                                 for k in ['average_scores', 'rounds', 'score_history']})
                    result['baseline_strong_full_response'] = baseline_strong_result['full_response']
                    break
                except Exception as e:
                    if "rate limit" in str(e).lower():
                        wait_time = (2 ** attempt) * 1
                        print(f"Rate limit hit on baseline strong, waiting {wait_time} seconds...")
                        await asyncio.sleep(wait_time)
                        if attempt == 2:
                            raise
                    else:
                        raise

            await asyncio.sleep(0.5)  # Add small delay between evaluator calls

            # Run baseline weak with 1 round
            for attempt in range(3):
                try:
                    baseline_weak_evaluator = evaluators['baseline_weak']
                    baseline_weak_result = await baseline_weak_evaluator.evaluate(
                        row['question'],
                        row['model_a_answer'],
                        row['model_b_answer'],
                        num_rounds=1
                    )
                    result['baseline_weak_winner'] = baseline_weak_result['winner']
                    result.update({f'baseline_weak_{k}': baseline_weak_result[k] 
                                 for k in ['average_scores', 'rounds', 'score_history']})
                    result['baseline_weak_full_response'] = baseline_weak_result['full_response']
                    break
                except Exception as e:
                    if "rate limit" in str(e).lower():
                        wait_time = (2 ** attempt) * 1
                        print(f"Rate limit hit on baseline weak, waiting {wait_time} seconds...")
                        await asyncio.sleep(wait_time)
                        if attempt == 2:
                            raise
                    else:
                        raise
                        
        except Exception as e:
            print(f"Error evaluating row {idx}: {str(e)}")
            result['samre_winner'] = None
            result['baseline_strong_winner'] = None
            result['baseline_weak_winner'] = None
            result['error'] = str(e)
        
        # Save checkpoint after each evaluation
        os.makedirs('checkpoints', exist_ok=True)
        json.dump(result, open(checkpoint_file, 'w'))
        
        if (idx + 1) % 10 == 0:
            print(f"Processed {idx + 1}/{total} conversations")
            
        return result

async def evaluate_conversations_async(df, evaluators, semaphore_limit=3):
    """Evaluate conversations asynchronously"""
    # Reduce semaphore limit
    semaphore_limit = 1  # Process one at a time to avoid rate limits
    
    # Process in smaller batches
    batch_size = 10
    results = []
    
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        tasks = [
            evaluate_conversation_pair(row[1], evaluators, Semaphore(semaphore_limit), idx, len(df))
            for idx, row in enumerate(batch.iterrows(), start=i)
        ]
        batch_results = await asyncio.gather(*tasks)
        results.extend(batch_results)
        
        # Add delay between batches
        if i + batch_size < len(df):
            print(f"Completed batch {i//batch_size + 1}, waiting before next batch...")
            #await asyncio.sleep(5)  # 5 second delay between batches
            
    return pd.DataFrame(results)

async def main():
    async with ModelEvaluator.create(mode="samre") as samre_evaluator, \
               ModelEvaluator.create(mode="baseline_strong") as baseline_strong_evaluator, \
               ModelEvaluator.create(mode="baseline_weak") as baseline_weak_evaluator:
        return await evaluate_conversations_async(
            df,
            {
                'samre': samre_evaluator, 
                'baseline_strong': baseline_strong_evaluator,
                'baseline_weak': baseline_weak_evaluator
            },
            semaphore_limit=1
        )

# Run evaluation with checkpoint recovery
try:
    eval_df = await main()
except Exception as e:
    print(f"Error during evaluation: {str(e)}\nRecovering from checkpoints...")
    eval_df = pd.DataFrame([json.load(open(f'checkpoints/{f}')) 
                           for f in os.listdir('checkpoints') 
                           if f.endswith('.json')])
finally:
    eval_df.to_csv('eval_df.csv', index=False)
    eval_df.head()

# Drop rows with any null values on the model winner columns
eval_df = eval_df.dropna(subset=['baseline_strong_winner', 'baseline_weak_winner', 'samre_winner'])
```

# Results

Now that the evaluation is complete, I will evaluate the performance of each of the three methods by looking at how well each method agreed with the human judgments. I'll use Krippendorff's alpha to measure agreement, since it is a robust measure of agreement that can handle non-binary ratings (among other things).

```{python}
#| code-fold: true
#| code-fold-show: false
#| code-summary: "Click to view the code that calculates agreement"
from krippendorff import alpha
import numpy as np
from sklearn.preprocessing import LabelEncoder

def calculate_agreement(df, rater1_col, rater2_col):
    """
    Calculate Krippendorff's alpha between two raters.
    
    Args:
        df: DataFrame containing the ratings
        rater1_col: Name of first rater's column
        rater2_col: Name of second rater's column
    
    Returns:
        float: Krippendorff's alpha score
    """
    # Create label encoder
    le = LabelEncoder()
    
    # Combine all unique values from both columns
    all_values = pd.concat([df[rater1_col], df[rater2_col]]).unique()
    le.fit(all_values)
    
    # Transform the ratings to numeric values
    ratings1 = le.transform(df[rater1_col].fillna('missing'))
    ratings2 = le.transform(df[rater2_col].fillna('missing'))
    
    # Reshape data for krippendorff alpha calculation
    # Each row represents one item, each column represents one rater
    reliability_data = np.vstack([ratings1, ratings2])
    
    return alpha(reliability_data=reliability_data, level_of_measurement='nominal')

# Calculate agreement scores for all methods
human_baseline_strong_agreement = calculate_agreement(eval_df, 'human_winner', 'baseline_strong_winner')
human_baseline_weak_agreement = calculate_agreement(eval_df, 'human_winner', 'baseline_weak_winner')
human_samre_agreement = calculate_agreement(eval_df, 'human_winner', 'samre_winner')

# Create a DataFrame with the agreement scores
agreement_df = pd.DataFrame({
    'Evaluator Pair': ['Human-Baseline Strong', 'Human-Baseline Weak', 'Human-SAMRE'],
    'Krippendorff Alpha': [human_baseline_strong_agreement, human_baseline_weak_agreement, human_samre_agreement]
})

# Round the scores to 3 decimal places
agreement_df['Krippendorff Alpha'] = agreement_df['Krippendorff Alpha'].round(3)

# Display the DataFrame
agreement_df

# Calculate the percent difference between Baseline-Strong and Baseline-Weak, and SAMRE and Baseline-Strong
baseline_strong_baseline_weak_diff = (human_baseline_strong_agreement - human_baseline_weak_agreement) / human_baseline_strong_agreement
baseline_strong_samre_diff = (human_baseline_strong_agreement - human_samre_agreement) / human_baseline_strong_agreement
samre_baseline_weak_diff = (human_samre_agreement - human_baseline_weak_agreement) / human_samre_agreement

# Display the percent difference
print(f"SAMRE vs. Baseline-Weak: {samre_baseline_weak_diff:.0%}")
print(f"Baseline-Strong vs. Baseline-Weak: {baseline_strong_baseline_weak_diff:.0%}")
print(f"Baseline-Strong vs. SAMRE: {baseline_strong_samre_diff:.0%}")
```

Although none of the methods yielded particularly strong agreement with the human judges, we can observe a few things:
1. As reported in the paper, SAMRE yielded significantly better agreement than Baseline-Weak (0.307 vs. 0.252, an increase of ~18%).
2. Baseline-Strong yielded significantly better agreement than Baseline-Weak (0.391 vs. 0.252, an increase of ~36%).
3. Importantly, Baseline-Strong also yielded significantly better agreement than SAMRE (0.391 vs. 0.252, an increase of ~21%)!

Thus, SAMRE does not perform better than a baseline that is designed with best practices and similar multi-round aggregation.

# Conclusion

In this post, I have shown that SAMRE does not perform better than a well-engineered baseline method. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.

